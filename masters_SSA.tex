\documentclass[specialist,
               substylefile = spbu.rtx,
               subf,href,colorlinks=true, 12pt]{disser}

\usepackage[a4paper,
            mag=1000, includefoot,
            left=3cm, right=1.5cm, top=2cm, bottom=2cm, headsep=1cm, footskip=1cm]{geometry}
\usepackage[T2A]{fontenc}
\usepackage[cp1251]{inputenc}
\usepackage[english,russian]{babel}

\usepackage{arydshln}
%\usepackage[utf8]{inputenc}
%\usepackage[T1]{fontenc}
\ifpdf\usepackage{epstopdf}\fi
\pagestyle{plain}
% подключаем hyperref (для ссылок внутри  pdf)
\ifpdf\usepackage{epstopdf}\fi
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm}
\DeclareMathOperator*{\argminB}{argmin}   % Jan Hlavacek
\newtheorem{predl}{Утверждение}
\newtheorem{proposition}{Утверждение}
\newtheorem{def1}{Определение}
\newtheorem{notice}{Замечание}
\newtheorem{lemma}{Лемма}
\newtheorem{theorem}{Теорема}
%\newtheorem{algorithm}{Алгоритм}
\newenvironment{proof}[1][Доказательство]{\noindent\textbf{#1.} }{\hfill $\Box$ \\}

\DeclareMathOperator{\med}{med}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\tr}{tr}
\newcommand{\tX}[1]{\mathsf{#1}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\def\X{\mathbf{X}}
\def\Y{\mathbf{Y}}
\def\U{\mathbf{U}}
\def\V{\mathbf{V}}
\def\T{\mathrm{T}}
\def\EE{\mathbb{E}}
\def\DD{\mathbb{D}}

\newcommand{\E}{\mbox{E}}
\newcommand{\D}{\mbox{D}}

\usepackage[ruled,vlined]{algorithm2e}
% Перевод плагина
\SetKwInput{KwData}{Исходные параметры}
\SetKwInput{KwResult}{Результат}
\SetKwInput{KwIn}{Входные данные}
\SetKwInput{KwOut}{Выходные данные}
\SetKwIF{If}{ElseIf}{Else}{если}{тогда}{иначе если}{иначе}{конец условия}
\SetKwFor{While}{до тех пор, пока}{выполнять}{конец цикла}
\SetKw{KwTo}{от}
\SetKw{KwRet}{возвратить}
\SetKw{Return}{возвратить}
\SetKwBlock{Begin}{начало блока}{конец блока}
\SetKwSwitch{Switch}{Case}{Other}{Проверить значение}{и выполнить}{вариант}{в противном случае}{конец варианта}{конец проверки значений}
\SetKwFor{For}{цикл}{выполнять}{конец цикла}
\SetKwFor{ForEach}{для каждого}{выполнять}{конец цикла}
\SetKwRepeat{Repeat}{повторять}{до тех пор, пока}
\SetAlgorithmName{Алгоритм}{алгоритм}{Список алгоритмов}

\usepackage{hyperref}

% Точка с запятой в качестве разделителя между номерами цитирований
%\setcitestyle{semicolon}

% Использовать полужирное начертание для векторов
\let\vec=\mathbf

% Включать подсекции в оглавление
\setcounter{tocdepth}{2}

\graphicspath{{fig/}}


%----------------------------------------------------------------
\begin{document}

%
% Титульный лист на русском языке
%

% Название организации
\institution{%
    Санкт-Петербургский государственный университет
}

\title{Выпускная квалификационная работа}

% Тема
\topic{Робастные варианты метода анализа сингулярного спектра}

% Автор
\author{\textsc{Третьякова} Александра Леонидовна}

\group{%
    Уровень образования: магистратура\\
    Направление 01.04.02 <<Прикладная математика и информатика>>\\
    Основная образовательная программа ВМ.5688.2018 <<Прикладная математика и информатика>> \\
    Профессиональная траектория <<Статистическое моделирование>>
}

% Научный руководитель
\sa       {Н.\,Э.~Голяндина}
\sastatus {Доцент, кафедра статистического моделирования\,\\
           к.\,ф.-м.\,н., доцент}

% Рецензент
\rev      {А.\,Н.~Пепелышев}
\revstatus{Лектор, Университет Кардиффа (Великобритания)\\
           к.\,ф.-м.\,н.}

% Город и год
\city{Санкт-Петербург}
\date{\number\year}

\maketitle

%%
%% Titlepage in English
%%
%
\institution{%
	Saint Petersburg State University \\
	Applied Mathematics and Computer Science \\
	Statistical Modelling
}
%
\title{Graduation Project}
%
%% Topic
\topic{Robust versions of the singular spectrum analysis method}
%
%% Author
\author{\textsc{Tretyakova} Aleksandra Leonidovna} % Full Name
\group{}

%% Scientific Advisor
\sa       {N.\,E.~Golyandina}
\sastatus {Associate Professor, Department of Statistical Modelling}
%
%% Reviewer
\rev      {A.\,N.~Pepelyshev}
\revstatus{Lecturer, Cardiff University}
%
%% City & Year
\city{Saint Petersburg}
\date{\number\year}

\maketitle[en]

\pagestyle{footcenter}
\chapterpagestyle{footcenter}

\tableofcontents
\intro
%\section{Введение}
В реальной жизни часто возникают задачи исследования различных процессов с течением времени. Пусть имеется $x(t)$~--- функция, описывающая некоторый процесс во времени. Если произвести измерения через одинаковые промежутки времени $t_i$, где $i=1,\ldots,N$, тогда $x_i=x(t_i)$ представляют собой временной ряд $\tX{X}=(x_1, \ldots, x_{N})$.

Для решения многих задач, к примеру, экономических, таких как планирование производства или инвестиций, оказывается полезным на основе данных за предшествующий период выделить основную динамику и тенденции, а также спрогнозировать развитие процесса. В данной работе для исследования временных рядов будет применен метод анализа сингулярного спектра (Singular Spectrum Analysis, или коротко SSA), другое название «Гусеница»~\cite{SSA_and_rel_tech,SSA}, который позволяет анализировать ряд без задания его параметрической модели. Метод нашел свое применение в задачах исследования климатических явлений~\cite{Climatic}, динамических систем~\cite{Dynamic1,Dynamic2} и во многих других областях. Пусть имеется временной ряд $\tX{X}=(x_1, \ldots, x_{N})$ длины $N$, который представляет собой сумму сигнала и шума: $x_i = s_i+r_i,$ $i=1,\ldots, N.$ Данный метод позволяет получить разложение интересующего нас временного ряда $\tX{X}$ на интерпретируемые аддитивные составляющие:
\begin{equation*}
\mathsf{X}=\tX{S}+\tX{R},
\end{equation*}
где
$\tX{S}$~--- сигнал,
$\tX{R}$~--- шум, например, некоторый стационарный процесс.

%Традиционно при постановке задачи отделения сигнала от шума шум предполагается гауссовским.
% со сравнительно небольшой дисперсией. Таким образом, очень большие значения шума маловероятны.
На практике часто возникают выделяющиеся наблюдения или выбросы, которые можно интерпретировать как ошибки в данных или сбои измерительного прибора, значительно большие, чем размер шума. % В таком случае обычно рассматривают модель шума в виде (обычно гауссовского) шума с небольшой дисперсией и шума с большой дисперсией в отдельно взятых точках ряда.
%Выбросы иногда соответствуют ошибкам в записи данных или сбоям измерительного прибора.
%Несмотря на то, что их можно отфильтровать,
Отфильтровать их оказывается непростой задачей, необходимо сначала разобраться со структурой ряда, чтобы понять, что данное значение является выбросом.
%(ссылка, например, на медианный фильтр)
Поэтому разработка исходно устойчивых к выбросам методов представляет значительный интерес.

Ранее в работе~\cite{vkr} уже были предложены несколько устойчивых к выбросам вариантов метода,  но они оказались слишком трудоемкими. Поэтому необходимо найти методы, которые бы оставались устойчивыми, но время работы алгоритмов было бы меньше. В данной работе стоит задача предложить менее трудоемкие варианты метода, сравнить робастные модификации между собой и с базовым SSA.

%Метод SSA имеет параметр, называемой длиной окна. При достаточно большой длине окна метод отделяет компоненты ряда друг от друга за счет так называемой разделимости \cite{SSA}. Благодаря разделимости метод SSA позволяет выделить сигнал из зашумленного ряда,
%отделить тренд от периодических компонент.

%Кроме того, при малой длине окна метод можно рассматривать как метод фильтрации ряда с помощью конечного линейного фильтра \cite{SSATimeSeries}.
%Одну из модификаций стандартного метода, SSA с центрированием, описанную в разделе 5.1 пособия~\cite{SSA}, также можно рассматривать как метод фильтрации.

В методе SSA при выделении сигнала используются два проектора, которые могут строиться по различным нормам. Один из проекторов --- это проектор на пространство ганкелевых матриц, второй --- проектор на множество матриц ранга, не превосходящего $r$. В стандартном методе SSA используются проекторы в пространстве матриц по норме $\mathbb{L}_2$ (норма Фробениуса).

В качестве модификаций в работе~\cite{vkr} рассматривался стандартный прием использования аппроксимации (проекции) по норме в  $\mathbb{L}_1$ вместо  $\mathbb{L}_2$. Если построение проектора на ганкелевы матрицы по норме $\mathbb{L}_1$ не представляет трудности, то вычисление проектора на матрицы ранга, не превосходящего $r$, по норме $\mathbb{L}_1$ не имеет решения в замкнутой форме. Имеются методы, численно решающие приближенные задачи, но не известно достаточно хороших методов для задачи, которую требуется решить при построении проектора на матрицы ранга, не превосходящего $r$.

Еще одной идеей для достижения устойчивости метода к выделяющимся наблюдениям является присвоение значениям в точках, содержащих выбросы, меньший вес. В данной работе рассмотрим алгоритм, описанный в статье~\cite{Chen}, включающий в себя итеративное обновление весов. Метод имеет параметры: $\{\sigma_{ij},~ i=1,\ldots,L, j=1,\ldots,K\}$ (нормировка для остатков) и $\alpha$ (значение, начиная с которого точку считать выбросом). В оригинальном методе из статьи~\cite{Chen} параметры $\sigma_{ij}$ полагаются одинаковыми для всех элементов траекторной матрицы, то есть $\sigma_{ij} = \sigma~ \forall i,j$. Данный метод оказывается неподходящим для рядов, где шум имеет непостоянную дисперсию. Поэтому стоит задача предложить его модификацию, которая бы оставалась устойчивой как в случае рядов с шумом постоянной дисперсии, так и в случае рядов с гетероскедастичным шумом. В модификации метода предполагается замена параметра $\sigma_{ij}$ на элементы траекторной матрицы тренда (оценки математического ожидания) ряда из модулей остатков.  Так как в таком случае матрица $\mathbf{\Sigma} = \{\sigma_{ij}\}_{i,j=1}^{L,K}$ ганкелева, то это соответствует присваиванию весов элементам ряда. Поэтому возникает идея вычислять проекцию не траекторной матрицы ряда, а проектировать сам ряд на множество рядов ранга, не превосходящего $r$. Поэтому рассмотрим вариант с использованием модифицированного метода Гаусса-Ньютона из статьи~\cite{MGN} для решения задачи построения проекции исходного ряда на множество рядов ранга, не превосходящего $r$, по взвешенной норме в $\mathbb{L}_2$.
% Введем классификацию методов согласно используемым нормам. В общем случае методы будут называться LiSVD-LjH-SSA, где $i, j$ могут быть равны 1 или 2. LiSVD означает проектор на матрицы ранга, не превосходящего $r$, по норме в пространстве $\mathbb{L}_2$ (L2SVD) или $\mathbb{L}_1$ (L1SVD), а LjH --- проектор на пространство ганкелевых матриц по норме $\mathbb{L}_2$ (L2H) или $\mathbb{L}_1$ (L1H). Для более короткой записи будем называть стандартный метод с двумя проекторами по норме в $\mathbb{L}_2$ методом L2-SSA, а метод с двумя проекторами в $\mathbb{L}_1$ --- L1-SSA.
% Будет рассматриваться L2SVD-L1H-SSA как переходный вариант между L2-SSA и L1-SSA. Вариант L1SVD-L2H-SSA также будет рассмотрен, но ожидаемо, что метод L1-SSA в результате будет более робастным.

Структура работы следующая. В главе~\ref{sec:BasicSSA} опишем базовый алгоритм метода SSA, введем необходимые понятия и обозначения, обсудим выбор параметров метода на основе теории метода SSA.

В начале главы~\ref{sec:modifications} рассмотрим общую схему методов без указания конкретной нормы.  Приведем вид проекторов на пространство ганкелевых матриц по трем нормам: по норме в пространствах $\mathbb{L}_2$, $\mathbb{L}_1$ и взвешенной норме в $\mathbb{L}_2$. В данной главе представим нахождение $\mathbb{L}_1$-проектора на множество матриц ранга, не превосходящего $r$. В работе идет речь о последовательном методе проектирования. Этот метод рассматривается в R-пакете в рамках построения устойчивого к выбросам анализа главных компонент~\cite{pcaL1book}. Далее опишем метод с итеративным обновлением весов и его модификацию, подходящую для рядов с гетероскедастичным шумом. Также опишем модификацию этого метода, где веса присваиваются не траекторной матрице, а самому ряду. Приведем алгоритмы для каждого из методов. Произведем подсчет и сравнение теоретической трудоемкости описанных методов.
%Будем рассматривать два случая: когда траекторная матрица близка к квадратной, и случай с вытянутой траекторной матрицей. В обоих случаях асимптотически теоретическая трудоемкость последовательного метода с проекторами по норме в пространстве $\mathbb{L}_1$ оказывается больше теоретической трудоемкости метода с итеративным обновлением весов с проекторами по взвешенной норме в $\mathbb{L}_2$.

Глава~\ref{sec: experiments} будет содержать численные сравнения с исследованием влияния выброса на результат восстановления сигнала. В данной главе представим сравнение рассмотренных методов между собой и с классическим SSA. Сравнение будем проводить при отсутствии выделяющихся наблюдений, при 1\% выбросов в случайных точках ряда и при 5\% выбросов.

Проведем сравнение на нескольких модельных примерах. Один из них уже был представлен в работе~\cite{vkr}, но мы добавим большее количество выбросов. Второй пример --- ряд с растущей амплитудой с шумом, имеющим непостоянную дисперсию. Третий пример --- ряд с сильно растущей амплитудой и большим разбросом значений, но с шумом постоянной дисперсии. В конце проведем сравнение нескольких методов на реальном ряде с выбросами.  %Было показано, что метод, использующий взвешенный метод наименьших квадратов оказывается наиболее устойчивым среди других методов для первого примера. Предложенная в пункте~\ref{sec: IRLS} модификация для второго ряда дает наименьшую ошибку в присутствии выделяющихся наблюдений.

%Была осуществлена попытка перенести вывод о преимуществе метода с регуляризацией на другие ряды. Поэтому в качестве второго примера был рассмотрен другой ряд, у которого быстро растет амплитуда, и разброс значений получается большим.

В заключении опишем основные результаты работы, подведем итоги. На основе проведенных исследований  дадим рекомендации, какой из рассмотренных методов следует использовать в том или ином случае.

%Работа в текущем семестре заключалась в рассмотрении еще одной модификации метода с обновлением весов, где веса присваиваются не траекторной матрице, а самому ряду. Также была рассмотрена зависимость трудоемкости последовательного метода и метода с обновлением весов от длины ряда. 

%Работа в текущем семестре заключалась в рассмотрении идеи с добавлением маленьких весов точкам, содержащим выбросы. Был рассмотрен метод, использующий взвешенный метод наименьших квадратов. Так как были обнаружены недостатки этого метода в случае рядов с растущей или убывающей амплитудой, то была разработана модификация с целью борьбы с этими недостатками. Проведено сравнение теоретических трудоемкостей рассматриваемых методов и их сравнение между собой. На трех примерах было проведено сравнение методов, а также проверена значимость сравнений.



\chapter{Стандартный метод SSA и его свойства}\label{sec:BasicSSA}
%\section{Постановка задачи}
%Пусть $x(t)$~--- функция, описывающая некоторый процесс во времени. Обозначим $t_i$~--- моменты времени через одинаковые интервалы, где $i=1,\ldots,N$, тогда $x_i=x(t_i)$ представляют собой временной ряд $(x_1, \ldots, x_{N})$. 
%
%Рассмотрим вещественнозначный временной ряд
%%\begin{equation*}
%$\tX{X}=(x_1, \ldots, x_{N})$,
%%\end{equation*}
%где $N>2$~--- длина ряда. Предположим, что ряд ненулевой, т.е. $\exists i: x_i\ne0$.
%
%Задача состоит в разложении временного ряда на интерпретируемые аддитивные составляющие: 
%\begin{equation*}
%\mathsf{X}=\tX{S}+\tX{R},
%\end{equation*}
%где
%$\tX{S}$~--- сигнал,
%$\tX{R}$~--- шум, например, некоторый стационарный процесс. В данной работе будем рассматривать белый гауссовский шум.
%
%Необходимо исследовать устойчивость метода SSA к выделяющимся наблюдениям. В данной работе будем моделировать выбросы прибавлением в отдельно взятой точке достаточно большого значения и исследовать устойчивость метода в зависимости от этого значения.

\section{Алгоритм метода SSA}\label{sec: SSA}
Кратко опишем базовый алгоритм метода «Гусеница»-SSA, следуя \cite{SSA}.
\subsection{Вложение}
На первом шаге алгоритма выбирается некоторое целое число $L$: $1<L<N$, называемое \emph{длиной окна}. Исходный временной ряд переводится в последовательность многомерных векторов длины $L$. В результате образуются $K=N-L+1$ векторов вложения 
\begin{equation*} 
X_i=(x_{i},\ldots,x_{i+L-1})^\mathrm{T}, 1\le i\le K.
\end{equation*} 

\emph{Траекторной матрицей} ряда $\tX{X}$ называется матрица \\

$\mathbf{X}=[X_1:\ldots:X_K]=\left( \begin{array} {ccccc}
x_1 & x_2 & x_3&\ldots &x_{K} \\
x_2 & x_3 & x_4&\ldots &x_{K+1} \\
x_3 & x_4 & x_5&\ldots &x_{K+2} \\
\vdots& \vdots& \vdots&\ddots &\vdots \\
x_{L}& x_{L+1} & x_{L+2}&\ldots &x_{N} \\
\end{array}\right)$.\\

Заметим, что построенная таким образом траекторная матрица $\mathbf{X}$ является \emph{ганкелевой}, т.е. элементы, находящиеся на диагоналях $i+j=\mathrm{const}$, равны между собой.
\subsection{Сингулярное разложение}
Пусть $\mathbf{S}=\mathbf{X}\mathbf{X}^\textrm{T}$, обозначим $\lambda_1\ge\lambda_2\ge\ldots\ge\lambda_d>0$~--- ненулевые собственные числа матрицы $\mathbf{S}$, $U_1,\ldots,U_d$ — ортонормированная система собственных векторов матрицы $\mathbf{S}$, соответствующих собственным числам. 
\emph{Сингулярным разложением} матрицы $\mathbf{X}$ называется разложение
\begin{equation*} 
\mathbf{X}=\mathbf{X}_1+\ldots+\mathbf{X}_d=\sum \limits_{i=1}^{d}{\sqrt{\lambda_i}U_iV_i^\mathrm{T}},
\end{equation*} 
где $\sqrt{\lambda_i}$~--- \emph{сингулярные числа}, 
$U_i$~--- \emph{левые сингулярные вектора}, 
$V_i=\frac{1}{\sqrt{\lambda_i}}X^\mathrm{T}U_i$~--- \emph{правые сингулярные вектора}.

Набор $(\sqrt{\lambda_i},U_i,V_i)$ назовем \emph{$i$-ой собственной тройкой} сингулярного разложения.
\subsection{Группировка}
Разделим множество индексов $\{1,\ldots,d\}$ на $m$ дизъюнктных подмножеств $I_1,\ldots,I_m$. Пусть $I=\{i_1,\ldots,i_p\}$. Тогда \emph{результирующая матрица} $\mathbf{X}_I$ имеет вид
\begin{equation*} 
\mathbf{X}_I=\mathbf{X}_{i_1}+\ldots+\mathbf{X}_{i_p}.
\end{equation*} 

Таким образом, получаем разложение матрицы $\mathbf{X}$ в сгруппированном виде 
\begin{equation*} 
\mathbf{X}=\mathbf{X}_{I_1}+\ldots+\mathbf{X}_{I_m}.
\end{equation*} 
\subsection{Диагональное усреднение}
На последнем шаге каждая матрица $\mathbf{X}_{I_i}$ переводится в новый ряд с помощью усреднения элементов матрицы вдоль антидиагоналей $i+j=k+1$. Применяя диагональное усреднение к результирующим матрицам, получаем ряды $\tilde{\tX{X}}^{(k)}=(\tilde{x}^k_1,\ldots,\tilde{x}^k_{N})$. 

В результате получаем разложение исходного ряда $(x_1,\ldots, x_{N})$ в сумму $m$ рядов:
\begin{equation*}
x_n=\sum_{k=1}^{m}{\tilde{x}_n^{(k)}}.
\end{equation*} 
\section{Разделимость}\label{sec: partibility}
Введем понятие разделимости, следуя \cite{SSA}. 

Пусть $\tX{X}_1$ и $\tX{X}_2$ --- временные ряды длины $N$ и $\tX{X}=\tX{X}_1 + \tX{X}_2$. Пусть выбрана длина окна $L$, тогда каждый из рядов порождает $L$-траекторную матрицу: $\mathbf{X}_1$, $\mathbf{X}_2$ и $\mathbf{X}$. Обозначим $\mathcal{L}_1^L$ и $\mathcal{L}_2^L$ --- линейные пространства, порожденные столбцами траекторных матриц $\mathbf{X}_1$ и $\mathbf{X}_2$. Аналогично обозначим $\mathcal{L}_1^K$ и $\mathcal{L}_2^K$ --- линейные пространства, порожденные строками траекторных матриц $\mathbf{X}_1$ и $\mathbf{X}_2$, $K=N-L+1$.

\begin{def1}
	Ряды $\tX{X}_1$ и $\tX{X}_2$ называются слабо $L$-разделимыми, если $\mathcal{L}_1^L \perp \mathcal{L}_2^L$ и $\mathcal{L}_1^K \perp \mathcal{L}_2^K$.
\end{def1}

В результате выполнения этапа разложения в алгоритме SSA получаем некоторое разложение траекторной матрицы ряда, которое не обязательно соответствует разделимости двух рядов. Поэтому необходимо усилить понятие разделимости.

\begin{def1}
	Если ряды $\tX{X}_1$ и $\tX{X}_2$ слабо $L$-разделимы и множество собственных чисел сингулярного разложения траекторной матрицы одного ряда не пересекается с множеством собственных чисел разложения второго ряда ($\lambda_{1k}\ne\lambda_{2m}~\forall k,m$), то ряды $\tX{X}_1$ и $\tX{X}_2$ сильно $L$-разделимы.
\end{def1}

\begin{proposition}
	Пусть ряды $\tX{X}_1$ и $\tX{X}_2$ сильно $L$-разделимы. Тогда любое сингулярное разложение траекторной матрицы $\mathbf{X}$ ряда $\tX{X}$ можно разбить на две части, являющиеся сингулярными разложениями траекторных матриц рядов $\tX{X}_1$ и $\tX{X}_2$.
\end{proposition}

Однако условия разделимости являются слишком жесткими и редко выполнены в реальных задачах. Поэтому в пособии~\cite{SSA} введено также понятие асимптотической разделимости.

%Для ряда $\tX{X}=(x_1, \ldots, x_{N})$ положим $X_{i,j}=(x_i,\ldots,x_j),~ 1\le i \le j <N$. Пусть $\tX{X}^{(1)}_N=(x_1^{(1)}, \ldots, x_{N}^{(1)}),~\tX{X}^{(2)}_N=(x_1^{(2)}, \ldots, x_{N}^{(2)}) $. Пусть 
%\begin{equation*}
%\rho_{i,j}^{(M)} = \frac{(X_{i,i+M-1}^{(1)},X_{j,j+M-1}^{(2)})}{\|X_{i,i+M-1}^{(1)}\| ~\|X_{j,j+M-1}^{(2)}\|},~~ i,j \ge 1,~ M\le N-\max(i,j),
%\end{equation*}
%где $\|~.~\|$ --- евклидова норма, $(~.~,~.~)$ --- скалярное произведение векторов.
%Если знаменатель равен нулю, то предполагаем, что $\rho_{i,j}^{(M)} = 0$.
%
%Число $\rho_{i,j}^{(M)}$ равно косинусу угла между векторами $X_{i,i+M-1}^{(1)}$ и $X_{j,j+M-1}^{(2)}$.
%\begin{def1}
%Ряды $\tX{X}^{(1)}_N$ и $\tX{X}^{(2)}_N$ называются $\varepsilon$-разделимыми при длине окна $L$, если 
%\begin{equation*}
%\rho^{(L,K)} = \max (\max \limits_{1\le i,j\le K} |\rho_{i,j}^{(L)}|, \max  \limits_{1\le i,j\le L} |\rho_{i,j}^{(K)}|) \le \varepsilon,
%\end{equation*}
%$K=N-L+1$. Если число $\varepsilon$ мало, то ряды называются приближенно разделимыми.
%\end{def1}
% Если $\rho^{(L,K)} = 0$, то это соответствует точной разделимости.
% 
% Введем понятие асимптотической разделимости. Пусть $\tX{X}^{(1)}=(x^{(1)}_1,\ldots,x^{(1)}_n,\ldots)$, $\tX{X}^{(2)}=(x^{(2)}_1,\ldots,x^{(2)}_n,\ldots)$ и для любого $N>2$ ряды $\tX{X}^{(1)}_N$ и $\tX{X}^{(2)}_N$ состоят из первых $N$ членов рядов $\tX{X}^{(1)}$ и $\tX{X}^{(2)}$. Тогда если выбрать последовательность длин окон $1<L=L(N)<N$, получим последовательность $\rho_N=\rho^{(L(N),K(N))}$.
% \begin{def1}
% 	Если $\rho^{(L(N),K(N))} \to 0$ при некоторой последовательности $L=L(N)$, $N\to~\infty$, то ряды $\tX{X}^{(1)}$ и $\tX{X}^{(2)}$ называются асимптотически $L(N)$-разделимыми. Если для любой последовательности $L(N)$: $L(N) \to \infty, K(N) \to \infty$ ряды $\tX{X}^{(1)}$ и $\tX{X}^{(2)}$ асимптотически $L(N)$-разделимы, то они называются асимптотически разделимыми.
% \end{def1}

Асимптотическая раделимость выполняется для более широкого класса рядов, чем точная разделимость. К примеру, $e^{\alpha n}$ и $\sin(2\pi\omega n)$,  где $\alpha\ne 0,~ \omega \in  (0,0.5]$, асимптотически разделимы.

Для достижения лучшей разделимости необходимо выбирать большую длину окна $(L \sim N/2)$. Большая длина окна позволяет выделить сигнал из зашумленного ряда, отделить тренд от периодических компонент. Не имеет смысла брать длину окна, большую чем половина длины ряда, а маленькая длина окна может привести к смешиванию компонент ряда. 
\section{Ранг ряда}\label{sec:rank}
Пусть 	$\tX{X}_N=\tX{X}_{N}^{(1)}+\tX{X}_{N}^{(2)}$ и ряды $\tX{X}_{N}^{(1)}$ и $\tX{X}_{N}^{(2)}$ разделимы. Тогда в сингулярном разложении ряда 	$\tX{X}_N$ часть слагаемых относится к сингулярному разложению ряда $\tX{X}_{N}^{(1)}$, а другая часть --- к сингулярному разложению ряда $\tX{X}_{N}^{(2)}$. Необходимо выяснить, сколько слагаемых относится к первому ряду и как их идентифицировать. На этапе группировки индексы слагаемых, относящихся к первому ряду, образуют подмножество $I_1$, остальные --- подмножество $I_2$. Для того, чтобы понимать, сколько компонент относить к первому подмножеству, введем понятие ранга. 

Рассмотрим ряд  $\tX{X}_N = (x_1, \ldots, x_{N})$, пусть $L$ -- длина окна. 

Обозначим $\mathcal{L}^{(L)}=\text{span}(X_1, \ldots, X_K)$ --- траекторное пространство ряда $\tX{X}_N$, где $X_i=(x_{i},\ldots,x_{i+L-1})^T$ --- векторы вложения, $1\le i\le K$.
\begin{def1}
	Пусть $0< d \le \min (L,K) $. Будем говорить, что ряд $\tX{X}_N$ имеет $L$-ранг $d$, если $\dim{\mathcal{L}^{(L)}} = d$.
\end{def1}

Например, в случае экспоненциального ряда $e^{\alpha n}$ для любых $N$ и $L$ ранг ряда равен 1, а ранг гармонического ряда $\sin(2\pi \omega n+\phi)$ равен 2 при $\omega<1/2$ и 1 при $\omega=1/2$, $\phi \in [0,2\pi)$.


\chapter{Модификации метода SSA c проекторами по некоторой норме}\label{sec:modifications}

Будем рассматривать вариант метода SSA для выделения сигнала, когда группировка заключается в выборе первых $r$ компонент. Для стандартного метода SSA это эквивалентно проекции по норме Фробениуса траекторной матрицы ряда на множество матриц ранга, не превосходящего $r$.

\section{Схема методов}		
Пусть имеется временной ряд $\tX{X}=(x_1, \ldots, x_{N})$.

Выбирается длина окна $L$, и исходный временной ряд переводится в последовательность многомерных векторов длины $L$. В результате образуются $K=N-L+1$ векторов вложения 
\begin{equation*} 
X_i=(x_{i},\ldots,x_{i+L-1})^\mathrm{T}, 1\le i\le K.
\end{equation*} 

%\emph{Траекторной матрицей} ряда $F$ называется матрица \\
%
%$\mathbf{X}=[X_1:\ldots:X_K]=\left( \begin{array} {ccccc}
%x_1 & x_2 & x_3&\ldots &x_{K} \\
%x_2 & x_3 & x_4&\ldots &x_{K+1} \\
%x_3 & x_4 & x_5&\ldots &x_{K+2} \\
%\vdots& \vdots& \vdots&\ddots &\vdots \\
%x_{L}& x_{L+1} & x_{L+2}&\ldots &x_{N} \\
%\end{array}\right)$.\\
%
%Заметим, что построенная таким образом траекторная матрица $\mathbf{X}$ является \emph{ганкелевой}, т.е. элементы, находящиеся на диагоналях $i+j=\mathrm{const}$, равны между собой.

Обозначим  $\mathcal{M}$ --- пространство матриц $L\times K$,

$\mathcal{M}_{\mathcal{H}}$ --- пространство ганкелевых матриц $L\times K$,

$\mathcal{M}_{r}$ --- пространство матриц ранга, не превосходящего $r$.

Определим следующие операторы: 

\begin{itemize} 
	\item{ Оператор вложения $\mathcal{T}:\mathbb{R}^N \rightarrow \mathcal{M}_{\mathcal{H}}: \mathcal{T} (\tX{X}) = \mathbf{X} $.
	}
	\item{ $\Pi_{r}:\mathcal{M}\rightarrow \mathcal{M}_r$ --- проектор на множество матриц ранга, не превосходящего $r$, по некоторой норме в пространстве матриц.
	}
	\item{
		$\Pi_{\mathcal{H}}:\mathcal{M} \rightarrow \mathcal{M}_{\mathcal{H}}$ --- проектор на пространство ганкелевых матриц по некоторой норме в пространстве матриц.
		%по норме Фробениуса посредством усреднения элементов на диагоналях $i+j=\text{const}$.
	}
\end{itemize}

В результате применения данных операторов получаем оценку сигнала:
\begin{equation*}
\tilde{\tX{S}} = \mathcal{T}^{-1} \Pi_{\mathcal{H}} \Pi_{r} \mathcal{T} (\tX{X}).
\end{equation*}

Это соответствует алгоритму SSA, описанному в разделе~\ref{sec: SSA}, для случая, когда восстановление производится по одной группе, состоящей из первых $r$ компонент.

Проекторы $ \Pi_{r}$ и $\Pi_{\mathcal{H}} $ можно строить по различным нормам. С точки зрения вычислений, удобно выбирать $\mathbb{L}_2$-норму для построения проекторов на пространство ганкелевых матриц и матриц ранга, не превосходящего $r$, поскольку целевая функция является гладкой и выпуклой, и решить задачу минимизации довольно просто, можно даже говорить о задании решения в явной форме. Однако при наличии выбросов норма Фробениуса оказывается недостаточно устойчивой. Норма в пространстве $\mathbb{L}_1$ является более устойчивой к выделяющимся наблюдениям, однако сложность в ее использовании состоит в негладкой и невыпуклой строго целевой функции, поэтому возникает проблема в применении стандартных методов оптимизации. Существует также вариант использования взвешенной нормы в $\mathbb{L}_2$ с присваиванием меньших весов точкам, содержащим выделяющиеся наблюдения.

В работе будут рассмотрены проекторы по нормам в пространствах $\mathbb{L}_2$, $\mathbb{L}_1$ и взвешенной норме в пространстве $\mathbb{L}_2$. Будем рассматривать следующие методы: 
\begin{itemize}
	\item Стандартный метод SSA, где оба проектора $ \Pi_{r}$ и $\Pi_{\mathcal{H}} $ строятся по норме в пространстве $\mathbb{L}_2$,
	\item Метод c проекторами $ \Pi_{r}$ и $\Pi_{\mathcal{H}} $ по норме в пространстве $\mathbb{L}_1$,
	\item Метод c проекторами $ \Pi_{r}$ и $\Pi_{\mathcal{H}} $ по взвешенной норме в пространстве $\mathbb{L}_2$.
\end{itemize} % Будем называть его L2-SSA. Метод с проекцией на множество ганкелевых матриц в $\mathbb{L}_1$ и проекцией на множество матриц ранга, не превосходящего $r$, в $\mathbb{L}_2$ назовем L2SVD-L1H-SSA. Метод с обеими проекциями в $\mathbb{L}_1$ будем называть L1-SSA. 

\section{Вид проекторов на пространство ганкелевых матриц по различным нормам}
Рассмотрим, как выглядят проекторы на пространство ганкелевых матриц по нормам в пространствах $\mathbb{L}_2$, $\mathbb{L}_1$ и взвешенной норме в $\mathbb{L}_2$.
\begin{def1} 
	Пусть $\mathbf{A}$ --- матрица $L\times K$.
	
	Норма в пространстве $\mathbb{L}_2$ (норма Фробениуса):  $\|\mathbf{A}\|_\mathrm{F} = \sqrt{\sum\limits_{i=1}^{L} {\sum\limits_{j=1}^{K} a_{ij}^2}}$.			
\end{def1}

\begin{lemma} [Известный результат]\label{lemma: minEsq}
	$\argminB\limits_a \mathbb{E} (\xi - a)^2 = \mathbb{E} \xi $.
\end{lemma}
\begin{proof}
	\begin{equation*}
	\mathbb{E} (\xi - a)^2=\mathbb{E} ((\xi-\mathbb{E} \xi)+(\mathbb{E} \xi -a))^2 = \mathbb{E} ((\xi-\mathbb{E} \xi)^2+2(\xi-\mathbb{E} \xi)(\mathbb{E} \xi -a)+(\mathbb{E} \xi -a)^2) =
	\end{equation*} 
	\begin{equation*}	
	=\mathbb{E} (\xi-\mathbb{E} \xi)^2 + 2(\mathbb{E} \xi -a)\mathbb{E} (\xi-\mathbb{E} \xi)+(\mathbb{E} \xi -a)^2 = \mathbb{E} (\xi-\mathbb{E} \xi)^2 +(\mathbb{E} \xi -a)^2 .~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
	\end{equation*}
	Следовательно, $\argminB\limits_a \mathbb{E} (\xi - a)^2 = \mathbb{E} \xi $.
\end{proof}

Необходимо построить проектор на множество ганкелевых матриц по норме Фробениуса. Заметим, что 
\begin{equation*}
\|\mathbf{X}-\mathbf{A}\|_\mathrm{F}^2 =\sum\limits_{i=1}^L {\sum\limits_{j=1}^K (x_{ij}-a_{ij})^2} = \sum\limits_{k=1}^{L+K-1} {\sum\limits_{i+j=k+1} (x_{ij}-a_{k})^2}\longrightarrow \min\limits_{\mathbf{A} \in \mathcal{M}_{\mathcal{H}} }.
\end{equation*}
Тогда если взять в качестве $\xi$ случайные величины, принимающие значения на побочной диагонали с равными вероятностями, то из Леммы~\ref{lemma: minEsq} следует, что проектор на множество ганкелевых матриц по норме Фробениуса строится посредством усреднения элементов на диагоналях $i+j=\text{const}$.

Опишем теперь вид данного проектора по норме в пространстве $\mathbb{L}_1$.
\begin{def1} 
	Пусть $\mathbf{A}$ --- матрица $L\times K$.
	
	Норма в пространстве $\mathbb{L}_1$ :  $\|\mathbf{A}\|_1 = \sum\limits_{i,j}^{} {|a_{ij}|}$.		
\end{def1}

%Лемма~\ref{lemma: minMed} содержит хорошо известный результат. Для полноты изложения приведем его доказательство.
\begin{lemma}[Известный результат] \label{lemma: minMed}
	~~\\
	В случае непрерывного распределения	$\argminB\limits_a \mathbb{E} |\xi - a| = \med{\xi} $.
\end{lemma}
Доказательство этой леммы приведено в работе~\cite{vkr}.
%\begin{proof}
%	%	Для простоты изложения докажем в случае непрерывного распределения.
%	Пусть $p(x)$ --- плотность распределения. 
%	\begin{equation*}
%	\mathbb{E} |\xi - a| =\int\limits_{-\infty}^\infty |x - a| p(x)dx = \underbrace{\int\limits_{-\infty}^a (a - x) p(x)dx}_{I(a)} + \underbrace{\int\limits_{a}^\infty (x - a) p(x)dx}_{J(a)}.
%	\end{equation*}
%	Используя формулу дифференцирования по параметру функции $F(a) =\int_{\alpha (a)}^{\beta (a)} f(x,a)dx:$
%	\begin{equation*}
%	F'(a) =\int\limits_{\alpha (a)}^{\beta (a)} f'_{a}(x,a)dx+\beta ' (a)f(\beta (a),a) - \alpha ' (a)f(\alpha (a),a),
%	\end{equation*}
%	получим
%	\begin{equation*}
%	I'(a) =\int\limits_{-\infty}^{a} p(x)dx+(a-a)p(x)=P(\xi \le a),
%	\end{equation*}
%	\begin{equation*}
%	J'(a) =\int\limits_{a}^{\infty} p(x)dx-(a-a)p(x)=-P(\xi \ge a).
%	\end{equation*}
%	Условие стационарности приобретает вид:
%	\begin{equation*}
%	P(\xi \le a)=P(\xi \ge a),
%	\end{equation*}
%	откуда следует, что $a=\med \xi$.
%\end{proof}
%
%\begin{notice}\label{notice1}
%	Лемма~\ref{lemma: minMed} доказана для непрерывного распределения. Так как для дискретного распределения медиана не определена однозначно, то будем брать в качестве выборочной медианы в случае четного объема выборки полусумму двух центральных значений. 
%\end{notice}
%Аналогично рассуждениям для проектора в $\mathbb{L}_2$, из Леммы~\ref{lemma: minMed} с учетом замечания~\ref{notice1}

Используя лемму~\ref{lemma: minMed}, получаем, что	$\Pi_{\mathcal{H}}$ в пространстве $\mathbb{L}_1$ строится посредством взятия выборочной медианы значений на побочных диагоналях $i+j=\text{const}$.

\begin{def1} 
	Пусть $\mathbf{A}$ --- матрица $L\times K$, $\mathbf{W}$ --- матрица весов $L\times K$.
	
	Норма в пространстве $\mathbb{L}_2$ с весами $\mathbf{W}$ :  $\|\mathbf{A}\|_\mathrm{W} = \sqrt{\sum\limits_{i=1}^{L} {\sum\limits_{j=1}^{K} w_{ij}a_{ij}^2}}$.			
\end{def1}

Известно следующее утверждение о построении проектора на пространство ганкелевых матриц по данной норме.
\begin{proposition}[\cite{Zvonarev}]
	Для построения проекции $\widehat{\mathbf{A}} = \Pi_{\mathcal{H}}\mathbf{A} = \{\widehat{a}_{ij}\}_{i,j = 1}^{L,K}$  необходимо суммировать элементы на диагоналях $i+j=\text{const}$ с весами и нормировать на сумму весов: 
	\begin{equation*}
	\widehat{a}_{ij} = \frac{\sum\limits_{l,k: l+k=i+j} w_{lk}a_{lk}}{\sum\limits_{l,k: l+k=i+j} w_{lk}}.
	\end{equation*}
\end{proposition}

\begin{notice}
	В случае ганкелевой матрицы весов $\mathbf{W}$ проектор на пространство ганкелевых матриц по взвешенной норме в $\mathbb{L}_2$ совпадает с проектором на пространство ганкелевых матриц по норме в  $\mathbb{L}_2$.
\end{notice}

\section{Построение проектора по норме в $\mathbb{L}_1$ на множество матриц ранга, не превосходящего $r$. Последовательный метод}\label{sec: l1pca}
В отличие от проектора на множество матриц ранга $r$ по норме Фробениуса, построение данного проектора в пространстве $\mathbb{L}_1$ является вычислительно сложной задачей. Рассмотрим один из методов построения проектора на множество матриц ранга, не превосходящего $r$, по норме в пространстве $\mathbb{L}_1$.


%
%Опишем подробно каждый шаг алгоритма.
%
%1. Инициализация. Пусть
%\begin{equation*}
%m =
%\left( \begin{array} {c}
%\med{(|y_{11}|,\ldots, |y_{1p}|)} \\
%\vdots\\
%\med{(|y_{n1}|,\ldots, |y_{np}|)}  \\
%\end{array}\right).
%\end{equation*}
%Возьмем в качестве начального значения $U_1^{0} = m/ \lVert m \rVert_2$.
%~~ \\
%
%2. 
%Находим проекцию в $\mathbb{L}_1$ каждого столбца матрицы $\Y$ на вектор $U_1^{(k)}$, то есть для каждого $j=1,\ldots, p$ решаем задачу минимизации
%%\begin{equation*}
%%\min _{c_j}  \norm{ \biggl\lVert \left( 
%%\begin{array} {c}
%%y_{1j} \\
%%\vdots \\
%%y_{nj} \\
%%\end{array}\right) - c_j * \left( 
%%\begin{array} {c}
%%u_1 \\
%%\vdots \\
%%u_n \\
%%\end{array}\right) }
%%\rVert_1
%%\end{equation*} 
%\begin{equation*}
%\lVert (y_{1j},\ldots, y_{nj})^{\mathrm{T}} - c_j * (u_1, \ldots, u_n)^{\mathrm{T}} \rVert_1 \to \min _{c_j} 
%\end{equation*} 
%методом взвешенной медианы (обозначим здесь $U_1^{(k)}=(u_1, \ldots, u_n)^{\mathrm{T}}$). 
%
%Далее нормируем полученный вектор $C = (c_1,\ldots, c_p)$ и полагаем 
%\begin{equation*}
%V_1^{(k)} = C / \lVert C \rVert _2.
%\end{equation*}
%
%3. 
%Находим проекцию в $\mathbb{L}_1$ каждой строки матрицы $\Y$ на вектор $V_1^{(k)}$, то есть для каждого $i=1,\ldots, n$ решаем задачу 
%\begin{equation*}
%\lVert (y_{i1},\ldots, y_{ip}) - d_i * (v_1, \ldots, v_p) \rVert_1 \to \min _{d_i} 
%\end{equation*} 
%методом взвешенной медианы.
%
%Далее нормируем полученный вектор $D = (d_1,\ldots, d_n)^{\mathrm{T}}$ и полагаем
%\begin{equation*}
%U_1^{(k+1)} = D / \lVert D \rVert _2.
%\end{equation*}
%
%4. Критерий остановки: продолжаем выполнять шаги 2 и 3, пока изменение в $\mathbb{L}_2$ вектора $U_1$ превосходит $\varepsilon$, то есть
%\begin{equation*}
%\text{While~~} \sum_{i=1}^n (U_{1_i}^{(k+1)} - U_{1_i}^{(k)})^2 > \varepsilon.
%\end{equation*} 
%По умолчанию $\varepsilon = 10^{-10}$.
%~~ \\
%
%5. В результате имеем $U_1^*, V_1^*$. Далее находим $\lambda_1^*$, решая задачу
%\begin{equation*}
%\sum_{i=1}^n \sum_{j=1}^p |y_{ij} - \lambda_1 U_1^* V_1^{*^{\T}}| \to \min _{\lambda_1} .
%\end{equation*}
%
%6. Из матрицы $\Y$ вычитаем первую компоненту
%\begin{equation*}
%\Y = \Y - \lambda_1^* \U_1^* \V_1^{*^{\T}}
%\end{equation*}
%и продолжаем искать остальные собственные тройки.
%
%В итоге получаем представление матрицы $\Y$ в виде $\U \mathbf{\Lambda} \V^{\T}$, где $\U$ составлена из $U_1,\ldots, U_p$, а $\V$ состоит из $V_1,\ldots, V_p$. Точнее, мы нашли решение задачи минимизации функции $\lVert \Y -\U \mathbf{\Lambda} \V^{\T}\rVert_1$.
%~~\\
%
%Важно отметить, что
%\begin{itemize}
%	\item собственные числа на выходе не отсортированы по убыванию,
%	\item собственные вектора получаются не ортогональными, в отличие от L2-SVD.
%\end{itemize}
%
%В пакете pcaMethods \cite{pcaMethods} имеется метод robustSvd для вычисления проекции в  $\mathbb{L}_1$ на множество матриц ранга, не превосходящего $r$. Более подробно метод описан в статье~\cite{pcaMethods_article}.

%\subsection{Последовательный метод} \label{sec: l1pca}
Стоит задача проектирования матрицы $\mathbf{X}$ на множество матриц ранга, не превосходящего $r$. Задачу оптимизации можно представить в виде
\begin{equation*}
\min_{\mathbf{V},\mathbf{U}} \|\mathbf{X}^{\textrm{T}}-\mathbf{V}\mathbf{U}^{\textrm{T}}\|_1=\sum_{i=1}^{L}\|X_i - \mathbf{V}U_i\|_1,
\end{equation*}
где $\mathbf{V}$ --- матрица $K \times r$, $\mathbf{U}$ --- матрица $L \times r$. Столбцы матрицы $\mathbf{V}$ определяют главные компоненты. Матрица $\mathbf{E}=\mathbf{U}$$\mathbf{V}^{\textrm{T}}$ --- проекция $\mathbf{X}$ на множество матриц ранга, не превосходящего $r$, которую необходимо найти.

В пакете pcaL1 \cite{pcaL1} имеется метод l1pca, позволяющий вычислить проекцию в $\mathbb{L}_1$ на множество матриц ранга, не превосходящего $r$. Подробнее метод описан в статье \cite{pcaL1book}.

Приведем алгоритм в наших обозначениях. Пусть $\Y \in \mathbb{R}^{L\times K}$.
Задача выглядит следующим образом: 
\begin{equation*} 
\lVert \Y - \U \V^\T \rVert_1 \to \min_{\U,\V}. 
\end{equation*}
Далее представлен алгоритм последовательного метода решения данной задачи.\\
%Предположим, что $p\le n$ и матрица $\Y$ полного ранга. \\

%\begin{enumerate}
%	\item Инициализация $\mathbf{U}(0)\in \mathbb{R}^{n\times p}$, нормировка столбцов  $\mathbf{U}(0)$,
%	\item $t:=t+1$,
%	\item  $\mathbf{V}(t) = \argminB\limits_{\mathbf{V}\in \mathbb{R}^{p\times p}} \|\mathbf{Y}-\mathbf{U}(t-1)\mathbf{V}^{\mathrm{T}}\|_1$,
%	\item  $\mathbf{U}(t) = \argminB\limits_{\mathbf{U}\in \mathbb{R}^{n\times p}} \|\mathbf{Y}-\mathbf{U}\mathbf{V}^{\mathrm{T}}(t)\|_1$,
%	\item Нормировка столбцов $\mathbf{U}(t)$,
%	\item  if $\mathbf{U}(t) \ne \mathbf{U}(t-1)$ (по критерию остановки) then Go to Step 2 \\
%	else $\mathbf{U}:=\mathbf{U}(t);~ \mathbf{V}:=\mathbf{V}(t)$.
%	
%	Критерий остановки: 
%	\begin{equation*}
%	\text{While~~} ((\max_{i,j} |u_{ij}^{(k)}-u_{ij}^{(k-1)}|>\varepsilon)\text{~и~}(\text{iter}\le \text{MaxIter})),
%	\end{equation*} по умолчанию $\varepsilon = 10^{-4}, \text{MaxIter} = 10,$
%	\item End.
%\end{enumerate}

\begin{algorithm}[H]
	\SetAlgoLined
	\KwIn{$\mathbf{Y} \in \mathbb{R}^{L\times K}$ --- траекторная матрица ряда, $r$ --- ранг сигнала;  параметры критерия остановки: $\varepsilon = 10^{-4}$, ~~~~~~ максимальное число итераций $N_{iter} = 10$}
	\KwOut{$\widehat{\mathbf{Y}} = \mathbf{U}\mathbf{V}^{\mathrm{T}}$ --- проекция траекторной матрицы на множество матриц ранга, не превосходящего $r$}
	
	1. Инициализация $\mathbf{U}(0)\in \mathbb{R}^{L\times r}$, нормировка столбцов  $\mathbf{U}(0)$\;
	2. $t:=0$\;
	3. \Repeat{ $\max \limits_{i=1,\ldots,L, j=1,\ldots,r} |u_{ij}(t)-u_{ij}(t-1)|>\varepsilon \text{~\textrm{и}~} \text{t}< N_\text{iter}$}{
		a. $t:=t+1$\;
		b. $ \mathbf{V}(t) = \argminB\limits_{\mathbf{V}\in \mathbb{R}^{K\times r}} \|\mathbf{Y}-\mathbf{U}(t-1)\mathbf{V}^{\mathrm{T}}\|_1 $ \;
		c. $\mathbf{U}(t) = \argminB\limits_{\mathbf{U}\in \mathbb{R}^{L\times r}} \|\mathbf{Y}-\mathbf{U}\mathbf{V}^{\mathrm{T}}(t)\|_1$ \;
		d. Нормировка столбцов $\mathbf{U}(t)$\;	
	}
	4. $\mathbf{U}:=\mathbf{U}(t);~ \mathbf{V}:=\mathbf{V}(t)$
	\caption{Последовательный метод построения $\mathbb{L}_1$-проектора на множество матриц ранга, не превосходящего $r$}
\end{algorithm}~\\


%\begin{algorithm}\label{alg1}
%	~~
%	\begin{enumerate}
%		\item Инициализация $\mathbf{V}(0)\in \mathbb{R}^{r\times n}$, нормировка столбцов  $\mathbf{V}(0)$,
%		\item $t:=t+1$,
%		\item  $\mathbf{U}(t) = \argminB\limits_{\mathbf{U}\in \mathbb{R}^{m\times r}} \|\mathbf{X}-\mathbf{U}\mathbf{V}^{\mathrm{T}}(t-1)\|_1$,
%		\item  $\mathbf{V}(t) = \argminB\limits_{\mathbf{V}\in \mathbb{R}^{r\times n}} \|\mathbf{X}-\mathbf{U}(t)\mathbf{V}^{\mathrm{T}}\|_1$,
%		\item Нормировка столбцов $\mathbf{V}(t)$,
%		\item  if $\mathbf{V}(t) \ne \mathbf{V}(t-1)$ then Go to Step 2 \\
%		else $\mathbf{V}:=\mathbf{V}(t);~ \mathbf{U}:=\mathbf{U}(t)$,
%		\item End.
%	\end{enumerate}
%\end{algorithm}

Решаем задачу, меняя на каждой итерации $\mathbf{U}$ и $\mathbf{V}$ и разбивая исходную задачу на линейные подзадачи. $\U(0)$ можно инициализировать с помощью сингулярного разложения траекторной матрицы $\mathbf{Y}$ в пространстве $\mathbb{L}_2$.

Рассмотрим подробнее решение задачи 
\begin{equation}\label{task1}
\mathbf{V}(t) = \argminB\limits_{\mathbf{V}\in \mathbb{R}^{K\times r}} \|\mathbf{Y}-\mathbf{U}(t-1)\mathbf{V}^{\mathrm{T}}\|_1.
\end{equation} 
Целевую функцию можно представить в виде $$\|\mathbf{Y}-\mathbf{U}(t-1)\mathbf{V}^{\mathrm{T}}\|_1 = \sum\limits_{i=1}^{K} \|Y_i-\mathbf{U}(t-1)\mathbf{v}_i \|_1, $$ где $Y_i\in \mathbb{R}^L$ --- столбцы $\mathbf{Y}$, $\mathbf{v}_i\in\mathbb{R}^r$ --- строки $\mathbf{V}$.
Согласно~\cite{KeKanade}, задача~(\ref{task1}) может быть разбита на $K$ независимых небольших подзадач 
\begin{equation}\label{subtask}
\mathbf{v}_i = \argminB\limits_{\mathbf{x}} \|Y_i-\mathbf{U}(t-1)\mathbf{x}\|_1.
\end{equation}
С помощью решения каждой такой подзадачи получаем оценку $\mathbf{v}_i$, $i=1,\ldots,K$. Глобальное решение задачи~(\ref{subtask}) находится с помощью задачи линейного программирования  
\begin{equation*}
\min_{\bm{\delta}} \mathbf{1}^\mathrm{T}\bm{\delta}
\end{equation*}
с ограничениями $$ -\bm{\delta} \le Y_i - \U(t-1)\mathbf{x} \le \bm{\delta},$$ где $\mathbf{1} \in \mathbb{R}^L$ --- вектор-столбец, состоящий из единиц. Другими словами, задача линейного программирования находит минимальную границу $\bm{\delta}$ такую, что область $$ -\bm{\delta} \le Y_i - \U(t-1)\mathbf{x} \le \bm{\delta}$$  является непустой.

Задача $\mathbf{U}(t) = \argminB\limits_{\mathbf{U}\in \mathbb{R}^{L\times r}}  \|\mathbf{Y}-\mathbf{U}\mathbf{V}^{\mathrm{T}}(t)\|_1$ решается аналогичным образом.

%Задача $\mathbf{U}(t) = \argminB\limits_{\mathbf{U}\in \mathbb{R}^{L\times r}}  \|\mathbf{Y}-\mathbf{U}\mathbf{V}^{\mathrm{T}}(t)\|_1$ может быть записана как линейная задача вида
%
%$$\sum\limits_{i=1}^K \sum\limits_{j=1}^L (\lambda_{ij}^{+} + \lambda_{ij}^{-}) \to \min\limits_{\mathbf{U},\lambda^{+}, \lambda^{-}}$$ с ограничениями
%
%$$Y_i - \mathbf{U}\mathbf{v}_i(t)+\lambda^{+}_i- \lambda^{-}_i = 0,~~~ i=1,\ldots,K,$$
%$$ \text{~~~~~~~~~~~~~~~~~~~~~~}\lambda^{+}_i, \lambda^{-}_i\ge 0,~~~ i=1,\ldots,K,$$
%
%где $Y_i\in \mathbb{R}^L$ --- столбцы $\mathbf{Y}$, $\mathbf{v}_i(t)\in\mathbb{R}^r$ --- строки $\mathbf{V}(t)$.

%\section {Сравнение методов построения $\mathbb{L}_1$-проектора}
%
%В данном разделе сравним методы из разделов~\ref{sec: rodrig} и \ref{sec: l1pca} между собой. 
%
%Метод из раздела~\ref{sec: rodrig} основан на том, чтобы в стандартном методе L2-SSA заменить сингулярное разложение на другое разложение для повышения устойчивости к выбросам. Далее берутся первые $r$ компонент данного разложения, группируются, и применяется диагональное усреднение.
%
%В статье~\cite{Hawkins}, где вводится метод robustSvd, используемый вместо обычного сингулярного разложения в методе из раздела~\ref{sec: rodrig}, нет точной формулировки задачи, решаемой данным методом. Также стоит заметить, что при взятии первых $r$ компонент разложения, мы не получим проекцию на пространство матриц ранга, не превосходящего $r$.
%
%Приведем другие важные отличия между методами.
%
%Важно отметить, что в алгоритме из раздела~\ref{sec: rodrig} поиск решения ведется последовательно для каждой компоненты, то есть по очереди находится каждая компонента, она вычитается, и далее производится поиск остальных компонент. В методе из раздела~\ref{sec: l1pca} все собственные векторы ищутся параллельно в виде матрицы. 
%
%Заметим, что в методе, использующем взвешенную медиану, собственные числа не отсортированы по убыванию. Это может привести к уменьшению точности. Например, если мы нашли неточно первые несколько компонент, вклад которых достаточно мал, а затем ищем компоненту с большим вкладом, то мы уже не так точно найдем эту компоненту.
%
%Еще одно существенное отличие в методах --- это то, что задачи минимизации целевой функции решаются по-разному. В одном варианте используется метод взвешенной медианы, а задача в последовательном методе решается с помощью решения задач линейного программирования.

\section {Построение проектора по взвешенной норме в $\mathbb{L}_2$ на множество матриц ранга, не превосходящего $r$}\label{sec: IRLS}
В данном разделе опишем метод с итеративным обновлением весов из статьи~\cite{Chen}, а также модификацию этого метода, подходящую для рядов с гетероскедастичным шумом.

\subsection{Метод с итеративным обновлением весов}
Пусть $\mathbf{Y} \in \mathbb{R}^{L\times K}$ --- траекторная матрица ряда. % Обозначим $\widehat{\mathbf{Y}} = \mathbf{U}\mathbf{V}^{\mathrm{T}} = \{\widehat{y}_{ij}\}_{i,j=1}^{L,K}$.
Необходимо решить задачу
\begin{equation*}
\norm{\mathbf{W}^{1/2}\odot(\mathbf{Y}-\mathbf{U}\mathbf{V}^{\mathrm{T}})}^2_F \longrightarrow \min_{\mathbf{U},\mathbf{V}}, 
\end{equation*}
где $\odot$ --- поэлементное умножение, $\mathbf{W}^{1/2}$ --- поэлементное взятие корня, веса $w_{ij} = w(\frac{y_{ij}-\hat{y}_{ij}}{\sigma_{ij}})$ вычисляются по формуле, как и в известном методе локальной регрессии loess, %$w(x)=\frac{\partial \rho(x)}{\partial |x|} \frac{1}{|x|}$:~~~ 
$$w(x) = 
\begin{cases}
(1-(\frac{|x|}{\alpha})^2)^2, &|x|\le\alpha\\
0, &|x|>\alpha
\end{cases}.$$ 
Значения $\alpha$ и $\{\sigma_{ij},~ i=1,\ldots,L,~j=1,\ldots,K\}$ --- параметры. График весовой функции $w(x)$ представлен на рисунке~\ref{TukeyPic}. Значение веса зависит от нормированных остатков: если остаток маленький по модулю, то вес максимальный, если остаток по модулю превосходит заданный параметр $\alpha$, то вес обнуляется.

\begin{figure}[!h]
	\center{\includegraphics[width=0.42\linewidth]{weights1}}
	\caption{График весовой функции $w(x)$.}
	\label{TukeyPic}
\end{figure}

%Идея заключается в замене исходной задачи 
%\begin{equation*}
%\min_{\hat{\mathbf{Y}} \in \mathcal{M}_r} \|\mathbf{Y}-\hat{\mathbf{Y}}\|_F.
%\end{equation*} 
%на задачу
%
%\begin{equation}\label{task1}
%\min_{\hat{\mathbf{Y}} \in \mathcal{M}_r} \|\mathbf{Y}-\hat{\mathbf{Y}}\|_\rho = \min_{\hat{\mathbf{Y}} \in \mathcal{M}_r} \sum_{i=1}^n \sum_{j=1}^p \rho(\frac{y_{ij}-\hat{y}_{ij}}{\sigma}).
%\end{equation} 

Алгоритм решения этой задачи представлен в статье~\cite{Chen}. Для начала опишем алгоритм решения задачи аппроксимации для фиксированной матрицы весов $\mathbf{W}\in \mathbb{R}^{L\times K}.$ Алгоритм \ref{alg} решает задачу \begin{equation*}
\norm{\mathbf{W}^{1/2}\odot(\mathbf{Y}-\mathbf{U}\mathbf{V}^{\mathrm{T}})}^2_F \longrightarrow \min_{\mathbf{U},\mathbf{V}}, 
\end{equation*} при фиксированной $\mathbf{W}$. \\

%Опишем метод, представленный в статье~\cite{Chen}, использующий для решения задачи~\ref{task1} взвешенный метод наименьших квадратов с обновлением весов на каждой итерации. Веса должны зависеть от того, насколько большой остаток в точке. В качестве $\rho(x)$ возьмем функцию Тьюки, которая имеет вид

% \begin{equation}\label{Tukey}
% \rho (x) = 
% \begin{cases}
%\frac{1}{6}\alpha^2\{1-(1-(\frac{|x|}{\alpha})^2)^3\}, &|x|\le\alpha\\
%\frac{1}{6}\alpha^2, &|x|>\alpha
%\end{cases},
% \end{equation} 
%где $\alpha$ --- параметр. Причина выбора такой функции в качестве метрики заключается в том, что на краях она не так сильно возрастает, как квадратичная функция, а точнее, данная функция остается постоянной при $|x|>\alpha$, что приводит к более устойчивому к выбросам результату.
%
%Представим матрицу $\hat{\mathbf{Y}}$ в виде $\hat{\mathbf{Y}}=\mathbf{U}\mathbf{V}^{\textrm{T}}$, где $\mathbf{U}\in \mathbb{R}^{n\times r}, \mathbf{V}\in \mathbb{R}^{p\times r} $. Задача нахождения проекции на множество матриц ранга, не превосходящего $r$, сводится к задаче 
%
%\begin{equation*}
%\min_{\mathbf{U},\mathbf{V}} \|\mathbf{Y}-\mathbf{U}\mathbf{V}^{\textrm{T}}\|_\rho.
%\end{equation*} 
%
%
%
%Метод имеет параметры $\alpha$ и $\sigma$. Их произведение $\alpha \sigma $ по сути является порогом для принятия решения о том, является ли наблюдение выбросом или нет. Опишем алгоритм. \\

%\begin{enumerate}
%	\item Инициализация $\mathbf{U}\in \mathbb{R}^{n\times r}$ и $\mathbf{V}\in \mathbb{R}^{p\times r}$,
%	\item Выбор параметра $\alpha$,
%	\item  Вычисление матрицы остатков $\mathbf{R}=\{r_{ij}\}_{i,j=1}^{n,p} = \mathbf{Y}-\mathbf{U}\mathbf{V}^{\mathrm{T}}$,
%	\item  Обновление параметра $\sigma$,
%	\item Вычисление матрицы весов $\mathbf{W}$, используя функцию $w(x)=\frac{\partial \rho(x)}{\partial |x|} \frac{1}{|x|}$:
%	 \begin{equation*}
%	w(x) = 
%	\begin{cases}
%	(1-(\frac{|x|}{\alpha})^2)^2, &|x|\le\alpha\\
%	0, &|x|>\alpha
%	\end{cases}, ~~~~\text{где}~ x=\frac{1}{\sigma}r_{ij},
%	\end{equation*} 
%	то есть для каждого элемента матрицы $\frac{1}{\sigma}\mathbf{R}$ применяем функцию $w(x)$,
%	\item Вычисление матрицы $\mathbf{U}$ с помощью решения задачи
%	\begin{equation}\label{taskA}
%	(y_i-\mathbf{V}u_i)^\mathrm{T} \mathbf{W}_i (y_i-\mathbf{V}u_i) \to \min_{u_i},~~ i=1,\ldots n,
%	\end{equation} 
%	где $\mathbf{W}_i=\diag(w_i)\in \mathbb{R}^{p\times p}$ --- диагональная матрица, составленная из $i$-ой строки матрицы $\mathbf{W}$.
%	\item Вычисление матрицы $\mathbf{V}$ с помощью решения задачи
%	\begin{equation}\label{taskB}
%	(Y_j-\mathbf{U}v_j)^\mathrm{T} \mathbf{W}^j (Y_j-\mathbf{U}v^j) \to \min_{v_j},,~~ j=1,\ldots p,
%	\end{equation} 
%	где $\mathbf{W}^j=\diag(W_j)\in \mathbb{R}^{n\times n}$ --- диагональная матрица, составленная из $j$-го столбца матрицы $\mathbf{W}$.
%	\item Если не выполнен критерий сходимости или максимальное число итераций $\text{MaxIterAM}$ не достигнуто, то повторяем шаги 6--7 (alternating minimization), 
%	\item Если не выполнен критерий сходимости или максимальное число итераций $\text{MaxIterIRLS}$ не достигнуто, то повторяем шаги 2--8 (iteratively reweighed least-squares), 
%%	\item  if $\norm{\mathbf{W}^{1/2}\odot(\mathbf{Y}-\mathbf{U}\mathbf{V}^{\mathrm{T})}}^2_F \le \varepsilon_1$ and $iter \le \text{maxiter}$ then Go to Step 6, \\
%%	\item  if $\norm{\mathbf{W}^{1/2}\odot(\mathbf{Y}-\mathbf{U}\mathbf{V}^{\mathrm{T})}}^2_F \le \varepsilon_2$ and $ITER \le \text{MAXITER}$ then Go to Step 3, \\
%	\item End.
%\end{enumerate}

\begin{algorithm}[H]\label{alg}
	\SetAlgoLined
	\KwIn{$\mathbf{Y} \in \mathbb{R}^{L\times K}$ --- траекторная матрица ряда, $r$ --- ранг сигнала,  $\mathbf{W} \in \mathbb{R}^{L\times K}$ --- матрица весов; ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ параметры критерия остановки:  $\varepsilon = 10^{-4}$, ~~~~~~~~~~~~~ максимальное число итераций $N_\alpha = 5$}
	\KwOut{$\widehat{\mathbf{Y}} = \mathbf{U}\mathbf{V}^{\mathrm{T}}$ --- решение задачи взвешенной аппроксимации при фиксированной матрице весов $\mathbf{W}$}
	
	1. $t:=0$\;
	2. \Repeat{ $\norm{\mathbf{W}^{1/2}\odot(\mathbf{Y}-\mathbf{U}\mathbf{V}^{\mathrm{T})}}^2_F > \varepsilon\text{~и~} \text{t}< N_\alpha$}{
		a. Вычисление матрицы $\mathbf{U}\in \mathbb{R}^{L\times r}$ с помощью решения задачи
		\begin{equation}\label{taskA}
		(y_i-\mathbf{V}u_i)^\mathrm{T} \mathbf{W}_i (y_i-\mathbf{V}u_i) \to \min_{u_i},~~ i=1,\ldots L,
		\end{equation} 
		где $\mathbf{W}_i=\diag(w_i)\in \mathbb{R}^{K\times K}$ --- матрица, составленная из $i$-ой строки $\mathbf{W}$\;
		b. Вычисление матрицы $\mathbf{V}\in \mathbb{R}^{K\times r}$ с помощью решения задачи
		\begin{equation}\label{taskB}
		(Y_j-\mathbf{U}v_j)^\mathrm{T} \mathbf{W}^j (Y_j-\mathbf{U}v_j) \to \min_{v_j},,~~ j=1,\ldots K,
		\end{equation} 
		где $\mathbf{W}^j=\diag(W_j)\in \mathbb{R}^{L\times L}$ --- матрица, составленная из $j$-го столбца $\mathbf{W}$\;
		c. $t:=t+1$.	
	}
	\caption{Алгоритм решения задачи взвешенной аппроксимации для фиксированной матрицы весов $\mathbf{W}$}
\end{algorithm}
~~\\
\begin{notice}
	1. Задача~\ref{taskA} решается с помощью QR-разложения матрицы $\mathbf{V}^\mathrm{T}\mathbf{W}_i\mathbf{V}$.
	2.  Задача~\ref{taskB} решается с помощью QR-разложения матрицы $\mathbf{U}^\mathrm{T}\mathbf{W}^j\mathbf{U}$.
\end{notice}

Далее представлен алгоритм решения задачи построения проектора на множество матриц ранга, не превосходящего $r$, методом с итеративным обновлением весов. Алгоритм содержит вычисление вспомогательной матрицы $\mathbf{\Sigma} = \{\sigma_{ij}\}_{i,j=1}^{L,K}$, которое обсудим далее в разделе~\ref{sigma}. \\

\begin{algorithm}[H]
	\SetAlgoLined
	\KwIn{$\mathbf{Y} \in \mathbb{R}^{L\times K}$ --- траекторная матрица ряда, $r$ --- ранг сигнала; параметры критерия остановки: $\varepsilon = 10^{-4}$, ~~~~~~~~~~~~~ максимальное число итераций $N_{IRLS} = 10$}
	\KwOut{$\widehat{\mathbf{Y}} = \mathbf{U}\mathbf{V}^{\mathrm{T}}$ --- проекция траекторной матрицы на множество матриц ранга, не превосходящего $r$}
	
	1. Инициализация $\mathbf{U}\in \mathbb{R}^{n\times r}$ и $\mathbf{V}\in \mathbb{R}^{p\times r}$ (например, с помощью сингулярного разложения матрицы $\mathbf{Y}$)\;
	2. Выбор параметра $\alpha$\;
	3. $t:=0$\;	
	4. \Repeat{ $\norm{\mathbf{W}^{1/2}\odot(\mathbf{Y}-\mathbf{U}\mathbf{V}^{\mathrm{T}})}^2_F > \varepsilon \text{~и~} t< N_{IRLS}$}{
		a. Вычисление матрицы остатков $\mathbf{R}=\{r_{ij}\}_{i,j=1}^{n,p} = \mathbf{Y}-\mathbf{U}\mathbf{V}^{\mathrm{T}}$\;
		b. Обновление матрицы $\mathbf{\Sigma} = \{\sigma_{ij}\}_{i,j=1}^{L,K}$\;
		c. Вычисление матрицы весов $\mathbf{W} = \{w_{ij}\}_{i,j=1}^{L,K} = \{w(\frac{r_{ij}}{\sigma_{ij}})\}_{i,j=1}^{L,K} $, используя %$w(x)=\frac{\partial \rho(x)}{\partial |x|} \frac{1}{|x|}$:
		\begin{equation*}
		w(x) = 
		\begin{cases}
		(1-(\frac{|x|}{\alpha})^2)^2, &|x|\le\alpha\\
		0, &|x|>\alpha
		\end{cases},% ~~~~\text{где}~ x=\frac{1}{\sigma}r,
		\end{equation*} 
		d. Решение задачи взвешенной аппроксимации (обновление матриц $\mathbf{U}$, $\mathbf{V}$)
		\begin{equation*}
		\norm{\mathbf{W}^{1/2}\odot(\mathbf{Y}-\mathbf{U}\mathbf{V}^{\mathrm{T}})}^2_F \longrightarrow \min_{\mathbf{U},\mathbf{V}}
		\end{equation*}	с помощью алгоритма~\ref{alg}\;
		e. $t:=t+1$.
	}
	\caption{Метод с итеративным обновлением весов для нахождения проекции на множество матриц ранга, не превосходящего $r$}
\end{algorithm}
~\\

Авторы статьи~\cite{Chen}, ссылаясь на проведенные численные эксперименты, предлагают $\forall~ i,j$ выбрать $\sigma_{ij} = \sigma = 1.4826 \med {|\tX{R}-\med {|\tX{R}|}|}$, где $\tX{R}$ --- это вектор, составленный из всех элементов матрицы остатков $\mathbf{R} = \{r_{ij}\}_{i,j=1}^{L,K}$, то есть
\begin{equation*}
\tX{R}~=~(r_{11},\ldots,r_{1K}; r_{21},\ldots r_{2K};\ldots;r_{L1},\ldots,r_{LK}).
\end{equation*} 
Параметр $\alpha$ предлагается взять равным $4.685$. Также говорится, что максимальное количество итераций $N_{\alpha}$ и $N_{IRLS}$, необходимых для сходимости, достаточно взять 5 и 10 для достижения приемлемой точности.

Инициализировать матрицы $\mathbf{U}$ и $\mathbf{V}$ можно с помощью первых $r$ компонент сингулярного разложения матрицы $\mathbf{Y}$. Будем использовать один из вариантов truncated SVD из пакета svd~\cite{svd}, который находит только заданное число компонент, не вычисляя полное разложение. Подробнее о методе говорится в статье~\cite{svd_article}.  
%Критерий сходимости:
%\begin{equation*}
%\norm{\mathbf{W}^{1/2}\odot(\mathbf{Y}-\mathbf{U}\mathbf{V}^{\mathrm{T})}}^2_F \le \varepsilon.
%\end{equation*} 

\subsection{Выбор параметра $\sigma_{ij}$}\label{sigma}
У выбора $\sigma_{ij}$ не зависящими от $i,j$ присутствуют существенные недостатки. Описанный алгоритм из статьи не подходит, к примеру, для рядов с гетероскедастичным шумом. По умолчанию остатки нормировались на $\sigma_{ij}$, которая задавалась константой. Нормировка остатков на константный параметр $\sigma_{ij} = \sigma ~~ \forall i,j$ в случае шума с непостоянной дисперсией приводит к неправильной идентификации точек с выбросами. Если шум растет к концу ряда, то веса у всех значений на конце ряда некорректно занижаются, и точки, не содержащие выбросов, могут получить вес, меньший, чем у выбросов в начале ряда. Поэтому приходим к выводу, что нормирующий параметр необходимо задавать динамически. 

Будем рассматривать матрицу $\mathbf{\Sigma}=\{\sigma_{ij}\}_{i,j=1}^{L,K}$ ганкелевой, что соответствует приписыванию весов элементам ряда. Тогда элементы на диагоналях $i+j=\mathrm{const}$ матрицы $\mathbf{\Sigma}$ равны между собой, и можем обозначить $\sigma_{i+j-1} = \sigma_{lk}$ $~~\forall l,k:~l+k=i+j$, где $i=1,\ldots,L, ~j=1,\ldots,K$. Обозначим новый параметр $\bm{\sigma} = (\sigma_1,\ldots,\sigma_N)^\mathrm{T}$. Будем задавать параметр $\bm{\sigma}$ как тренд (математическое ожидание) ряда, состоящего из модулей остатков. Для оценки математического ожидания как тренд из ряда модулей остатков будем предполагать медленную зависимость модуля остатков от индекса. Выделять тренд будем следующими способами: локальной регрессией loess, скользящей медианой или взвешенной локальной регрессией lowess. 

\subsection{Выбор параметра $\alpha$}\label{alpha}
У метода с итеративным обновлением весов есть параметр $\alpha$, который влияет на то, какие точки будем считать выбросами, а какие --- нет. Для того, чтобы понять, какое значение следует взять в качестве $\alpha$, выведем вероятностную формулу для этого параметра. Будем задавать вероятность $\gamma$ и подбирать параметр $\alpha$ так, чтобы нормированные модули остатков попадали в промежуток $(0;\alpha)$ с вероятностью $\gamma$. Для вывода вероятностной формулы для параметра $\alpha$ введем следующее определение.

\begin{def1}
	Если $r \sim N(0, \sigma^2)$, то $|r| \sim N_{H}(\sigma^2)$ --- полунормальное распределение с параметром $\sigma^2$, функция распределения:
	\begin{equation}\label{halfnormal_function}
	F_H(x;\sigma^2) = \frac{2}{\sqrt{\pi}}\int\limits_{0}^{x/\sqrt{2}\sigma^2} e^{-z^2} dz = \mathrm{erf} (\frac{x}{\sqrt{2}\sigma^2}).
	\end{equation}
	Среднее и дисперсия:
	$\mathbb{E}|r|=\sigma \sqrt{\frac{2}{\pi}}, ~~~ \mathbb{D}|r|=\sigma^2 (1-\frac{2}{\pi})$.
\end{def1}

%Пусть $R=(r_1,\ldots,r_q)^\mathrm{T}$ --- вектор остатков, где $q=mn$. 
%Если остатки $r_i \sim N(0, \sigma^2)$, то $|r_i| \sim N_{H}(\sigma^2)$, где $N_{H}(\sigma^2)$ --- полунормальное распределение с параметром $\sigma^2$. Пусть $\mathbb{E}|r_i| = \mu$. Для полунормального распределения известны следующие формулы для среднего и дисперсии:
%\begin{equation*}
%\mathbb{E}|r_i|=\sigma \sqrt{\frac{2}{\pi}}, ~~~ \mathbb{D}|r_i|=\sigma^2 (1-\frac{2}{\pi}) = C\mu^2, ~~ \text{где}~ C=\text{const}.
%\end{equation*}

%В алгоритме при вычислении весов проводится сравнение $|\frac{R}{\sigma}|$ с константой $\alpha$.  Если модуль превосходит $\alpha$, то веса обнуляются. Чем ближе значение модуля к $\alpha$, тем ниже вес в данной точке. Необходимо как-то осмысленно выбирать данный параметр. Выбор параметра $\alpha$ и его вероятностная интерпретация представлены в разделе~\ref{alpha}.
%Но так как в конце ряда разброс остатков больше, чем в начале, то при задании $\alpha$ константой, многие точки получают маленькие веса. Поэтому необходимо либо задавать пороговое значение динамически, либо ввести дополнительное преобразование, чтобы отрегулировать разброс остатков в начале и конце ряда. Квадратичная связь дисперсии и среднего модуля остатков наталкивает на мысль брать логарифм модулей остатков перед тем, как проводить сравнение с $\alpha$. Однако очень маленькие значения остатков могут привести к большим отрицательным значениям логарифма, поэтому попробуем для начала извлекать корень из ряда модулей остатков. 


%Имеем ряд $\tX{X}=\tX{S}+\tX{\mathcal{E}}$, то есть $x_i=s_i+\varepsilon_i$, где $\varepsilon_i \sim N(0,\sigma_{\epsilon}^2)$. Пусть мы знаем сигнал $\tX{S}$, тогда остатки $\tX{R}$ соответствуют ряду $\mathcal{E}$.
%
%Попробуем описать параметр $\alpha$ на вероятностном языке. Этот параметр является неким порогом, обнуляющим веса элементов матрицы остатков $\mathbf{R}^{*} = \{r_{ij}^*\}_{i,j=1}^{n,p} = \mathcal{T} (|\sigma^{-1}\tX{R}|)$:
%\begin{equation*}
%w(r_{ij}^*) = 
%\begin{cases}
%(1-(\frac{|r_{ij}^*|}{\alpha})^2)^2, &|r_{ij}^*|\le\alpha\\
%0, &|r_{ij}^*|>\alpha
%\end{cases}.
%\end{equation*} 

После ганкелизации матрицы остатков $\mathbf{R}=\mathbf{Y}-\U\V^\mathrm{T}$, получаем ряд из остатков $\tX{R}=\{r_i\}_{i=1}^N$.  Напомним, что мы рассматриваем ряд вида $$x_i = s_i + \varepsilon_i, ~~ i=1,\ldots,N,$$ где $s_i$ --- сигнал, $\varepsilon_i$ --- шум. Если предположить отделимость сигнала от шума, то матрица $\mathbf{R}$ соответствует траекторной матрице шума $\{\varepsilon_i\}_{i=1}^N$. Для случайного шума точная отделимость от сигнала невозможна, однако будем предполагать разделимость в дальнейших рассуждениях. Тогда в предположении точной отделимости сигнала от шума ряд из остатков соответствует шуму, то есть $\tX{R}=\{r_i\}_{i=1}^N=\{\varepsilon_i\}_{i=1}^N$.
Обозначим $r^*=\frac{|\varepsilon|}{\sigma}$, где $\bm{\sigma} =(\sigma_1,\ldots,\sigma_N)$ --- тренд из ряда $|\tX{R}|$. В предположении точной отделимости это означает, что $\sigma_i = \mathbb{E}|\varepsilon_i|$.

Зададим вероятность $\gamma$: $\mathrm{P}(r^*\in (0,\alpha))=\gamma$. Распишем:

\begin{equation*}
\mathrm{P}(r^*\in (0,\alpha))=\mathrm{P}(0\le \frac{|\varepsilon|}{\sigma} \le \alpha) = \mathrm{P}(\frac{|\varepsilon|}{\sigma} \le \alpha) = \gamma.
\end{equation*}

Для того, чтобы получить выражение для $\alpha$, нам необходимо знать распределение $\frac{|\varepsilon|}{\sigma}$.

\begin{proposition}\label{lemma1}
	Пусть $\varepsilon_i\sim N(0,\sigma_{\varepsilon}^2)$. Пусть $\sigma=\mathbb{E} |\varepsilon|$. Тогда $r^{*}=\frac{|\varepsilon|}{\sigma}$  имеет полунормальное распределение $N_h(\frac{\pi}{2}),$ среднее $\mathbb{E} r^* = 1,$ дисперсия $\mathbb{D} r^* = \frac{\pi}{2}-1$.
\end{proposition}
\begin{proof}
	Если $\varepsilon_i \sim N(0,\sigma_{\varepsilon}^2)$, то 
	\begin{equation*}
	\frac{\varepsilon}{\sigma} = \frac{\varepsilon}{\EE|\varepsilon|} \sim N(0,\frac{\sigma^2_{\varepsilon}}{\sigma^2_{\varepsilon}\frac{2}{\pi}}) = N(0,\frac{\pi}{2}).
	\end{equation*} 
	Тогда $\frac{|\varepsilon|}{\sigma} \sim N_h(\frac{\pi}{2})$ по определению полунормального распределения.
	
	Посчитаем среднее и дисперсию $r^*$.
	\begin{equation*}
	r^*=\frac{|\varepsilon|}{\sigma}=\frac{|\varepsilon|}{\EE|\varepsilon|} = \frac{|\varepsilon|}{\sigma_{\varepsilon}\sqrt{\frac{2}{\pi}}}.
	\end{equation*}
	Тогда
	\begin{equation*}
	\EE{r^*} = \frac{\EE|\varepsilon|}{\sigma_{\varepsilon}\sqrt{\frac{2}{\pi}}} = 1, ~~~~~
	\DD{r^*} = \frac{\DD|\varepsilon|}{\sigma_{\varepsilon}^2 \frac{2}{\pi}} = \frac{\sigma_{\varepsilon}^2(1-\frac{2}{\pi})}{\sigma_{\varepsilon}^2 \frac{2}{\pi}} = \frac{\pi}{2}-1.
	\end{equation*}
\end{proof}	

Из утверждения~\ref{lemma1} следует, что уравнение $\mathrm{P}(\frac{|\varepsilon|}{\sigma} \le \alpha) = \gamma$ перепишется в виде $F_h(\alpha) = \gamma$, где $F_h$ --- функция распределения $N_h(\frac{\pi}{2})$.

Используя формулу~(\ref{halfnormal_function}) для функции распределения полунормального распределения, получаем уравнение
\begin{equation*}
\text{erf}(\frac{\alpha}{\sqrt{2}\frac{\pi}{2}}) = \gamma,
\end{equation*}
где $\text{erf}$ --- функция ошибок, которая имеется во многих пакетах в R. Отсюда получаем вероятностную формулу для параметра $\alpha$
\begin{equation}\label{alpha_gamma}
\alpha = \frac{\sqrt{2}\pi}{2} \text{erf}^{-1}(\gamma).
\end{equation}
К примеру, для $\gamma = 0.95$ получим $\alpha \approx 3.079$. Для $\gamma = 0.99$ получим $\alpha \approx 4.046$.
%Пусть теперь шум $\xi_i$ гетероскедастичный, $\xi_i = t_i\varepsilon_i,$ где $\varepsilon_i \sim N(0,\sigma^2_{\varepsilon})$, то есть дисперсия шума $\xi_i$ уже не равна константе. Тогда снова $r^* \sim N_h(\frac{\pi}{2})$ и дисперсия $r^*$ постоянна, тогда все рассуждения остаются верными в предположениях нормальности шума.

На основе утверждения~\ref{lemma1} можно можно провести следующие рассуждения для временных рядов:
\begin{notice} 
	1. Пусть дисперсия шума постоянна, $x_i = s_i + \varepsilon_i, ~ \varepsilon_i\sim N(0,\sigma_{\varepsilon}^2)$, сигнал $s_i$ точно отделим от шума. Пусть $\sigma=\EE |\varepsilon|$. Тогда $r^{*}=\frac{|\varepsilon|}{\sigma}$  имеет полунормальное распределение $N_h(\frac{\pi}{2})$ среднее $\mathbb{E} r^* = 1,$ дисперсия $\mathbb{D} r^* = \frac{\pi}{2}-1$. \\
	2. Пусть шум гетероскедастичный, $x_i = s_i + t_i\varepsilon_i,$ $t_i>0$, $\varepsilon_i\sim N(0,1)$, сигнал $s_i$ точно отделим от шума. Пусть $\tX{R}=(r_1,\ldots,r_N)^\mathrm{T}$ --- вектор из остатков, $\tX{R}_+=(|r_1|, \ldots, |r_N|)^\mathrm{T}$ --- вектор из модулей остатков, $\bm{\sigma}=(\sigma_1,\ldots,\sigma_N)^\mathrm{T} = \mathbb{E}(\tX{R}_+)$. Тогда каждая компонента вектора $\bm{r}^*=(\frac{|r_1|}{\sigma_1}, \ldots, \frac{|r_N|}{\sigma_N})^\mathrm{T}$ имеет полунормальное распределение: $r_i^* = \frac{|r_i|}{\sigma_i} \sim N_H(\frac{\pi}{2})$. В таком случае $\EE r_i^* = 1,$ $\DD r_i^* = \frac{\pi}{2}-1$. 
\end{notice}
Другими словами, получаем, что и в случае шума постоянной дисперсии, и в случае гетероскедастичного шума, в предположении нормальности шума $r^*$ имеет полунормальное распределение $N_H(\frac{\pi}{2})$.

%\begin{proof}
%	
%	Если сигнал $s_i$ отделим от шума, то остатки $R=t_i\varepsilon_i$.
%	
%	Известно, что если $\varepsilon \sim N(0,\sigma_\varepsilon^2),$ то $\mathbb{E}|\varepsilon| = \sigma_\varepsilon \sqrt{\frac{2}{\pi}}$, $\mathbb{D}|\varepsilon| = \sigma_\varepsilon^2 (1-\frac{2}{\pi}).$
%	%$$\mathbb{E}|R| = Ae^{an} \mathbb{E} |\varepsilon| = Ae^{an}\sqrt{\frac{2}{\pi}}.$$
%	Выпишем нормированные остатки: $\frac{r}{\sigma} = \frac{t_i\varepsilon}{t_i\mathbb{E}|\varepsilon|} \sim N(0,\frac{1}{\frac{2}{\pi}}) = N(0,\frac{\pi}{2}).$
%	
%	Тогда получим следующее: $\frac{|r|}{\sigma} \sim  N_h(\frac{\pi}{2})$ по определению полунормального распределения.
%	
%	
%	$$|r^*|=\frac{|t_i\varepsilon|}{\sigma}=\frac{t_i|\varepsilon|}{t_i\mathbb{E}|\varepsilon|} = \frac{|\varepsilon|}{\sqrt{\frac{2}{\pi}}}.$$ Тогда (аналогично шуму с постоянной дисперсией) посчитаем среднее и дисперсию: $$\mathbb{E}{r^*} = \frac{\mathbb{E}|\varepsilon|}{\sqrt{\frac{2}{\pi}}} = 1,~~   \mathbb{D}{r^*} = \frac{\mathbb{D}|\varepsilon|}{\frac{2}{\pi}} = \frac{(1-\frac{2}{\pi})}{\frac{2}{\pi}} = \frac{\pi}{2}-1.$$
%\end{proof}

\subsection{Модификация метода с итеративным обновлением весов}

Далее представлена модификация алгоритма, подходящая для рядов с гетероскедастичным шумом. \\

%\begin{enumerate}
%	\item [4.a]  Ганкелизация матрицы $\mathbf{R}$ и получение ряда длины $N$ из остатков: $\tX{R} = \mathcal{T}^{-1} \Pi_{\mathcal{H}} (\mathbf{R})$,
%	\item [4.b]  Вычисление $\sigma= (\sigma_1,\ldots,\sigma_{N})$ как тренд из ряда $|\tX{R}|$,
%	\item [4.c]  Вычисление ряда $|\sigma^{-1}\tX{R}|$ и получение матрицы $\mathbf{R}^{*} = \{r_{ij}^*\}_{i,j=1}^{n,p} = \mathcal{T} (|\sigma^{-1}\tX{R}|)$,
%	\item [5.] Вычисление матрицы весов $\mathbf{W}$, используя функцию $w(x)=\frac{\partial \rho(x)}{\partial |x|} \frac{1}{|x|}$:
%	\begin{equation*}
%	w(x) = 
%	\begin{cases}
%	(1-(\frac{|x|}{\alpha})^2)^2, &|x|\le\alpha\\
%	0, &|x|>\alpha
%	\end{cases},
%	\end{equation*} 
%	где $x=r_{ij}^*$.
%\end{enumerate}
\small{
\begin{algorithm}[H]
	\SetAlgoLined
	\KwIn{$\mathbf{Y} \in \mathbb{R}^{L\times K}$ --- траекторная матрица ряда, $r$ --- ранг сигнала;
		параметры критерия остановки: $\varepsilon = 10^{-4}$, ~~~~~~~~~~~~~ максимальное число итераций $N_{IRLS} = 10$}
	\KwOut{$\widehat{\mathbf{Y}} = \mathbf{U}\mathbf{V}^{\mathrm{T}}$ --- проекция траекторной матрицы на множество матриц ранга, не превосходящего $r$}
	
	1. Инициализация $\mathbf{U}\in \mathbb{R}^{n\times r}$ и $\mathbf{V}\in \mathbb{R}^{p\times r}$ (например, с помощью сингулярного разложения матрицы $\mathbf{Y}$)\;
	2. Выбор параметра $\alpha$\;	
	3. $t:=0$\;
	4. \Repeat{ $\norm{\mathbf{W}^{1/2}\odot(\mathbf{Y}-\mathbf{U}\mathbf{V}^{\mathrm{T}})}^2_F > \varepsilon \text{~и~} t < N_{IRLS}$}{
		a. Вычисление матрицы остатков $\mathbf{R}=\{r_{ij}\}_{i,j=1}^{n,p} = \mathbf{Y}-\mathbf{U}\mathbf{V}^{\mathrm{T}}$\;
		b. Ганкелизация матрицы $\mathbf{R}$ и получение ряда длины $N$ из остатков: $\tX{R} = \mathcal{T}^{-1} \Pi_{\mathcal{H}} (\mathbf{R}) = (r_1,\ldots,r_N)^\mathrm{T}$\;
		c. Пусть $\tX{R}_+=(|r_1|, \ldots, |r_N|)^\mathrm{T}$ --- вектор из модулей остатков.  Вычисление $\bm{\sigma} = (\sigma_1,\ldots,\sigma_{N})^\mathrm{T}$ как оценки мат. ожидания $\mathbb{E}(\tX{R}_+)$ некоторым методом: локальной регрессией loess, скользящей медианой или взвешенной локальной регрессией lowess (подробнее оценка мат. ожидания обсуждалась в пункте~\ref{sigma})\;
		d. Вычисление ряда $|\bm{\sigma}^{-1}\tX{R}| = (\frac{|r_1|}{\sigma_1},\ldots,\frac{|r_N|}{\sigma_N})^\mathrm{T}$ и получение матрицы $\mathbf{R}^{*} = \{r_{ij}^*\}_{i,j=1}^{L,K} = \mathcal{T} (|\bm{\sigma}^{-1}\tX{R}|)$\;
		e. Вычисление матрицы весов $\mathbf{W}= \{w_{ij}\}_{i,j=1}^{L,K} = \{w(r_{ij}^*)\}_{i,j=1}^{L,K}$, используя %$w(x)=\frac{\partial \rho(x)}{\partial |x|} \frac{1}{|x|}$:
		\begin{equation*}
		w(x) = 
		\begin{cases}
		(1-(\frac{|x|}{\alpha})^2)^2, &|x|\le\alpha\\
		0, &|x|>\alpha
		\end{cases}\; %~~~ \text{где}~ x=r^*.
		\; \end{equation*} 
		f. Решение задачи взвешенной аппроксимации (обновление матриц $\mathbf{U}$ и $\mathbf{V}$)
		\begin{equation*}
		\norm{\mathbf{W}^{1/2}\odot(\mathbf{Y}-\mathbf{U}\mathbf{V}^{\mathrm{T}})}^2_F \longrightarrow \min_{\mathbf{U},\mathbf{V}}\;
		\end{equation*}	
		g. $t:=t+1$.
	}
	\caption{Модификация метода с итеративным обновлением весов для нахождения проекции на множество матриц ранга, не превосходящего $r$}
\end{algorithm}
}
\normalsize{
Посмотрим на график весов, чтобы убедиться, что только точки, содержащие выбросы, получили маленькие веса. На рисунках~\ref{series_IRLS} и~\ref{weights_IRLS} изображен график ряда c 5\% выбросов и веса, получившиеся в результате применения модификации метода. На графике весов видно, что точки, в которых содержались выделяющиеся наблюдения, получили нулевые веса. В остальных точках веса колеблются от 0.8 до 1.

\begin{figure}[h]
	\begin{center}
		\begin{minipage}[h]{0.48\linewidth}
			\includegraphics[width=1\linewidth]{series_IRLS}
			\caption{График ряда с 5\% выбросов.} %% подпись к рисунку
			\label{series_IRLS} %% метка рисунка для ссылки на него
		\end{minipage}
		\hfill 
		\begin{minipage}[h]{0.48\linewidth}
			\includegraphics[width=1\linewidth]{weights_IRLS}
			\caption{Веса.}
			\label{weights_IRLS}
		\end{minipage}
	\end{center}
\end{figure}

%\subsection{Исследование ряда из модулей остатков}
%
%В исходном алгоритме при вычислении матрицы весов $\mathbf{W}$ функция $w(x)$ применяется к каждому элементу матрицы $\frac{1}{\sigma}\mathbf{R}$, которая представляет из себя нормированную на определенную константу матрицу остатков. В модификации метода мы заменили константу $\sigma$ на ряд $\sigma=(\sigma_1,\ldots,\sigma_N)$, равный тренду ряда из модулей остатков. 
%
%Далее необходимо проверить, является ли модель ряда из модулей нормированных остатков $|\sigma^{-1}R|$ аддитивной или мультипликативной.  Пусть у нас есть ряд
%\begin{equation*}
%x_i=s_i+s_i^\beta\varepsilon_i,
%\end{equation*}
%где $s_i$ --- сигнал, $\varepsilon_i$ --- шум. Необходимо оценить параметр $\beta$. Если $\beta=0$, то модель является аддитивной. Если $\beta \ne 0$ (например, $\beta=1$), то модель мультипликативная. Для этого воспользуемся тестом Парка, описанным в статье~\cite{Park}. За оценку $\beta$ мы можем взять оценку по методу наименьших квадратов линейной модели 
%\begin{equation*}
%\log(|\widehat{e}_i|) = \beta \log(|\widehat{s}_i|) + \delta_i,
%\end{equation*}
%где $\widehat{s}_i$ --- оценка сигнала, $\widehat{e}_i = x_i-\widehat{s}_i$ --- остатки.

\subsection{Модификация метода с итеративным обновлением весов с использованием метода Гаусса-Ньютона}
Заметим, что использование ганкелевой матрицы $\{\sigma_{ij}\}_{i,j=1}^{L,K}$ и ее замена на параметр $\bm{\sigma} = (\sigma_1,\ldots,\sigma_N)^\mathrm{T}$ соответствует приписыванию весов элементам ряда. Однако в рассмотренной модификации после нормировки модуля остатков на параметр $\bm{\sigma}$ и вычисления матрицы весов, мы приписываем веса элементам траекторной матрицы ряда и вычисляем проекцию на множество матриц ранга, не превосходящего $r$. Возникает идея приписывать веса самому ряду, а не матрице, и находить оценку сигнала с этими весами.

Для нахождения проекции на множество рядов ранга, не превосходящего $r$, по взвешенной норме можно использовать модифицированный метод Гаусса-Ньютона, описанный в статье~\cite{MGN}, который итеративно решает задачу 
\begin{equation*}
\tX{X}^* = \argminB_{\tX{X}\in\bar{\mathcal{D}}_r} \norm{\tX{Y}-\tX{X}}_{\mathbf{W}},
\end{equation*}	
где $\tX{Y}$ --- ряд длины $N$, $\bar{\mathcal{D}}_r$ --- замыкание множества рядов ранга, не превосходящего $r$, $\mathbf{W} \in \mathbb{R}^{N\times N}$ --- матрица весов, $\norm{\tX{Y}}_{\mathbf{W}} = \tX{Y}^\mathrm{T}\mathbf{W}\tX{Y}$. 

Ниже представлен алгоритм модификации метода с итеративным обновлением весов с использованием метода Гаусса-Ньютона. \\
}

\begin{algorithm}[H]
	\SetAlgoLined
	\KwIn{$\tX{Y}$ --- ряд, $\mathbf{Y} \in \mathbb{R}^{L\times K}$ --- траекторная матрица ряда $\tX{Y}$, $r$ --- ранг сигнала;
		параметры критерия остановки: $\varepsilon = 10^{-4}$, ~~~~~~~~~~~~~ максимальное число итераций $N_{IRLS} = 10$}
	\KwOut{$\widehat{\tX{Y}}$ --- проекция ряда $\tX{Y}$ на множество рядов ранга, не превосходящего $r$}
	
	1. Инициализация $\mathbf{U}\in \mathbb{R}^{n\times r}$ и $\mathbf{V}\in \mathbb{R}^{p\times r}$ (например, с помощью сингулярного разложения матрицы $\mathbf{Y}$)\;
	2. Выбор параметра $\alpha$\;	
	3. Вычисление матрицы остатков $\mathbf{R}=\{r_{ij}\}_{i,j=1}^{n,p} = \mathbf{Y}-\mathbf{U}\mathbf{V}^{\mathrm{T}}$\;
	4. $t:=0$\;
	5. \Repeat{ $\norm{\mathbf{Y}-\widehat{\mathbf{Y}}}_\mathbf{W} > \varepsilon \text{~и~} t < N_{IRLS}$}{
		a. Ганкелизация матрицы $\mathbf{R}$ и получение ряда длины $N$ из остатков: $\tX{R} = \mathcal{T}^{-1} \Pi_{\mathcal{H}} (\mathbf{R}) = (r_1,\ldots,r_N)^\mathrm{T}$\;
		b. Пусть $\tX{R}_+=(|r_1|, \ldots, |r_N|)^\mathrm{T}$ --- вектор из модулей остатков.  Вычисление $\bm{\sigma} = (\sigma_1,\ldots,\sigma_{N})^\mathrm{T}$ как оценки мат. ожидания $\mathbb{E}(\tX{R}_+)$ некоторым выбранным методом (подробнее оценка мат. ожидания обсуждалась в пункте~\ref{sigma})\;
		с. Вычисление ряда $|\bm{\sigma}^{-1}\tX{R}| = (\frac{|r_1|}{\sigma_1},\ldots,\frac{|r_N|}{\sigma_N})^\mathrm{T} = (r_1^*,\ldots,r_N^*)^\mathrm{T}$\;
		d. Вычисление вектора весов $\tX{W}= (w_1,\ldots,w_N)^\mathrm{T} = (w(r_1^*),\ldots,w(r_N^*))^\mathrm{T}$, используя %$w(x)=\frac{\partial \rho(x)}{\partial |x|} \frac{1}{|x|}$:
		\begin{equation*}
		w(x) = 
		\begin{cases}
		(1-(\frac{|x|}{\alpha})^2)^2, &|x|\le\alpha\\
		0, &|x|>\alpha
		\end{cases}\; %~~~ \text{где}~ x=r^*.
		\; \end{equation*} 
		e. Решение задачи взвешенной аппроксимации с помощью модифицированного метода Гаусса-Ньютона с матрицей весов $\mathbf{W}=\diag (\tX{W}) \in \mathbb{R}^{N\times N}$:
		\begin{equation*}
		\tX{X}^* = \argminB_{\tX{X}\in\bar{\mathcal{D}}_r} \norm{\tX{Y}-\tX{X}}_{\mathbf{W}}.
		\end{equation*}	
		
		f. Пусть $\widehat{\mathbf{Y}}$ --- траекторная матрица ряда $\tX{X}^*$. Обновление матрицы остатков $\mathbf{R}=\{r_{ij}\}_{i,j=1}^{n,p} = \mathbf{Y}-\widehat{\mathbf{Y}}$ и ряда $\tX{Y} = \tX{X}^*$ \;
		g. $t:=t+1$.
	}
	\caption{Модификация метода с итеративным обновлением весов с использованием метода Гаусса-Ньютона}
\end{algorithm}


\section{Оценка трудоемкости методов}

Вопросу трудоемкости L1-SSA уделялось большое внимание во многих работах, посвященных построению $\mathbb{L}_1$-проекции на множество матриц ранга, не превосходящего $r$. В статье~\cite{Optimal} приведен алгоритм, решающий точно эту задачу. Пусть имеется вещественная матрица  $\mathbf{X} \in \mathbb{R}^{L\times K}$, необходимо вычислить проекцию на множество матриц ранга, не превосходящего $r$. Трудоемкость алгоритма, решающего задачу в явном виде, составляет $O(L^{\mathrm{rank}(\mathbf{X})r-r+1})$. В данной работе мы рассматривали методы, решающие эту задачу приближенно, но более эффективно.

Сравним теоретические трудоемкости описанных алгоритмов. 

%\subsubsection{Метод, использующий взвешенную медиану}
%
%Вычислим порядок операций, требующихся для построения проекции матрицы размерности $n\times p$ на множество матриц ранга, не превосходящего $r$, методом из раздела~\ref{sec: rodrig}. Трудоемкость нахождения медианы составляет $O(N)$, где $N$ --- объем выборки. То есть на внутренний цикл необходимо $np$ операций. Алгоритм находит все $p$ собственных троек. Таким образом, порядок операций составляет
%\begin{equation}\label{complexity_robustSvd}
%\mathrm{T}_{\mathrm{robustSvd}} = O(p^2nN_{iter}),
%\end{equation}
%где $N_{iter}$ --- число итераций, необходимых для сходимости. Число итераций не превышает максимального числа итераций, которое можно задать константным, не зависящим от $n$ и $p$.


\subsubsection{Последовательный метод}

Вычислим трудоемкость последовательного алгоритма из раздела~\ref{sec: l1pca}. Трудоемкость составляет $O((KP_1+P_2)N_{iter})$, где $P_1$ и $P_2$ --- трудоемкость решения задач линейного программирования, а $N_{iter}$ --- общее количество итераций для сходимости метода, которое также считаем не зависящим от $L, K, r$. Согласно~\cite{Chvatal}, сложность вычисления задачи линейного программирования с $v$ переменными и $c$ ограничениями составляет $O(c \log v)$. В статье~\cite{pcaL1book}, содержащей описание алгоритма последовательного метода, вычислено количество переменных и ограничений в решаемых задачах, и получено, что  трудоемкость может быть оценена как 
\begin{equation}\label{complexity_pcaL1}
\mathrm{T}_{\mathrm{l1pca}} = O(LK\log (2LK+Lr)N_{iter}),
\end{equation}


\subsubsection{Метод с итеративным обновлением весов}

Теоретическая трудоемкость метода из раздела~\ref{sec: IRLS} составляет, согласно статье~\cite{Chen},
\begin{equation}\label{complexity_IRLS}
\mathrm{T}_{\mathrm{IRLS}} = O(LKr^2N_\alpha N_{IRLS}),
\end{equation}
где $N_\alpha$ и $N_{IRLS}$ --- общее количество итераций для решения задач~(\ref{taskA}), (\ref{taskB}) и сходимости взвешенного метода наименьших квадратов с обновлением весов. Количество итераций мы брали постоянными и не зависящими от $L, K$ и $r$. При подсчете трудоемкости авторы статьи~\cite{Chen} используют книгу~\cite{Golub}, в которой приводятся эффективные алгоритмы QR-разложения матрицы.

\subsubsection{Сравнение теоретических трудоемкостей}

Сравним теоретические трудоемкости последовательного метода и взвешенного метода наименьших квадратов. Необходимо сравнить~(\ref{complexity_pcaL1}) и~(\ref{complexity_IRLS}). Задача сводится к сравнению $\log(L(2K+r))N_{iter}$ и $r^2N_{\alpha}N_{IRLS}$. Авторы статьи~\cite{Chen} утверждают, что максимальное число итераций $N_{\alpha}$ и $N_{IRLS}$ для метода с обновлением весов достаточно взять 5 и 10 соответственно. Проведенные нами вычислительные эксперименты, описанные в разделе~\ref{iterations}, показали, что максимальное число итераций для последовательного метода $N_{iter}$ достаточно взять равным 5 (эксперименты проводились на рядах ранга 3 и 2 при $L=120,~ K=121$). Отличия в ошибках восстановления сигнала при увеличении числа итераций незначительны --- ошибка уменьшается не более чем на 0.2\%. Рассмотрим 2 случая.


Пусть $L$ фиксировано, маленькое, а $K = N-L+1 \sim N$. Это соответствует случаю, когда длина окна маленькая, и траекторная матрица вытянута. Трудоемкость метода с обновлением весов оказывается меньше. 

Пусть траекторная матрица ряда близка к квадратной, то есть $L \sim N/2, ~ K =N-L+1 \sim N/2$. Тогда, если поводить сравнение, то получаем, что надо сравнить $\log(\frac{N}{2}(N+r))$ и $r^2$. Трудоемкость метода с обновлением весов снова оказывается меньше трудоемкости последовательного метода.

%Можно сделать вывод, что теоретическая трудоемкость метода с итеративным обновлением весов оказывается меньше трудоемкости последовательного метода.


Для того, чтобы иметь возможность сравнить время работы методов, необходимо критерии остановки сделать такими, чтобы методы выдавали примерно одинаковые по точности результаты. Однако это оказывается нетривиальной задачей, поэтому поставим максимальное количество итераций для каждого метода такие, чтобы точность оказывалась приемлемой для каждого из методов. Сравним время работы, учитывая количество итераций.

В таблице~\ref{tab_time} приведено время работы методов для 10 реализаций ряда  $$f_n=e^{4n/N} \sin(2\pi n/30) + Ae^{4n/N}\varepsilon_n, ~ \varepsilon_n \sim N(0,1).$$ Ранг такого ряда равен $2$, длина окна выбрана $L=120$. Длину ряда будем постепенно увеличивать. Число выбросов положим равным 3 (это соответствует 1\% выбросов при длине ряда $N=240$), они будут находиться в случайных точках ряда.

\begin{table}
	\caption{Время работы программы и число итераций для последовательного метода и метода с итеративным обновлением весов (для $M=10$ реализаций ряда) в зависимости от длины ряда $N$.}
	\label{tab_time}
	\centering
	\begin{tabular}{|c||c|c|c|c|c||c|}
		\hline
		& $N=240$ & $N=360$ & $N=480$ & $N=600$ & $N=720$ & $N_{iter}$ \\ 
		\hline
		IRLS & 23 \text{sec.} & 44 \text{sec.} & 60 \text{sec.} & 67 \text{sec.} & 89 \text{sec.} & 5*10 \\
		l1pca & 54 \text{sec.} & 136 \text{sec.} & 403 \text{sec.} & 721 \text{sec.} & 1080 \text{sec.}& 5 \\
		\hline
	\end{tabular}
\end{table}

%Если посмотреть на рисунки~\ref{time1} и \ref{time2}, то можно заметить, что, начиная с длины ряда $N=360$, время работы метода IRLS линейно зависит от $N$ при фиксированной длине окна $L=120$ и $r=2$. Это соответствует полученной формуле~(\ref{complexity_IRLS}) (при $N>120$ траекторная матрица вытянута, $K\sim N$). Для последовательного метода зависимость также соответствует полученным теоретическим результатам. При фиксированной длине окна $L$ трудоемкость должна составлять $O(N\log(N))$. 
При длине ряда $N>240$ траекторная матрицы становится вытянутой, $K\sim N$, $L=120$ --- фиксировано. Исходя из формулы~(\ref{complexity_IRLS}), время работы, разделенное на соответствующую длину ряда $N$, должно не зависеть от $N$ для метода с обновлением весов. Для последовательного метода, пользуясь формулой~(\ref{complexity_pcaL1}), можно сделать вывод, что полученная величина должна логарифмически зависеть от $N$. Графики представлены на рисунках~\ref{time1} и \ref{time2}. % то можно заметить, что, начиная с длины ряда $N=360$, время работы метода IRLS линейно зависит от $N$ при фиксированной длине окна $L=120$ и $r=2$. Это соответствует полученной формуле~(\ref{complexity_IRLS}) (при $N>120$ траекторная матрица вытянута, $K\sim N$). Для последовательного метода зависимость также соответствует полученным теоретическим результатам. При фиксированной длине окна $L$ трудоемкость должна составлять $O(N\log(N))$. 

\begin{figure}[h]
	\begin{center}
		\begin{minipage}[h]{0.48\linewidth}
			\includegraphics[width=1\linewidth]{ttime1}
			\caption{Время работы метода l1pca} %% подпись к рисунку
			\label{time1} %% метка рисунка для ссылки на него
		\end{minipage}
		\hfill 
		\begin{minipage}[h]{0.48\linewidth}
			\includegraphics[width=1\linewidth]{ttime2}
			\caption{Время работы метода IRLS}
			\label{time2}
		\end{minipage}
	\end{center}
\end{figure}

%На первый взгляд, результаты противоречат теоретическому выводу о том, что трудоемкость метода IRLS меньше. Это можно объяснить тем, что теоретические трудоемкости мы сравнивали асимптотически, а при выбранных нами для вычислительных экспериментов $N$, $L$, $r$ результат получился обратный. 
%
%Вычислим, при каких $N$ для такого примера трудоемкость последовательного метода начнет превосходить трудоемкость IRLS.
%
%Траекторная матрица ряда близка к квадратной, $L\sim\frac{N}{2}, K\sim\frac{N}{2}$. Используя формулы~\ref{complexity_pcaL1} и ~\ref{complexity_IRLS}, получаем, что надо сравнить $\mathrm{T}_{\mathrm{l1pca}} = \log(\frac{N^2}{2}+N)$ и $\mathrm{T}_{\mathrm{IRLS}} = 2^2*10$. Лишь при $N$ порядка $6*10^8$ трудоемкость последовательного метода становится больше.
%
%Расширим замеры трудоемкости в динамике по параметрам для этого же примера (ранг сигнала $r=2$). Возьмем длину окна $L=120$ и будем увеличивать длину ряда $N$. Результат представлен в таблице~\ref{}.





\chapter{Вычислительные эксперименты} \label{sec: experiments}

%Одним из основных результатов работы~\cite{vkr} было то, что метод с регуляризацией на рассмотренном примере оказался наиболее устойчивым к выделяющимся наблюдениям среди остальных методов L1-SSA. Выброс находился всего в одной фиксированной точке ряда (в начальной либо средней части). 

Сравним результаты работы описанных методов на нескольких примерах. Для начала возьмем ряд с экспоненциальным трендом и гауссовским шумом, который уже был рассмотрен в работе~\cite{vkr}. Затем рассмотрим ряд с растущей амплитудой и два случая: случай с гетероскедастичным шумом и с шумом с постоянной дисперсией, проведем сравнения для таких рядов. Метод с обновлением весов и его модификации были реализованы мной на R \cite{tretyakova_2020}. Численные эксперименты были проведены с помощью программ, представленных также в \cite{tretyakova_2020}.

%Попробуем перенести вывод о преимуществе метода с регуляризацией на другие ряды с большим количеством выделяющихся наблюдений.
%
%Также сравним метод из статьи~\cite{Rodrigues} с последовательным методом и методом с регуляризацией.
%Рассмотрим два примера.

\section{Модельный пример №1}\label{ex1}

Для начала рассмотрим пример из работы~\cite{vkr}, но добавим большее количество выбросов (1\% и 5\%) в случайных точках ряда. Проверим, какой из приведенных алгоритмов окажется наиболее устойчивым.

Длина ряда $N=240$. Рассмотрим временной ряд 
\begin{equation*}
f_n= e^{n/N}+\sin{(2\pi n/120+\pi/6)}+\varepsilon_n, ~ \varepsilon_n \sim N(0,1).
\end{equation*}
На~рис.~\ref{series_plot} изображен график ряда при  $1\%$ выбросов с величиной выброса $5f_i$. В случайно выбранных точках ряда $f_i$ значение заменяется на $f_i+5f_i$.

\begin{figure}[!h]
	\center{\includegraphics[width=0.55\linewidth]{ser1plot}}
	\caption{График ряда при $1\%$ выбросов с величиной выброса $5f_i$.}
	\label{series_plot}
\end{figure}		

Cравнение будет проводиться по величине среднеквадратичной ошибки, согласованной с $\mathbb{L}_2$, которая вычисляется по формуле
\begin{equation}\label{MSE}
\text{MSE} = \mathbb{E} \left(\frac{1}{N} \sum \limits_{i=1}^{N}(s_i - \hat s_i )^2 \right),
\end{equation}
где $\tX{S}=(s_1,\ldots,s_N)^\mathrm{T}$ --- сигнал, $\hat{\tX{S}}=(\hat{s}_1,\ldots,\hat{s}_N)^\mathrm{T}$ --- его оценка.
Будем вычислять
\begin{equation*}
\text{RMSE} = \sqrt{\textrm{MSE}},
\end{equation*}
а также будем сравнивать методы по величине ошибки, согласованной с $\mathbb{L}_1$, которая имеет вид
\begin{equation}\label{MAD}
\text{MAD} = \mathbb{E} \left(\frac{1}{N} \sum \limits_{i=1}^{N}|s_i - \hat s_i | \right).
\end{equation}

Возьмем количество реализаций ряда $M=30$. Будем находить оценки математических ожиданий~(\ref{MSE}) и~(\ref{MAD}), а далее из оценки MSE будем извлекать корень, получая RMSE.

%Для начала попробуем сравнить работу методов без шума и без выделяющихся наблюдений. В таком случае стандартный метод L2-SSA, метод с весами IRLS и последовательный метод l1pca выдают нулевую ошибку с точностью до погрешности вычислений. Оценка ошибки восстановления сигнала без шума и выбросов методом, использующим взвешенную медиану, не равна нулю (около 0.043). Это доказывает, что метод решает не ту задачу, которую нам необходимо решить. Поэтому этот метод рассматривать не будем.


Ранг ряда равен $3$. Во всех методах берется длина окна $L=120$, равная половине длины ряда, и восстановление сигнала ведется по $3$ компонентам.

Для метода с итеративным обновлением весов и его модификации выберем следующие параметры:
\begin{enumerate}
	\item \textbf{Инициализация.} Возьмем в качестве начальных значений для обоих методов $\mathbf{U}=\mathbf{U}_r \mathbf{\Lambda}_r^{1/2}$, $\mathbf{V}=\mathbf{V}_r$, где $\mathbf{U}_r=[U_1:\ldots:U_r]$, $\mathbf{V}_r=[V_1:\ldots:V_r]$, $\mathbf{\Lambda}_r=\diag (\lambda_1,\ldots,\lambda_r)$ --- первые $r$ компонент сингулярного разложения траекторной матрицы. Для вычисления первых $r$ компонент будем использовать один из вариантов truncated SVD из пакета svd~\cite{svd}.
	\item \textbf{Параметр $\bm{\alpha}$.} Авторы статьи~\cite{Chen}, содержащей описание исходного метода, предлагают выбрать $\alpha=4.685$. Исходя из полученной формулы~(\ref{alpha_gamma}), связывающей параметр $\alpha$ и вероятность $\gamma$, получим для $\gamma=0.99$ значение параметра $\alpha=4.046$. Его и будем брать. Однако стоит заметить, что такая вероятностная интерпретация параметра $\alpha$ верна только для модификации метода.
\end{enumerate}


В таблице~\ref{tab1_a} представлены результаты сравнения для нескольких методов. Выброс добавлялся в случайных точках ряда заменой значения $f_i$ на $f_i+5f_i$.

При сравнении методов со стандартным использовался пакет Rssa \cite{Rssa}. Реализация модифицированного метода Гаусса-Ньютона находится в репозитории~\cite{MGNcode}.

\begin{table}
	\caption{Оценки RMSE и MAD для различных методов для $M=30$ реализаций ряда.}
	\label{tab1_a}
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline
		Method 	& 0\% & 1\% & 5\%  \\ 
		\hline
		\multicolumn{4}{|c|}{Оценки RMSE} \\
		\hline
		Basic SSA & \textbf{0.184} & 0.256 & 0.653  \\
		l1pca     & 0.217 & 0.223 & 0.250 \\
		IRLS  (orig.)   & \textbf{0.184} & \textbf{0.189} & \textbf{0.206}\\
		IRLS (loess) & 0.196 & 0.199 & \textbf{0.204}\\
		IRLS (median) & 0.210 & 0.221 & 0.223\\
		IRLS (lowess) & 0.206 & 0.207 & \textbf{0.211}\\
		IRLS (MGN) & 0.398 & 0.222 & 0.798\\
		\hline
		\multicolumn{4}{|c|}{Оценки MAD} \\
		\hline
		Basic SSA & \textbf{0.143} & 0.197 & 0.505 \\
		l1pca     & 0.175 & 0.179 & 0.203 \\
		IRLS  (orig.)   & \textbf{0.145} & \textbf{0.147} & \textbf{0.161}\\
		IRLS (loess) & 0.155 & 0.158 & \textbf{0.160}\\
		IRLS (median) & 0.170 & 0.178 & 0.178\\
		IRLS (lowess) & 0.165 & 0.166 & \textbf{0.168}\\
		IRLS (MGN) & 0.259 & 0.178 & 0.601\\
		\hline
	\end{tabular}
\end{table}

Первая строка таблиц соответствует стандартному методу SSA с большой длиной окна $(L=120)$. 
%Вторая строка соответствует методу с регуляризацией (ALM), описанным в работе~\cite{vkr}.
Вторая строка --- метод l1pca из пакета pcaL1~\cite{pcaL1} (соответствует последовательному методу из раздела~\ref{sec: l1pca}). Третья строка соответствует стандартному методу с итеративным обновлением весов из раздела~\ref{sec: IRLS}. Четвертая, пятая и шестая строки соответствуют модификации взвешенного метода наименьших квадратов с различными вариантами выделения тренда (локальная регрессия с параметром сглаживания 0.35, скользящая медиана с длиной окна 80 и взвешенная локальная регрессия с параметром сглаживания 0.35). Была использована реализация lowess в R из статьи~\cite{lowess}. Параметр сглаживания выбран 0.35, остальные параметры оставлены по умолчанию: число итераций равно 3, параметр $\delta$, требующийся для ускорения вычисления, равен $0.01(\max \limits_{i} {f_i} - \min \limits_{i} {f_i})$. Седьмая строка посвящена модификации метода с обновлением весов с использованием метода Гаусса-Ньютона. 

Из-за того, что дисперсия шума постоянная, можем предположить, что модификации IRLS с различными вариантами выделения тренда будут работать примерно одинаково. Поэтому подробнее об отличиях модификаций с разными вариантами выделения тренда пока что говорить не будем.

Жирным шрифтом в каждом столбце выделено лучшее значение и значение, которое незначимо отличается от лучшего. Проверка значимости сравнений представлена  далее в таблицах~\ref{tab: pval1a} и~\ref{tab: pval1b}.



Можно заметить, что при 5\% выбросов модификация с использованием метода Гаусса-Ньютона оказывается неустойчивой. Можно предположить, что это происходит из-за того, что выбросы оказываются близко друг к другу, либо близко к началу или концу ряда. Исследуем, как ведут себя методы в зависимости от положения выброса. Результаты представлены в таблице~\ref{tab_one}. 

Действительно, если выброс попадает близко к концу ряда (в точку $x_{235}$), то модификация с использованием метода Гаусса-Ньютона дает большую ошибку восстановления сигнала. Однако при наличии выброса в середине ряда ошибка восстановления сигнала с использованием этой модификации маленькая. Можно сделать вывод, что этот метод лучше использовать, если выбросов небольшое количество, либо при наличии информации, что выбросы не содержатся близко к началу или концу ряда. В наших исследованиях выбросы находятся в случайно выбранных точках ряда, чтобы максимально обобщить все варианты местоположения выделяющихся наблюдений. Поэтому в дальнейших сравнениях не будем рассматривать эту модификацию.

\begin{table}
	\caption{RMSE в зависимости от положения выброса.}
	\label{tab_one}
	\centering
	\begin{tabular}{|c||c|c|c|c|c|}
		\hline
		Method 	& $x_{50}$ & $x_{100}$ & $x_{150}$ & $x_{200}$  & $x_{235}$\\ 
		\hline
		Basic SSA & 0.106 & 0.239 & 0.137 & 0.170 & 0.259\\
		l1pca & 0.213 & 0.251 & 0.195 & 0.165 & 0.278  \\
		IRLS (orig.) &  0.111 & 0.201 & 0.121 & 0.163 & 0.222  \\
		IRLS (loess) &  0.135 & 0.217 & 0.163 & 0.178 & 0.243  \\
		IRLS (median) &  0.223 & 0.255 & 0.227 & 0.179 & 0.283  \\
		IRLS (lowess) & 0.168 & 0.230 & 0.188 & 0.187 & 0.255  \\
		IRLS (MGN) & 0.092 & 0.220 & 0.134 & 0.156 & 0.655 \\
		
		\hline
	\end{tabular}
\end{table}






%\begin{table}
%	\caption{Оценки MAD для различных методов для $M=10$ реализаций ряда.}
%	\label{tab1_b}
%	\centering
%	\begin{tabular}{|c|c|c|c|}
%		\hline
%		Method 	& 0\% & 1\% & 5\%  \\ 
%		\hline
%		Basic SSA & \textbf{0.139} & 0.164 & 0.505 \\
%		l1pca     & 0.151 & 0.188 & 0.186 \\
%		IRLS      & \textbf{0.140} & \textbf{0.152} & \textbf{0.145} \\
%		IRLS (modif.) & 0.156 & \textbf{0.160} & \textbf{0.159} \\
%		\hline
%	\end{tabular}
%\end{table}

%Из таблиц можно сделать вывод, что метод с регуляризацией остается наиболее устойчивым и при увеличении количества выбросов. 

При отсутствии выбросов наиболее точным все так же остается классический метод SSA, а также метод с итеративным  обновлением весов.  В присутствии выделяющихся наблюдений наиболее устойчивыми являются метод обновлением весов и его модификация с использованием локальной регрессии.

%Также заметим, что метод из статьи~\cite{Rodrigues} работает неплохо для этого ряда. 

%Итак, можно сделать вывод, что метод IRLS для рассмотренного ряда работает достаточно точно при отсутствии выбросов, а также является наиболее устойчивым к выделяющимся наблюдениям среди остальных рассмотренных методов. Модификация метода IRLS также дает небольшую ошибку при появлении выделяющихся наблюдений.

\subsubsection{Проверка значимости сравнения}

Опишем подробнее, как происходит сравнение метода, выдающего наименьшую ошибку, с остальными методами. Проверим значимость сравнения по критерию для зависимых выборок.
%Для каждой реализации ряда будем одновременно находить ошибку для всех методов и вычислять попарные разности между ошибками. Далее на основе этих данных для каждой пары попробуем выяснить, являются ли методы одинаково эффективными. 
Проверим гипотезу, что MSE для некоторых методов равны между собой.

$\mathrm{H}_0: \mathbb{E}(\xi_1-\xi_2)=0$.
Имеем две выборки $X=(x_1,\ldots,x_M)$ и $Y=(y_1,\ldots,y_M)$ объема $M$. Обозначим $\bar{X}$ и $\bar{Y}$ --- их выборочные средние, $s_x^2$ и $s_y^2$ --- выборочные дисперсии, $\hat\rho$ --- коэффициент корреляции. 
Статистика критерия 
\begin{equation*}
t = \frac{\sqrt{M}(\bar{X}-\bar{Y})}{\sqrt{s_x^2+s_y^2-2s_xs_y\hat\rho}}
\end{equation*}
имеет асимптотически нормальное распределение. Критерий является двухсторонним.

Проверим, является ли отличие между этими методами значимым. В таблице~\ref{tab: pval1a} приведены p-value для сравнения среднеквадратичных ошибок для стандартного SSA и остальных методов без выделяющихся наблюдений. Все сравнения оказываются значимыми при уровне значимости 0.05.
В таблице~\ref{tab: pval1b} приведены p-value для сравнения ошибок для модификации метода с обновлением весов и остальных методов при 5\% выбросов.  При уровне значимости 0.05 при без выбросов сравнение стандартного SSA с оригинальным методом с обновлением весов оказывается незначимым. При 5\% выбросов сравнения модификации IRLS (loess) с оригинальным IRLS и IRLS (lowess) оказываются незначимыми.


\begin{table}
	\caption{P-value для сравнения различных методов с наилучшим без выбросов.}
	\label{tab: pval1a}
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|c|}
			\hline
			0\% 	& l1pca & IRLS (orig.) & IRLS (loess) & IRLS (median) & IRLS (lowess)  \\ 
			\hline
			Basic SSA   & 4.7e-5  & \textbf{0.08} & 0.022 & 0.001 & 0.009\\
			\hline
		\end{tabular} \\
	\end{center}
\end{table}

\begin{table}
	\caption{P-value для сравнения различных методов с наилучшим в присутствии выбросов.}
	\label{tab: pval1b}
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|c|}
			\hline
			5\% & Basic SSA	& l1pca & IRLS (orig.) & IRLS (median) & IRLS (lowess)  \\ 
			\hline
			IRLS (loess) & 3.1e-13   & 1.1e-5 &   \textbf{0.589}  &  0.019 & \textbf{0.185}  \\
			\hline
		\end{tabular} \\
	\end{center}
\end{table}

%\begin{table}
%	\caption{P-value для сравнения RMSE методов l1pca и IRLS (modif.) в зависимости от количества выбросов}
%	\label{tab: difference1}
%	\begin{center}
%		\begin{tabular}{|c|c|c|c|}
%			\hline
%			& 0\% & 1\% & 5\%   \\ 
%			\hline
%			l1pca          & 0.194  & 0.226 & 0.236  \\
%			IRLS (modif.)  & 0.199 & 0.201 & 0.197  \\
%			\hline
%			p-value        & 0.539 & 0.078  & \textbf{0.003} \\
%			\hline
%		\end{tabular} \\
%	\end{center}
%\end{table}
%
%
%\begin{table}
%	\caption{P-value для сравнения RMSE методов IRLS и IRLS (modif.) в зависимости от количества выбросов}
%	\label{tab: difference4}
%	\begin{center}
%		\begin{tabular}{|c|c|c|c|}
%			\hline
%			& 0\% & 1\% & 5\%   \\ 
%			\hline
%			IRLS           & 0.178 & 0.190 & 0.186 \\
%			IRLS (modif.)  & 0.199 & 0.201 & 0.197 \\
%			\hline
%			p-value        & 0.096 & 0.157 & 0.254 \\
%			\hline
%		\end{tabular} \\
%	\end{center}
%\end{table}

%Таким образом, увеличение числа выбросов не влияет на устойчивость метода с регуляризацией. Но важно убедиться, что этот метод будет работать хорошо не только на одном примере. Далее попробуем заменить ряд на другой, с большим разбросом значений, и исследуем поведение методов на таком примере.

\section{Модельный пример №2}\label{ex4}
Попробуем рассмотреть не похожий на предыдущий пример ряд, добавив растущую амплитуду и шум непостоянной дисперсии, и исследуем устойчивость методов.

Длина ряда $N=240$. Рассмотрим ряд с гетероскедастичным шумом
\begin{equation*}
f_n=e^{4n/N} \sin(2\pi n/30) + Ae^{4n/N}\varepsilon_n, ~ \varepsilon_n \sim N(0,1).
\end{equation*}

График ряда представлен на рисунке~\ref{series_plot4}. Ранг ряда равен 2. 

\begin{figure}[!h]
	\center{\includegraphics[width=0.5\linewidth]{ser4plot}}
	\caption{График ряда при $1\%$ выбросов с величиной выброса $5f_i$, $A=0.5$.}
	\label{series_plot4}
\end{figure}

%В модификации метода IRLS будем выделять тренд следующими способами: скользящей медианой с длиной окна 80, локальной регрессией loess с параметром сглаживания 0.35 и взвешенной локальной регрессией lowess. Была использована реализация lowess в R, описанная в статье~\cite{lowess}. Параметр сглаживания 0.35, число итераций, как и в первом примере, равно 3, параметр $\delta$ равен $0.01(\max \limits_{i} {f_i} - \min \limits_{i} {f_i})$.  Также для сравнения вычислим реальный тренд из ряда из модулей остатков:
%
%$$\mathbb{E}|R| = Ae^{4n/N} \mathbb{E} |\varepsilon| = Ae^{4n/N}\sqrt{\frac{2}{\pi}}.$$

Из-за наличия шума непостоянной дисперсии можно предположить, что модификация метода с итеративным обновлением весов даст хороший результат. Тогда необходимо выбрать наиболее подходящий метод оценки математического ожидания ряда из модулей остатков для задания параметра $\bm{\sigma}$. На рисунке~\ref{trends} представлено выделение тренда из модуля остатков различными способами: локальной регрессией с параметром сглаживания 0.35, скользящей медианой с длиной окна 80 и взвешенной локальной регрессией с параметром сглаживания 0.35. Вычислим реальный тренд из ряда из модулей остатков:
$$\mathbb{E}|\tX{R}| = Ae^{4n/N} \mathbb{E} |\varepsilon| = Ae^{4n/N}\sqrt{\frac{2}{\pi}}.$$ Можно сделать следующие выводы. Прежде всего, локальная регрессия сильнее реагирует на выбросы, тренд чуть завышен. Скользящая медиана плохо работает на конце ряда, даже если брать длину окна в скользящей медиане большой. Взвешенная локальная регрессия хорошо справляется с выбросами и показывает результат, близкий к реальному тренду. 

\begin{figure}[!h]
	\center{\includegraphics[width=0.7\linewidth]{trends}}
	\caption{Выделение тренда из ряда $|\tX{R}|$ несколькими способами.}
	\label{trends}
\end{figure}

Если сравнить точки, получившие нулевые веса различными методами, можно сделать следующие выводы: наибольшие выбросы все методы идентифицировали одинаково хорошо, скользящая медиана обнуляет наибольшее количество точек, нулевые веса получили также точки, не являющиеся выбросами.
Loess и lowess присваивают нулевые веса одинаковым точки и не обнуляют ничего лишнего. Есть предположение, что с использованием этих двух методов ошибка восстановления сигнала будет наименьшая. Отличие в этих методах состоит в том, что выделенный с помощью lowess тренд лежит ниже, чем тренд, полученный с помощью loess. Поэтому lowess чуть сильнее занижает веса в точках, не являющихся выбросами. Возможно, из-за этого loess окажется лучше.

Результаты сравнения методов представлены в таблице~\ref{tab4_a}. Сразу можно отметить, что для ряда с гетероскедастичным шумом метод с обновлением весов дает большую ошибку даже в отсутствии выбросов. Без выбросов модификация метода IRLS с выделением тренда с помощью локальной регрессии, а также последовательный метод дают маленькую ошибку, как и стандартный SSA. %Наименьшая ошибка в присутствии выбросов получилась при использовании модификации IRLS со знанием реального тренда. На практике тренд из модуля остатков нам неизвестен, однако можно сделать вывод, что в данном случае качество выделения тренда важно. Также небольшую ошибку показала модификация IRLS с выделением тренда с помощью локальной регрессии. Метод l1pca при 5\% выбросов работает так же хорошо. 
В присутствии выбросов (1\%) оба эти метода показывают хорошие результаты, а при 5\% выбросов наилучшим оказывается метод с обновлением весов с использованием локальной регрессии.

\begin{table}
	\caption{Оценки RMSE для различных методов для $M=30$ реализаций ряда.}
	\label{tab4_a}
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline
		Method 	& 0\% & 1\% & 5\%  \\ 
		\hline
		\multicolumn{4}{|c|}{Оценки RMSE} \\
		\hline
		Basic SSA & $\mathbf{2.16}$  & 2.78  & 5.96 \\
		l1pca & \textbf{2.45} & \textbf{2.47}  & 2.87 \\
		IRLS (orig.) & 3.52 & 3.59 & 3.61 \\
		IRLS (loess) & $\mathbf{2.31}$ & \textbf{2.36} & $\mathbf{2.39}$ \\
		IRLS (median) & 2.84 & 2.84 & 2.86 \\
		IRLS (lowess) & 2.59 & 2.60 & 2.63 \\
		%\hdashline
		%IRLS (real trend) & \textbf{1.80} & \textbf{2.08} & $\mathbf{1.86}$ \\
		\hline
		\multicolumn{4}{|c|}{Оценки MAD} \\
		\hline
		Basic SSA & $\mathbf{1.08}$ & 1.33 & 2.91 \\
		l1pca & \textbf{1.18}  & \textbf{1.26} & 1.36 \\
		IRLS (orig.) & 1.63 & 1.65 & 1.66\\
		IRLS (loess) & $\mathbf{1.16}$ & \textbf{1.19} & $\mathbf{1.21}$ \\
		IRLS (median) & 1.37  & 1.38 & 1.38 \\
		IRLS (lowess) & 1.26 & 1.28 & 1.29\\
		%\hdashline
		%IRLS (real trend) & $\mathbf{0.92}$ & \textbf{1.06} & $\mathbf{0.97}$ \\
		\hline
	\end{tabular}
\end{table}

%Для того, чтобы остатки имели вид $R = Ae^{4n/N}\varepsilon_i$, надо, чтобы сигнал хорошо отделялся от шума. Можно провести следующий эксперимент: возьмем в модификации IRLS на первой итерации за оценку сигнала настоящий сигнал $e^{4n/N} \sin(2\pi n/30)$ (его траекторную матрицу). 

%\subsubsection{Проверка значимости сравнения}

В таблицах~\ref{tab: pval4a} и~\ref{tab: pval4b} представлены p-value для проверки значимости сравнения наилучших методов с остальными при 0\% и 5\% выбросов.

\begin{table}
	\caption{P-value для сравнения различных методов с наилучшим без выбросов.}
	\label{tab: pval4a}
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|c|}
			\hline
			0\% 	& l1pca & IRLS (orig.) & IRLS (loess) & IRLS (median) & IRLS (lowess)  \\ 
			\hline
			Basic SSA   & \textbf{0.57}  & 6.7e-9 & \textbf{0.69}  & 0.001 & 0.021 \\
			\hline
		\end{tabular} \\
	\end{center}
\end{table}

\begin{table}
	\caption{P-value для сравнения различных методов с наилучшим в присутствии выбросов.}
	\label{tab: pval4b}
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|c|}
			\hline
			5\% & Basic SSA	& l1pca & IRLS (orig.) & IRLS (median) & IRLS (lowess)  \\ 
			\hline
			IRLS (loess)    & 1.7e-5 &  0.020 & 8.7e-4 & 0.018 & 0.026  \\
			\hline
		\end{tabular} \\
	\end{center}
\end{table}

\section{Модельный пример №3}


Возьмем похожий ряд, но с шумом, имеющим постоянную дисперсию. Рассмотрим пример, предложенный в статье~\cite{Rodrigues}, и проведем для этого примера вычислительный эксперимент.

Пусть длина ряда $N=240$. Рассмотрим временной ряд 
\begin{equation*}
f_n= ne^{4n/N}\sin{(2\pi n/30)}+\varepsilon_n, ~ \varepsilon_n \sim N(0,1).
\end{equation*}

Ранг ряда равен 4. У такого ряда разброс собственных значений очень велик. Это может приводить к тому, что некоторые компоненты сигнала могут смешиваться с шумом. Однако шум рассматриваемого размера не портит отделимость сигнала от шума. Выбросы будут находиться в случайно выбранных точках ряда. Сравнение будем проводить при 1\% и 5\% выбросов, а также без выделяющихся наблюдений. В случайно выбранных точках $f_i$ значение будет заменяться на $f_i+1.5f_i$.

На~рис.~\ref{series_plot2} изображен график ряда при  $1\%$ выбросов с величиной выброса $1.5f_i$.

\begin{figure}[!h]
	\center{\includegraphics[width=0.5\linewidth]{ser2plot_2}}
	\caption{График ряда при $1\%$ выбросов с величиной выброса $1.5f_i$.}
	\label{series_plot2}
\end{figure}		

%Будем рассматривать большую длину окна $L=240$, это будет соответствовать выделению сигнала.


%Метод с регуляризацией ALM работает хуже на таком ряде, погрешность в случае отсутствия шума и выделяющихся наблюдений равна приблизительно 0.000003. Несмотря на то, что это значение мало, оно не так близко к нулю, как ошибки для методов L2-SSA и l1pca. При добавлении выбросов ошибка метода с регуляризацией начнет расти и метод перестанет быть наилучшим. 

%Возможные причины того, что на рассмотренных рядах лучшими оказываются разные методы:
%\begin{itemize}
%	\item увеличение длины ряда во втором примере, 
%	\item наличие у второго ряда больших по модулю значений, 
%	\item быстрый рост амплитуды второго ряда, 
%	\item наличие возрастающего тренда у первого ряда. 
%\end{itemize}
%
%Рассмотрим каждую причину отдельно.

% тем, что амплитуда второго ряда достаточно сильно растет, разброс значений оказывается слишком большим. Также такой эффект может быть связан с большой длиной второго ряда. Еще одно отличие между представленными рядами --- это наличие тренда у первого ряда. Рассмотрим каждую причину отдельно.

%Попробуем уменьшить длину ряда и проверить качество восстановление сигнала методом с регуляризацией.
%Рассмотрим похожий ряд, но с меньшей длиной.

%Пусть длина ряда $N=500$. Зададим ряд следующим образом.  Пусть
%\begin{equation*}
%f_n= 2ne^{4n/N}\sin{(2\pi n/60)}+\varepsilon_n, ~ \varepsilon_n \sim N(0,1).
%\end{equation*}

%На таком ряде проверим, будет ли метод с регуляризацией неустойчивым из-за сильно растущей амплитуды ряда. 

Результаты сравнения методов при различном проценте выделяющихся наблюдений представлены в таблице~\ref{tab2_a}.

%Из таблиц видно, что метод с регуляризацией оказывается уже не наилучшим методом.
%В данном случае метод l1pca восстанавливает сигнал более устойчиво. %Следовательно, при переходе от ряда, на котором метод с регуляризацией наилучший, к ряду с большим разбросом значений, увеличение длины ряда не играет большой роли. 

%В методе с регуляризацией присутствует нормировка траекторной матрицы перед началом выполнения алгоритма (алгоритм подробно описан в работе~\cite{vkr}). Соответственно, проблемы в том, что в ряде присутствуют большие по модулю значения, нет. Поэтому большей проблемой оказывается быстрорастущая амплитуда ряда. 

%Видим, что метод robustSvd работает на данном ряде плохо как при отсутствии выделяющихся наблюдений, так и с выбросами. Намного лучше работает последовательный вариант L1-SSA. Однако при большом количестве выбросов он тоже дает ошибку.

Без выделяющихся наблюдений оригинальный IRLS и его модификация с использованием loess незначимо отличаются от стандартного SSA. В присутствии выделяющихся наблюдений только модификация IRLS с использованием взвешенной локальной регрессии работает хорошо. 

Проверка значимости сравнения наилучшего метода с остальными при отсутствии выбросов представлена в таблице~\ref{tab: pval2a}.



\begin{table}
	\caption{Оценки RMSE и MAD для различных методов для $M=30$ реализаций ряда.}
	\label{tab2_a}
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline
		Method 	& 0\% & 1\% & 5\%  \\ 
		\hline
		\multicolumn{4}{|c|}{Оценки RMSE} \\
		\hline
		Basic SSA & \textbf{0.215} & 123.7 & 459.6  \\
		l1pca     & 0.256 & 8.65  & 21.11 \\
		IRLS (orig.)    & \textbf{0.216} & 220.4 & 398.2\\ 
		IRLS (loess) & \textbf{0.227}  & 115.25 & 303.2\\
		IRLS (median) & 0.256  & 20.21 &  38.21\\
		IRLS (lowess) & 0.243  & \textbf{0.260} &  \textbf{0.301}\\
		%IRLS (real trend) & 0.240  & \textbf{0.257} &  \textbf{0.262}\\
		\hline
		\multicolumn{4}{|c|}{Оценки MAD} \\
		\hline
		Basic SSA & \textbf{0.164} & 33.32 & 168.2 \\
		l1pca & 0.197 & 1.117 & 3.145 \\
		IRLS (orig.) & \textbf{0.165} & 27.311 & 88.54 \\ 
		IRLS (loess) & \textbf{0.174} & 22.18 & 31.180 \\
		IRLS (median) & 0.187 & 8.182 & 12.20 \\
		IRLS (lowess) & 0.180 & \textbf{0.179} & \textbf{0.187} \\
		%IRLS (real trend) & 0.180 & \textbf{0.189} & \textbf{0.180} \\
		\hline
	\end{tabular}
\end{table}

%Посмотрим на осмысленность восстановления последовательным методом и методом взвешенных наименьших квадратов. На рисунке~\ref{recplot} изображены исходный сигнал и восстановление методом l1pca и IRLS. Видно, что восстановление и тем, и другим методами довольно осмысленно и практически не отличается друг от друга (два графика практически полностью наложились друг на друга). Выбросы обрезались хорошо и восстановленный сигнал совпал с исходным.

%	\begin{figure}[!h]
%	\center{\includegraphics[width=0.5\linewidth]{recplot}}
%	\caption{График восстановленного ряда двумя методами при $5\%$ выбросов с величиной выброса $1.5y_i$.}
%	\label{recplot}
%\end{figure}	

%Для того, чтобы проверить, что восстановление сигнала описанными методами осмысленно, попробуем рассмотреть сам ряд, включая выбросы, как оценку сигнала и посчитать среднее отклонение от истинного сигнала. Ошибка в таком случае должна получиться больше, чем оценки ошибок методов. Результаты представлены в таблице~\ref{tab2_c}. Действительно, оценки ошибок восстановления сигнала рассматриваемыми методами оказались меньше, чем ошибка, если в качестве оценки сигнала рассматривать исходный ряд.
%
%\begin{table}
%	\caption{RMSE и MAD для исходного ряда в качестве оценки сигнала.}
%	\label{tab2_c}
%	\centering
%	\begin{tabular}{|c|c|c|c|}
%		\hline
%		 	& 0\% & 1\% & 5\%  \\ 
%		\hline
%		RMSE & 0.97 & 542.9 & 953.8 \\
%		MAD & 0.77 & 39.1  & 114.7 \\
%		\hline
%	\end{tabular}
%\end{table}


%\subsubsection{Проверка значимости сравнения}

%В таблице~\ref{tab: difference2} приведены p-value для сравнения среднеквадратичных ошибок для последовательного метода l1pca и модифицированного метода с весами IRLS (modif.). При уровне значимости 0.05 сравнения оказываются незначимыми.

%\begin{table}
%	\caption{P-value для сравнения MSE методов l1pca и IRLS (modif.) в зависимости от количества выбросов}
%	\label{tab: difference2}
%	\begin{center}
%		\begin{tabular}{|c|c|c|c|}
%			\hline
%			& 0\% & 1\% & 5\%   \\ 
%			\hline
%			l1pca & 0.234 & 0.246  & 0.270    \\
%			IRLS (modif.)  & 0.240  & 0.257 & 0.262   \\
%			\hline
%			p-value	& 0.340  & 0.343 & 0.371  \\
%			\hline
%		\end{tabular} \\
%	\end{center}
%\end{table}

\begin{table}
	\caption{P-value для сравнения различных методов с наилучшим без выбросов.}
	\label{tab: pval2a}
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|c|}
			\hline
			0\% &  l1pca & IRLS (orig.) & IRLS (loess) & IRLS (median) & IRLS (lowess) \\ 
			\hline
			Basic SSA & 4.1e-4 & \textbf{0.962} &  \textbf{0.107} & 0.004 & 0.024 \\
			\hline
		\end{tabular} \\
	\end{center}
\end{table}

%\begin{table}
%	\caption{P-value для сравнения различных методов с наилучшим в присутствии выбросов.}
%	\label{tab: pval2b}
%	\begin{center}
%		\begin{tabular}{|c|c|c|c|c|c|}
%			\hline
%			5\% & Basic SSA	& l1pca & IRLS (orig.) & IRLS (loess) & IRLS (median) \\
%			\hline
%			IRLS (lowess) & 0.037 & 0.33 & 0.14 & 0.27 & 0.24\\
%			\hline
%		\end{tabular} \\
%	\end{center}
%\end{table}

%Проверим зависимость устойчивости метода с регуляризацией от параметров метода. Метод имеет 3 параметра. Наиболее важным из них является коэффициент регуляризации $\lambda$. Остальные параметры технические: начальное значение параметра $\mu$ в функции Лагранжа и коэффициент увеличения параметра $\mu$ на каждой итерации (шаг). 
%
%Увеличение максимального числа итераций во внутреннем цикле и уменьшение $\varepsilon$ в критерии сходимости значимых результатов не дают. 
%
%Попробуем менять коэффициент регуляризации и найдем оценки ошибок при отсутствии шума и выбросов. В идеальном случае должны получиться ошибки, близкие к нулю. В таблице~\ref{tab_lambda} находятся оценки RMSE для ряда без шума и выбросов при различных $\lambda$. 
%
%
%\begin{table}
%	\caption{Зависимость оценки RMSE для метода с регуляризацией от величины $\lambda$.}
%	\label{tab_lambda}
%	\centering
%	\begin{tabular}{|c|c|c|c|}
%		\hline
%		 	& $\lambda = 1\E{-}03$ & $\lambda = 1\E{-}05$ & $\lambda = 1\E{-}09$  \\ 
%		\hline
%		ALM & $2.7\E{-}06$ & $2.4\E{-}05$ & $3.9\E{-}11$  \\
%		\hline
%	\end{tabular}
%\end{table}
%
%Таким образом, на данном примере при уменьшении коэффициента регуляризации метод оказывается точнее.
%

% Попробуем уменьшить рост амплитуды ряда и исследовать устойчивость методов. Возьмем ряд с меньшей скоростью роста амплитуды и посмотрим на ошибки (таблицы~\ref{tab3_a} и \ref{tab3_b}).
% 
% \begin{table}
% 	\caption{Оценки RMSE для различных методов для $M=10$ реализаций ряда.}
% 	\label{tab3_a}
% 	\centering
% 	\begin{tabular}{|c|c|c|c|}
% 		\hline
% 		Method 	& 0\% & 1\% & 5\%  \\ 
% 		\hline
% 		Basic SSA & 0.201 & 2.376 & 10.76  \\
% 		%	ALM & 0.201 &  0.219 & 0.282  \\
% 		l1pca & 0.260 & 0.256 & 0.300 \\
% 		robustSvd & 1.45 & 1.59 & 3.368 \\ 
% 		IRLS & 0.206 & 0.218 & 0.237 \\ 
% 		IRLS (modif.) &  &  & \\
% 		\hline
% 	\end{tabular}
% \end{table}
% 
% \begin{table}
% 	\caption{Оценки MAD для различных методов для $M=10$ реализаций ряда.}
% 	\label{tab3_b}
% 	\centering
% 	\begin{tabular}{|c|c|c|c|}
% 		\hline
% 		Method 	& 0\% & 1\% & 5\%  \\ 
% 		\hline
% 		Basic SSA & 0.151 & 1.52 & 6.557 \\
% 		%ALM & 0.134 &  0.143 & 0.158  \\
% 		l1pca & 0.201 & 0.196 & 0.231 \\
% 		robustSvd & 0.879 & 0.94 & 1.96 \\ 
% 		IRLS & 0.154 & 0.169 & 0.178 \\ 
% 		IRLS (modif.) &  &  & \\
% 		\hline
% 	\end{tabular}
% \end{table}
%
%%\begin{equation*}
%%f_n= 0.5ne^{4n/N}\sin{(2\pi n/120)}+\varepsilon_n, ~ \varepsilon_n \sim N(0,1).
%%\end{equation*}
%
%%Для сокращения времени вычислений уберем шум и будем брать выброс в фиксированной точке ряда. Параметр $\lambda$ возьмем такой же, как был изначально ($1\E{-}03$), чтобы можно было понять, влияет ли скорость роста амплитуды на работу методов. 
%%
%%Для начала вычислим ошибку без выброса. Для метода с регуляризацией она равна $1.7\E{-}07$. Для метода robustSvd --- $1.6\E{-}04$. Остальные методы, как уже говорилось ранее, дают приблизительно нулевую ошибку.
%%
%%В случае выброса в фиксированной точке $f_{390}$ (взята ближе к концу ряда, чтобы значение выброса не было слишком маленьким) метод с регуляризацией и последовательный метод оказываются наиболее устойчивыми и дают ошибки RMSE $1.80\E{-}06$ и $1.56\E{-}07$ соответственно. 
%
%При уменьшении скорости роста амплитуды ряда, метод IRLS восстанавливает сигнал наиболее устойчиво.

%Одной из возможных причин того, что на разных рядах лучшими оказываются разные методы, является наличие возрастающего тренда у первого ряда. Первый ряд растет, а у второго ряда изменяется только амплитуда. Попробуем постепенно уменьшать рост первого ряда и исследуем устойчивость методов. 

%При уменьшении тренда ошибка RMSE метода l1pca начинает постепенно уменьшаться. На исходном ряде из первого примера без шума и с выделяющимся наблюдением в фиксированной точке (ближе к концу ряда) оценка RMSE равна $3\E{-}08$. Оценка RMSE для метода с регуляризацией в таком случае была равна $1.5\E{-}11$. При постепенном уменьшении тренда метод l1pca начинает работать лучше, и уже на ряде 
%\begin{equation*}
%f_n= e^{n/(4N)}+\sin{(2\pi n/120+\pi/6)}
%\end{equation*}
%ошибки метода l1pca и метода с регуляризацией равны $9.8\E{-}14$ и $1.1\E{-}11$ соответственно.
%Поэтому при переходе от первого примера ко второму основным отличием, влияющим на ошибки методов, является то, что у второго ряда меняется только амплитуда, а возрастающий тренд отсутствует. В таком случае ошибка последовательного метода оказывается меньше, чем его ошибка на первом примере с трендом. Поэтому на первом примере лучшим оказывается метод с регуляризацией, но затем при уменьшении тренда ошибки последовательного метода убывают, и на втором примере последовательный метод оказывается лучше.

%\section{Пример 3}
%
%Основное отличие первого и второго примера состояло в том, что у второго ряда был сильный рост амплитуды. Однако у первого ряда присутствовал растущий экспоненциальный тренд, что могло также повлиять на работу методов. Попробуем рассмотреть простой стационарный ряд, у которого не растет амплитуда, и сравним методы на таком примере. Длина ряда по-прежнему $N=240$. Рассмотрим ряд 
%\begin{equation*}
%f_n=\sin{(2\pi n/120+\pi/6)}+\varepsilon_n, ~ \varepsilon_n \sim N(0,1).
%\end{equation*}
%
%График ряда при  $1\%$ выбросов с величиной выброса $5y_i$ изображен на~рис.~\ref{series_plot3}.
%
%\begin{figure}[!h]
%	\center{\includegraphics[width=0.5\linewidth]{ser3plot}}
%	\caption{График ряда при $1\%$ выбросов с величиной выброса $5y_i$.}
%	\label{series_plot3}
%\end{figure}
%
%Результаты сравнения представлены в таблице~\ref{tab3_a}. Метод IRLS и его модификация оказываются для этого ряда устойчивее, чем последовательный метод. 
% 
%\begin{table}
%	\caption{Оценки RMSE и MAD для различных методов для $M=10$ реализаций ряда.}
%	\label{tab3_a}
%	\centering
%	\begin{tabular}{|c|c|c|c|}
%		\hline
%		Method 	& 0\% & 1\% & 5\%  \\ 
%		\hline
%		\multicolumn{4}{|c|}{Оценки RMSE} \\
%		\hline
%		Basic SSA & \textbf{0.153} & \textbf{0.161} & 0.254 \\
%		l1pca & 0.165 & 0.187  & 0.207 \\
%		IRLS & \textbf{0.150} & \textbf{0.163} & \textbf{0.183}\\
%		IRLS (modif.) & 0.191 & \textbf{0.167} & \textbf{0.171}\\
%		\hline
%		\multicolumn{4}{|c|}{Оценки MAD} \\
%		\hline
%		Basic SSA & \textbf{0.123} & \textbf{0.122} & 0.192 \\
%		l1pca & 0.134 & 0.147 & 0.154 \\
%		IRLS & \textbf{0.123} & \textbf{0.122} & \textbf{0.139} \\
%		IRLS (modif.) & 0.130 & 0.150 & \textbf{0.137}\\
%		\hline
%	\end{tabular}
%\end{table}
%
%
%%\begin{table}
%%	\caption{Оценки MAD для различных методов для $M=10$ реализаций ряда.}
%%	\label{tab3_b}
%%	\centering
%%	\begin{tabular}{|c|c|c|c|}
%%		\hline
%%		Method 	& 0\% & 1\% & 5\%  \\ 
%%		\hline
%%		Basic SSA & \textbf{0.123} & 0.122 & 0.192 \\
%%		l1pca & 0.134 & 0.147 & 0.154 \\
%%		IRLS & \textbf{0.123} & 0.122 & \textbf{0.139} \\
%%		IRLS (modif.) & 0.130 & 0.150 & \textbf{0.137}\\
%%		\hline
%%	\end{tabular}
%%\end{table}
%
%%\subsubsection{Проверка значимости сравнения}
%%
%%В таблице~\ref{tab: difference3} приведены p-value для проверки значимости отличия последовательного метода и метода с весами. Как и в примере из пункта~\ref{ex1}, при отсутствии выбросов при уровне значимости 0.05 отличие не значимо, а с выбросами --- значимо.
%%
%%\begin{table}
%%	\caption{P-value для сравнения MSE методов l1pca и IRLS в зависимости от количества выбросов}
%%	\label{tab: difference3}
%%	\begin{center}
%%		\begin{tabular}{|c|c|c|c|}
%%			\hline
%%			& 0\% & 1\% & 5\%  \\ 
%%			\hline
%%			l1pca & 0.194 & 0.231  & 0.230   \\
%%			IRLS & 0.164 & 0.165 & 0.195    \\
%%			\hline
%%			p-value	& 0.105 & 0.003 & 0.002   \\
%%			\hline
%%		\end{tabular} \\
%%	\end{center}
%%\end{table}
%
%\begin{table}
%	\caption{P-value для сравнения различных методов с наилучшим без выбросов.}
%	\label{tab: pval3a}
%	\begin{center}
%		\begin{tabular}{|c|c|c|c|}
%			\hline
%			0\% 	& l1pca & IRLS & IRLS (modif.)   \\ 
%			\hline
%			Basic SSA         &  &  &   \\
%			\hline
%		\end{tabular} \\
%	\end{center}
%\end{table}
%
%\begin{table}
%	\caption{P-value для сравнения различных методов с наилучшим в присутствии выбросов.}
%	\label{tab: pval3b}
%	\begin{center}
%		\begin{tabular}{|c|c|c|c|}
%			\hline
%			5\% & Basic SSA	& l1pca & IRLS (modif.)   \\ 
%			\hline
%			IRLS         &  &  &   \\
%			\hline
%		\end{tabular} \\
%	\end{center}
%\end{table}



%\section{Выводы} \label{sec: results}
%
%Исходя из полученных результатов сравнений методов, можем сделать следующие выводы.
%
%%Увеличение числа выбросов в примере из работы~\cite{vkr} не влияет на устойчивость метода с регуляризацией.
%
%%Преимущество метода с регуляризацией исчезает при замене ряда на ряд с большим разбросом значений и быстрорастущей амплитудой. Наилучшим в таком случае оказывается последовательный вариант L1-SSA. 
%
%%При отсутствии выбросов и шума нулевую ошибку с точностью до погрешности вычислений дают только стандартный метод L2-SSA, последовательный метод l1pca и IRLS. Ошибка метода robustSvd достаточно большая, то есть метод не решает необходимую нам задачу.
%
%Хорошими оказались два метода: последовательный метод и взвешенный метод наименьших квадратов, включая его модификацию. 
%
%Для ряда, амплитуда которого остается постоянной, наилучшим методом при отсутствии выбросов остается стандартный L2-SSA, однако значение ошибки метода IRLS при отсутствии выбросов оказывается ненамного больше. Метод IRLS является для такого примера наиболее устойчивым.
%
%Для ряда с растущей амплитудой наиболее устойчивыми методами оказываются l1pca и модификация метода IRLS. 
%
%Теоретическая трудоемкость последовательного метода $O(np\log(2pn+nr)N_{iter})$ оказывается меньше теоретической трудоемкости метода с весами, которая составляет $ O(npr^2N_\alpha N_{iter}) $. Однако на практике время работы этих двух методов небольшое и не сильно отличающееся друг от друга: 54 и 39 секунд.


%Уменьшение параметра $\lambda$ в методе с регуляризацией улучшает точность работы метода в случае ряда с большой амплитудой. Присутствие больших по модулю значений в ряде проблемы не составляет, потому что в методе с регуляризацией присутствует нормировка траекторной матрицы. 

%Уменьшение скорости роста амплитуды ряда приводит к тому, что при отсутствии выбросов и шума метод с регуляризацией и метод из статьи~\cite{Rodrigues} работают более точно, чем работали на примере с большой скоростью роста амплитуды. В случае выбросов последовательный метод работает немного лучше, чем метод с регуляризацией. 

%Наличие возрастающего тренда влияет на точность восстановления сигнала последовательным методом. При уменьшении тренда ошибки последовательного метода уменьшаются, и на втором примере наилучшим оказывается последовательный метод.

%Если сравнивать методы по величине оценки RMSE, то можно сделать вывод, что при отсутствии выбросов наиболее точным методом является стандартный L2-SSA с большой длиной окна. Наиболее устойчивым к выбросам оказывается последовательный метод l1pca. При наличии выделяющихся наблюдений ошибка восстановления сигнала методом robustSvd, использующим взвешенную медиану, оказалась меньше, чем ошибка восстановления стандартным методом L2-SSA. Однако последовательный метод все же остается наиболее устойчивым по сравнению с остальными методами. 
%
%Аналогичные выводы можно сделать и при сравнении методов по величине оценки MAD. Наиболее точным методом оказывается L2-SSA с большой длиной окна, а наиболее устойчивым является последовательный метод из раздела~\ref{sec: l1pca}.

%В таблице~\ref{tab3} приведено время работы программы для данных методов для $M=10$ реализаций ряда. 


%Стандартный алгоритм L2-SSA работает во много раз быстрее, чем варианты метода L1-SSA. Сравним теперь два варианта метода L1-SSA между собой. Можно заметить, что время, затраченное на выполнение алгоритма, использующего взвешенную медиану, немного меньше, чем время работы последовательного метода. Однако, как уже говорилось ранее, последовательный метод является намного более устойчивым.
%



%\begin{table}
%	\caption{Время работы программы для различных методов для $M=10$ реализаций ряда.}
%	\label{tab3}
%	\centering
%	\begin{tabular}{|c|c|c|c|}
%		\hline
%		Method 	& 0\% & 1\% & 5\%  \\ 
%		\hline
%		Basic SSA & 0.5227 \text{sec} & 1.3491 \text{sec} & 1.3913 \text{sec}\\
%		l1pca & 264.3195 \text{min} & 635.9288 \text{min} & 586.0057 \text{min} \\
%		robustSvd & 149.6039 \text{min} & 390.4035 \text{min} & 383.8407 \text{min} \\ 
%		\hline
%	\end{tabular}
%\end{table}

\section{Выводы}

Результаты исследования для трех рассмотренных примеров представлены в таблице~\ref{tab_all}. На основе проведенного исследования можно сделать следующие выводы. Для первого примера без растущей амплитуды ряда в случае гауссовского шума наиболее точным методом без выбросов оказывается стандартный SSA. Стандартный метод с обновлением весов также является точным при отсутствии выбросов. Самыми устойчивыми к выбросам являются стандартный IRLS, а также его модификации IRLS (loess) и IRLS (lowess). 

В случае шума с непостоянной дисперсией преимущество стандартного метода с итеративным обновлением весов пропадает, однако его модификация с использованием локальной регрессии при выделении тренда из остатков оказывается устойчивой к выбросам и точной без выделяющихся наблюдений. 

Если же разброс значений ряда очень велик, то при отсутствии выбросов точными являются стандартный SSA, оригинальный IRLS и модификация IRLS (loess). Однако устойчивой является IRLS (lowess).

Исходя из проведенного исследования можно сказать, что если у ряда нет растущей амплитуды и разброс значений небольшой, то можно использовать метод с обновлением весов. Он достаточно точный без выбросов и устойчивый к выделяющимся наблюдениям. В случае появления растущей амплитуды ряда и шума с непостоянной дисперсией, преимущество метода с обновлением весов пропадает. В таком случае следует использовать его модификацию с использованием локальной регрессии. Если же разброс значений у ряда большой,то следует использовать модификацию с выделением тренда с помощью взвешенной локальной регрессии, которая хорошо справляется с выбросами.  

%Исходя из проведенного исследования можно сказать, что если у ряда нет растущей амплитуды и разброс значений небольшой, то следует исследовать разделимость. Если сигнал хорошо отделяется от шума, то в присутствии выбросов рекомендуется использовать взвешенный метод наименьших квадратов. Если разделимости нет, то можно использовать последовательный вариант L1-SSA. В случае появления у ряда растущей амплитуды, последовательный метод начинает работать хуже. В таком случае следует использовать модификацию взвешенного метода наименьших квадратов. Можно также выдвинуть гипотезу о том, что в случае гетероскедастичного шума следует использовать модификацию IRLS с выделением тренда с помощью локальной регрессии, поскольку она тогда точнее выделяет тренд из остатков, а в случае шума с постоянной дисперсией --- модификацию с выделением тренда с помощью взвешенной локальной регрессии, которая хорошо справляется с выбросами.  

\begin{table}
	\caption{Оценки RMSE и MAD для трех рассмотренных примеров для $M=30$ реализаций ряда.}
	\label{tab_all}
	\centering
	\begin{tabular}{|c||c|c||c|c||c|c|}
		\hline
		\multicolumn{7}{|c|}{Оценки RMSE} \\
		\hline
		Method 	& 0\% & 5\% & 0\% & 5\% & 0\% & 5\% \\ 
		\hline
		Basic SSA & \textbf{0.184} & 0.653  & $\mathbf{2.16}$  & 5.96 & \textbf{0.215} & 459.6 \\
		l1pca & 0.217 & 0.250  & \textbf{2.45} & 2.87 & 0.256 & 21.11\\
		IRLS (orig.) & \textbf{0.184} & \textbf{0.206}  & 3.52  & 3.61 & \textbf{0.216} & 398.2 \\
		IRLS (loess) & 0.196 & \textbf{0.204}  & $\mathbf{2.31}$ &  $\mathbf{2.39}$ &\textbf{0.227} & 303.2 \\
		IRLS (median) & 0.210 & 0.223 & 2.84  & 2.86 & 0.256 & 38.21 \\
		IRLS (lowess) & 0.206 & \textbf{0.211}  & 2.59 &  2.63 & 0.243 & \textbf{0.301} \\
		\hline
		\multicolumn{7}{|c|}{Оценки MAD} \\
		\hline
		Method 	& 0\% & 5\% & 0\% & 5\% & 0\% & 5\% \\ 
		\hline
		Basic SSA & \textbf{0.143} & 0.505  & $\mathbf{1.08}$  & 2.91 & \textbf{0.164} & 168.2\\
		l1pca & 0.175 & 0.203 & \textbf{1.18}  & 1.36 & 0.197 & 3.145 \\
		IRLS (orig.) & \textbf{0.145} & \textbf{0.161}  & 1.63  & 1.66 & \textbf{0.165} & 88.54\\
		IRLS (loess) & 0.155 & \textbf{0.160} & $\mathbf{1.16}$ &  $\mathbf{1.21}$ & \textbf{0.174} & 31.180 \\
		IRLS (median) & 0.170 & 0.178  & 1.37  &  1.38 & 0.187 & 12.20\\
		IRLS (lowess) & 0.165 & \textbf{0.188}  & 1.26  & 1.29 & 0.180 & \textbf{0.187} \\
		\hline
	\end{tabular}
\end{table}


\section{Исследование числа итераций}\label{iterations}

В последовательном методе l1pca и методе IRLS есть дополнительный параметр: максимальное число итераций в цикле. В методе с итеративным обновлением весов задается максимальное число итераций для внешнего цикла и для внутреннего. Исследуем, какое количество итераций требуется для сходимости этих методов. На рисунках~\ref{pcal1_1} и~\ref{pcal1_2} показана зависимость ошибки RMSE от числа итераций для двух примеров (первый пример с экспоненциальным трендом, второй пример с быстрорастущей амплитудой ряда) при 5\% выбросов в случайных точках. Длина ряда в обоих случаях полагалась равной $N=240$. На рисунках~\ref{irls_1} и~\ref{irls_2} представлена зависимость RMSE от числа итераций во внешнем цикле для метода IRLS. Сравнения для различного числа итераций проводились на одинаковых реализациях ряда.

Из графиков видно, что отличия в ошибках при увеличении итераций совсем незначительные, и в последовательном методе можно было бы ограничиться и 5 итерациями.

Для метода IRLS по графикам видно, что ошибка практически перестает убывать при 5 итерациях внешнего цикла для первого примера и 20 итерациях для второго. Число итераций во внутреннем цикле можно оставить по умолчанию равным 5, так как отличия в таком случае совсем незначительные.


\begin{figure}[h]
	\begin{center}
		\begin{minipage}[h]{0.45\linewidth}
			\includegraphics[width=1\linewidth]{niter_l1pca_1}
			\caption{Пример 1: Зависимость RMSE от числа итераций для последовательного метода l1pca, $L=120$.} %% подпись к рисунку
			\label{pcal1_1} %% метка рисунка для ссылки на него
		\end{minipage}
		\hfill 
		\begin{minipage}[h]{0.45\linewidth}
			\includegraphics[width=1\linewidth]{niter_l1pca_2}
			\caption{Пример 2: Зависимость RMSE от числа итераций для последовательного метода l1pca, $L=120$.}
			\label{pcal1_2}
		\end{minipage}
		\begin{minipage}[h]{0.45\linewidth}
			\includegraphics[width=1\linewidth]{niter_IRLS_1}
			\caption{Пример 1: Зависимость RMSE от числа итераций для метода IRLS, $L=120$.}
			\label{irls_1}
		\end{minipage}
		\hfill 
		\begin{minipage}[h]{0.45\linewidth}
			\includegraphics[width=1\linewidth]{niter_IRLS_2}
			\caption{Пример 2: Зависимость RMSE от числа итераций для метода IRLS, $L=120$.}
			\label{irls_2}
		\end{minipage}
	\end{center}
\end{figure}

Попробуем взять меньшую длину окна и проверим, сколько итераций понадобится в таком случае. На рисунке~\ref{L84plot} изображена зависимость RMSE от числа итераций во внешнем цикле для метода IRLS при выбранной длине окна $L=84$. Требуется около 15 итераций для сходимости. На рисунке~\ref{L60plot} изображена аналогичная зависимость для длины окна $L=60$. Видно, что необходимо уже порядка 40 итераций. 

\begin{figure}[h]
	\begin{center}
		\begin{minipage}[h]{0.48\linewidth}
			\includegraphics[width=1\linewidth]{L84}
			\caption{Пример 1: Зависимость RMSE от числа итераций для метода IRLS, $L=84$.} %% подпись к рисунку
			\label{L84plot} %% метка рисунка для ссылки на него
		\end{minipage}
		\hfill
		\begin{minipage}[h]{0.48\linewidth}
			\includegraphics[width=1\linewidth]{L60_new}
			\caption{Пример 1: Зависимость RMSE от числа итераций для метода IRLS, $L=60$.} %% подпись к рисунку
			\label{L60plot} %% метка рисунка для ссылки на него
		\end{minipage}
	\end{center}
\end{figure}

Проанализируем, почему при меньшей длине окна требуется большее количество итераций. Напомним, что изначально мы инициализируем матрицы $\mathbf{U}$ и $\mathbf{V}$ с помощью первых $r$ компонент сингулярного разложения. Сравним разделимость сигнала от шума в присутствии выбросов и без выбросов.  В таблице~\ref{tab_rmse} представлены ошибки выделения сигнала по первым 3 компонентам для ряда без выбросов и для ряда с выбросами для разных длин окна. Ошибки выделения сигнала без выброса и с выбросом считаются на одних и тех же реализациях ряда (шум одинаковый, во втором случае добавлены выбросы). В последнем столбце представлена ошибка между восстановлением сигнала с выбросом и без выброса. Видим, что для меньшей длины окна отделимость от шума оказывается хуже.

\begin{table}
	\caption{Ошибка RMSE выделения сигнала для ряда без выбросов и для ряда с 5\% выбросов (для $M=10$ реализаций ряда).}
	\label{tab_rmse}
	\begin{center}
		\begin{tabular}{|c|c|c|c|}
			\hline
			L & Without outliers & With outliers & Difference \\ 
			\hline
			120 & 0.146 & 0.678 & 0.673 \\	
			84 &  0.159  & 0.684 & 0.677 \\
			60 & 0.190   & 0.785 & 0.779 \\
			%48 & 0.247 & 0.834 & 0.830 \\	
			\hline
		\end{tabular} \\
	\end{center}
\end{table}

Можем сделать вывод, что нельзя говорить, что достаточно определенного числа итераций для сходимости. Число итераций зависит от того, насколько хорошо сигнал отделяется от шума в присутствии выделяющихся наблюдений. 


\section{Реальный ряд}
Продемонстрируем работу методов на реальном примере. Рассмотрим ряд --- импорт товаров в США из Кувейта~\cite{RealSeries} с ноября 1993 года по ноябрь 2012 года. Имеются данные за каждый месяц. Длина ряда $N=229$. Увеличим размер выбросов в двух точках: $x_{83}$ и $x_{222}$.

Возьмем длину окна $L=60$, посмотрим на матрицу взвешенных корреляций~(\ref{wcor}) между восстановленными компонентами ряда. Матрица w-корреляций является одним из средств идентификации компонент, относящихся к сигналу. Подробнее матрица взвешенных корреляций описана в пособии~\cite{SSA}. Равенство взвешенной корреляции нулю является необходимым условием разделимости компонент ряда.

\begin{figure}[h]
	\begin{center}
		\begin{minipage}[h]{0.6\linewidth}
			\includegraphics[width=1\linewidth]{wcor}
			\caption{Матрица взвешенных корреляций, $L=60$.} %% подпись к рисунку
			\label{wcor} %% метка рисунка для ссылки на него
		\end{minipage}
	\end{center}
\end{figure}
Посмотрим также на элементарные восстановленные ряды~(\ref{recseries}). Будем восстанавливать сигнал по первым 5 компонентам. 

\begin{figure}[h]
	\begin{center}
		\begin{minipage}[h]{0.6\linewidth}
			\includegraphics[width=1\linewidth]{Rec_series}
			\caption{Элементарные восстановленные ряды.} %% подпись к рисунку
			\label{recseries} %% метка рисунка для ссылки на него
		\end{minipage}
	\end{center}
\end{figure}

Так как настоящий сигнал нам неизвестен, то попробуем на месте выбросов поставить пропуски, заполнить пропущенные значения с помощью метода gapfill~\cite{gapfill}, который имеется в пакете Rssa~\cite{Rssa}, а затем выделить сигнал с помощью классического SSA. Полученный сигнал будем считать истинным. Сравним стандартный SSA и метод с обновлением весов из статьи~\cite{Chen} с предложенными модификациями с выделением тренда с помощью локальной регрессии loess и взвешенной локальной регрессии lowess. Результат восстановления сигнала различными методами представлен на рисунке~\ref{real_result}.

\begin{figure}[!h]
	\center{\includegraphics[width=0.8\linewidth]{Real_result}}
	\caption{Результат восстановления сигнала различными методами.}
	\label{real_result}
\end{figure}

Можно заметить, что стандартный метод с обновлением весов плохо справляется с выбросом на конце ряда, где дисперсия шума увеличивается. Наиболее близким к "истинному"$~$сигналу оказывается модификация метода с использованием loess.


\conclusion
В работе были приведены и исследованы некоторые варианты модификации метода SSA с целью повышения устойчивости к выбросам.

%В главе~\ref{sec:BasicSSA} были введены основные понятия и обозначения, описан алгоритм базового метода SSA.

%В главе~\ref{sec:modifications} была введена общая схема метода с проекторами на пространство ганкелевых матриц и множество матриц ранга, не превосходящего $r$, без указания конкретной нормы. %Далее описано построение проектора на пространство ганкелевых матриц по нормам в $\mathbb{L}_2$, $\mathbb{L}_1$ и взвешенной норме в $\mathbb{L}_2$.
Был проведен обзор двух известных подходов к построению устойчивых к выбросам вариантов SSA: замена проекторов по норме в $\mathbb{L}_2$ на проекторы по норме в $\mathbb{L}_1$ и взвешенной норме в $\mathbb{L}_2$. Для построения $\mathbb{L}_1$-проектора на множество матриц ранга, не превосходящего $r$, был использован последовательный метод, который уже реализован в R-пакете~\cite{pcaL1}.  Второй подход соответствует методу с итеративным обновлением весов из статьи~\cite{Chen}, где точкам, содержащим выбросы, присваивается меньший вес. Устойчивые модификации SSA были систематизированы и изложены в едином стиле. %Также была предложена и описана реализация метода с итеративным обновлением весов на R.

Однако, метод из статьи~\cite{Chen} оказался неподходящим для рядов с шумом непостоянной дисперсии. Была предложена модификация этого метода, которая расширяет его применимость на случай нестационарного шума. В модификации метода предполагается замена параметра $\sigma_{ij}$, который в оригинальном методе полагается равным константе, на элементы траекторной матрицы тренда ряда из модулей остатков. Были рассмотрены несколько вариантов выделения тренда: скользящая медиана, локальная регрессия loess, а также взвешенная локальная регрессия lowess. Также была выведена формула для второго параметра метода с обновлением весов $\alpha$, зависящая от вероятности $\gamma$ (разделы \ref{sigma} и \ref{alpha}). Благодаря полученной формуле стала понятна интерпретация этого параметра, а также даны рекомендации по его выбору. %Изначально шум предполагался нормальный, с постоянной дисперсией. Было показано, что для нормального шума с меняющейся дисперсией формула для параметра $\alpha$ остается верной.

Для рассмотренных методов было проведено их сравнение по трудоемкости. В трудоемкость входит число итераций, которое в статьях предполагается фиксированным. В работе показано, что предположение о достаточности фиксированного числа итераций не верно. Теоретически вывести достаточное число итераций не удалось. Однако, в предположении, что число итераций не растет с увеличением длины ряда, так как зависит от разделимости, которая только улучшается, удалось теоретически сравнить трудоемкости. Метод с итеративным обновлением весов оказался менее трудоемким.

Все рассматриваемые устойчивые модификации SSA были реализованы на R \cite{tretyakova_2020}. Последняя часть работы посвящена численным экспериментам. Сравнение методов проводилось на модельных примерах, сначала без выделяющихся наблюдений, а затем при 1\% и 5\% выбросов в случайных точках ряда. Сравнение по точности проводилось по величине оценок ошибок RMSE и MAD. На основе проведенных экспериментов были выработаны некоторые рекомендации по применению методов, однако остается открытым вопрос, какой метод выделения тренда лучше использовать для выбора параметра $\sigma_{ij}$.
%На нескольких модельных примерах были исследованы свойства методов. 
Численные примеры подтвердили полученные формулы для порядка трудоемкости, а именно, было подтверждено, что 
трудоемкость используемой реализации метода L1-SSA имеет порядок $O(LK\log (2LK+Lr))$,
а метод с итеративным обновлением весов --- $O(LKr^2)$, где $r$ --- ранг сигнала, $L$ --- длина окна, $K = N-L+1$, $N$ --- длина временного ряда.


%Сначала рассматривался пример из работы~\cite{vkr}, но с добавлением большего количества выбросов. Было показано, что в таком случае метод IRLS оказался довольно точным и устойчивым.

%Далее ряд был заменен на ряд с шумом непостоянной дисперсии. На таком примере преимущество метода IRLS пропадает. Однако его модификация с выделением тренда с помощью локальной регрессии  оказывается устойчивой к выбросам и точной при отсутствии выбросов. В случае шума с постоянной дисперсией, но с большим разбросом значений ряда устойчивым методом оказалась модификация IRLS с использованием взвешенной локальной регрессии.

%Было проведено исследование числа итераций, необходимых для сходимости методов. Из него можно сделать вывод, что нельзя говорить, что достаточно определенного числа итераций для сходимости. Число итераций зависит от того, насколько хорошо сигнал отделяется от шума в присутствии выделяющихся наблюдений.

%Далее были проведены сравнения на измененных рядах с целью определения причины того, что для рассмотренных рядов наилучшими оказались разные методы. В результате выяснилось, что при уменьшении тренда метод l1pca восстанавливает сигнал точнее. Поэтому на первом примере ошибки последовательного примера были чуть больше, а в случае второго ряда этот метод оказался наилучшим.

% При росте числа выделяющихся наблюдений наиболее устойчивым оказался последовательный метод l1pca, но время его работы значительно превышает время работы стандартного метода. Метод robustSvd работает немного быстрее, чем l1pca, но ошибка восстановления сигнала этим методом оказалась значительно больше.

Таким образом, в работе были исследованы два варианта метода, которые являются более устойчивыми к выделяющимся наблюдениям, чем стандартный SSA, а также предложено несколько устойчивых модификаций, подходящих для рядов с гетероскедастичным шумом.

% Один из методов выделяет сигнал достаточно точно и является устойчивым, но время работы алгоритма слишком большое. Другой метод работает быстрее, но восстанавливает сигнал неточно. Одним из решений данной проблемы может являться модификация метода robustSvd. Если изменить метод таким образом, чтобы компоненты находились в порядке убывания собственных значений, то метод должен работать точнее. Для этого можно прогонять метод один раз, сортировать полученные компоненты по убыванию собственных чисел, а затем запускать алгоритм еще раз, но за начальные приближения брать уже полученные компоненты. Тогда время работы метода увеличится совсем ненамного, а точность возрастет.

% Для начала было проведено сравнение нескольких вариантов оценки сигнала стандартным методом L2-SSA с различными параметрами: SSA с большой длиной окна с восстановлением по трем компонентам, SSA с небольшой длиной окна (с восстановлением по одной компоненте и по трем компонентам), SSA с маленькой длиной окна с восстановлением по одной компоненте, SSA с проекцией. Эти методы сравнивались с результатом фильтрации с помощью треугольного фильтра. Скользящая медиана также была рассмотрена, и было показано, что данный фильтр является устойчивым при большом размере выбросов, но при отсутствии выделяющихся наблюдений ошибка оказывается достаточно большой, поэтому подробно в работе рассматривались только линейные фильтры.
%
% В результате проведенных исследований среди вариантов метода L2-SSA можно сделать вывод, что метод SSA с большой длиной окна при отсутствии выделяющихся наблюдений является наиболее точным до величины выброса, в 7 раз большей дисперсии шума, но, при б\'ольших значениях выброса лучшим оказывается фильтр Бартлетта.  При восстановлении методом L2-SSA с небольшой длиной окна при таком размере выброса ошибка растет медленнее, хотя при отсутствии выброса ошибка восстановления данным методом была больше, чем ошибка метода L2-SSA с большой длиной окна.
%
%Затем была произведена замена проектора на ганкелевы матрицы на проектор по норме в $\mathbb{L}_1$, и проведено сравнение метода L2-SSA с L2SVD-L1H-SSA. Было показано, что при замене на проектор по норме в $\mathbb{L}_1$ оценка MSE для некоторых методов уменьшается, а для некоторых увеличивается. Однако оценка MAD в присутствии выделяющихся наблюдений становится меньше для всех методов. Был сделан вывод, что использование проектора по норме в $\mathbb{L}_1$ оказывает незначительное влияние на повышение устойчивости метода.
%
%Сравнение трех вариантов реализации метода L1-SSA между собой и с методом L2-SSA с большой длиной окна показало, что при отсутствии выбросов ошибка MSE оказывается наименьшей для стандартного метода L2-SSA, но наиболее устойчивым методом является L1-SSA с регуляризацией. Однако если сравнивать ошибки MAD, то наилучшим методом как при отсутствии выбросов, так и с выделяющимися наблюдениями, является метод с регуляризацией. Важно отметить, что последовательный метод и метод с регуляризацией оказались точнее и устойчивее метода, предложенного в статье~\cite{Hassani}.
%
%Таким образом, в работе были предложены два варианта метода L1-SSA, которые являются устойчивыми к выделяющимся наблюдениям, но они оказываются гораздо более трудоемкими, чем методы L2-SSA и L2SVD-L1H-SSA.


%\\
\nocite{*}\bibliography{biblio_report}
\bibliographystyle{gost2008}
%\addcontentsline{toc}{section}{Список литературы}




\end{document}

