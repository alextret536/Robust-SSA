\documentclass[specialist,
substylefile = spbu.rtx,
               subf,href,colorlinks=true, 12pt]{disser}

\usepackage[a4paper,
            mag=1000, includefoot,
            left=3cm, right=1.5cm, top=2cm, bottom=2cm, headsep=1cm, footskip=1cm]{geometry}
\usepackage[T2A]{fontenc}
\usepackage[cp1251]{inputenc}
\usepackage[english,russian]{babel}
\usepackage{arydshln}
%\usepackage[utf8]{inputenc}
%\usepackage[T1]{fontenc}
\ifpdf\usepackage{epstopdf}\fi
\pagestyle{plain}
% подключаем hyperref (для ссылок внутри  pdf)
\ifpdf\usepackage{epstopdf}\fi
\usepackage{amsmath}
\usepackage{amssymb}
\DeclareMathOperator*{\argminB}{argmin}   % Jan Hlavacek
\newtheorem{predl}{Утверждение}
\newtheorem{proposition}{Утверждение}
\newtheorem{def1}{Определение}
\newtheorem{notice}{Замечание}
\newtheorem{lemma}{Лемма}
\newtheorem{theorem}{Теорема}
\newtheorem{algorithm}{Алгоритм}
\newenvironment{proof}[1][Доказательство]{\noindent\textbf{#1.} }{\hfill $\Box$ \\}

\DeclareMathOperator{\med}{med}
\DeclareMathOperator{\diag}{diag}
\DeclareMathOperator{\tr}{tr}
\newcommand{\tX}[1]{\mathsf{#1}}
\newcommand{\norm}[1]{\left\lVert#1\right\rVert}

\def\X{\mathbf{X}}
\def\Y{\mathbf{Y}}
\def\U{\mathbf{U}}
\def\V{\mathbf{V}}
\def\T{\mathrm{T}}

\newcommand{\E}{\mbox{E}}
\newcommand{\D}{\mbox{D}}

\usepackage{hyperref}

% Точка с запятой в качестве разделителя между номерами цитирований
%\setcitestyle{semicolon}

% Использовать полужирное начертание для векторов
\let\vec=\mathbf

% Включать подсекции в оглавление
\setcounter{tocdepth}{2}

\graphicspath{{fig/}}

%----------------------------------------------------------------
\begin{document}

%
% Титульный лист на русском языке
%
% Название организации
\institution{%
	Санкт-Петербургский государственный университет \\
	Прикладная математика и информатика \\
	Статистическое моделирование
}

\title{Отчет о производственной (научно-исследовательской) практике}

% Тема
\topic{\normalfont\scshape%
	Робастные варианты метода анализа сингулярного спектра}

% Автор
\author{Третьякова Александра Леонидовна}

% Научный руководитель
\sa       {Н.\,Э.~Голяндина}
\sastatus {к.\,ф.-м.\,н., доцент}

% Рецензент
%\rev      {А.\,Ю.~Шлемов}
%\revstatus{м.\,н.\,с.}

% Город и год
\city{Санкт-Петербург}
\date{2019}


\maketitle

%%
%% Titlepage in English
%%
%
\institution{%
	Saint Petersburg State University \\
	Applied Mathematics and Computer Science \\
	Computational Stochastics and Statistical Models
}
%
\title{Graduation Project}
%
%% Topic
\topic{\normalfont\scshape %
	Robust versions of the SSA method for time series analysis}
%
%% Author
\author{Tretyakova Aleksandra Leonidovna} % Full Name
%
%% Scientific Advisor
\sa       {N.\,E.~Goliandina}
\sastatus {Candidate of Physics and Mathematics, Associate Professor}
%
%% Reviewer
\rev      {A.\,Y.~Shlemov}
\revstatus{Junior Research Fellow}
%% City & Year
\city{Saint Petersburg}
\date{\number\year}
%
%\maketitle[en]

\pagestyle{footcenter}
\chapterpagestyle{footcenter}

\tableofcontents
\intro
%\section{Введение}
В реальной жизни часто возникают задачи исследования различных процессов с течением времени. Пусть имеется $x(t)$~--- функция, описывающая некоторый процесс во времени. Если произвести измерения через одинаковые промежутки времени $t_i$, где $i=1,\ldots,N$, тогда $x_i=x(t_i)$ представляют собой временной ряд $\tX{X}=(x_1, \ldots, x_{N})$. 

Для решения многих задач, к примеру, экономических, таких как планирование производства или инвестиций, оказывается полезным на основе данных за предшествующий период выделить основную динамику и тенденции, а также спрогнозировать развитие процесса. Работа посвящена изучению одного из методов исследования временных рядов~--- «Гусеница»-SSA (Singular Spectrum Analysis).  Метод нашел свое применение в задачах исследования климатических явлений~\cite{Climatic}, динамических систем~\cite{Dynamic1,Dynamic2} и во многих других областях. Данный метод позволяет получить разложение интересующего нас временного ряда $\tX{X}=(x_1, \ldots, x_{N})$ на интерпретируемые аддитивные составляющие:
\begin{equation*}
\mathsf{X}=\tX{S}+\tX{R},
\end{equation*}
где
$\tX{S}$~--- сигнал,
$\tX{R}$~--- шум, например, некоторый стационарный процесс. 

Традиционно при постановке задачи отделения сигнала от шума шум предполагается гауссовским.
% со сравнительно небольшой дисперсией. Таким образом, очень большие значения шума маловероятны.
 Однако на практике часто возникают выделяющиеся наблюдения или выбросы, которые можно интерпретировать как ошибки в данных или сбои измерительного прибора, значительно большие, чем размер шума. % В таком случае обычно рассматривают модель шума в виде (обычно гауссовского) шума с небольшой дисперсией и шума с большой дисперсией в отдельно взятых точках ряда.
 %Выбросы иногда соответствуют ошибкам в записи данных или сбоям измерительного прибора. 
 %Несмотря на то, что их можно отфильтровать, 
 Отфильтровать их оказывается непростой задачей, необходимо сначала разобраться со структурой ряда, чтобы понять, что данное значение является выбросом.
%(ссылка, например, на медианный фильтр)
Поэтому разработка исходно устойчивых к выбросам методов представляет интерес.

Ранее в работе~\cite{vkr} уже были предложены несколько устойчивых к выбросам вариантов метода, но они оказались слишком трудоемкими и алгоритмы работали очень долго. Поэтому требуются модификации этих методов, которые бы оставались устойчивыми, но время работы алгоритмов было меньше. В данной работе стоит задача предложить менее трудоемкие модификации робастных методов и сравнить их с базовым методом.

%Метод SSA имеет параметр, называемой длиной окна. При достаточно большой длине окна метод отделяет компоненты ряда друг от друга за счет так называемой разделимости \cite{SSA}. Благодаря разделимости метод SSA позволяет выделить сигнал из зашумленного ряда,
%отделить тренд от периодических компонент.

%Кроме того, при малой длине окна метод можно рассматривать как метод фильтрации ряда с помощью конечного линейного фильтра \cite{SSATimeSeries}.
%Одну из модификаций стандартного метода, SSA с центрированием, описанную в разделе 5.1 пособия~\cite{SSA}, также можно рассматривать как метод фильтрации. 

В методе SSA при выделении сигнала используются два проектора, которые могут строиться по различным нормам. Один из проекторов --- это проектор на пространство ганкелевых матриц, второй --- проектор на множество матриц ранга, не превосходящего $r$. В стандартном методе SSA используются проекторы в пространстве матриц по норме $\mathbb{L}_2$ (норма Фробениуса).

В качестве модификаций в работе~\cite{vkr} рассматривался стандартный прием использования аппроксимации (проекции) по норме в  $\mathbb{L}_1$ вместо  $\mathbb{L}_2$. Если построение проектора на ганкелевы матрицы по норме $\mathbb{L}_1$ не представляет трудности, то вычисление проектора на матрицы ранга, не превосходящего $r$, по норме $\mathbb{L}_1$ не имеет решения в замкнутой форме. Имеются методы, численно решающие приближенные задачи, но не известно достаточно хороших методов для задачи, которую требуется решить при построении проектора на матрицы ранга, не превосходящего $r$.

 Введем классификацию методов согласно используемым нормам. В общем случае методы будут называться LiSVD-LjH-SSA, где $i, j$ могут быть равны 1 или 2. LiSVD означает проектор на матрицы ранга, не превосходящего $r$, по норме в пространстве $\mathbb{L}_2$ (L2SVD) или $\mathbb{L}_1$ (L1SVD), а LjH --- проектор на пространство ганкелевых матриц по норме $\mathbb{L}_2$ (L2H) или $\mathbb{L}_1$ (L1H). Для более короткой записи будем называть стандартный метод с двумя проекторами по норме в $\mathbb{L}_2$ методом L2-SSA, а метод с двумя проекторами в $\mathbb{L}_1$ --- L1-SSA.
 % Будет рассматриваться L2SVD-L1H-SSA как переходный вариант между L2-SSA и L1-SSA. Вариант L1SVD-L2H-SSA также будет рассмотрен, но ожидаемо, что метод L1-SSA в результате будет более робастным. 
 
 
 Структура работы следующая. В главе~\ref{sec:BasicSSA} опишем базовый алгоритм метода SSA, введем необходимые понятия и обозначения, обсудим выбор параметров метода на основе теории метода SSA.

В главе~\ref{sec:modifications} рассматривается общая схема методов без указания конкретной нормы. Один из ключевых вопросов – это способы нахождения $\mathbb{L}_1$-проектора на множество матриц ранга, не превосходящего $r$. В работе рассматриваются два метода проектирования, один из них взят из статьи~\cite{Rodrigues}, он использует метод взвешенной медианы для решения задачи минимизации. Второй метод рассматривается в R-пакете в рамках построения устойчивого к выбросам анализа главных компонент~\cite{pcaL1book}. Приведены алгоритмы для каждого из способов. Также проведено теоретическое сравнение вариантов метода L1-SSA между собой. 

Еще одной идеей для достижения устойчивости метода к выделяющимся наблюдениям является присвоение значениям в точках, содержащих выбросы, меньший вес. Был рассмотрен алгоритм, описанный в статье~\cite{Chen}, использующий взвешенный метод наименьших квадратов. Данный метод оказался неподходящим для нестационарных рядов с растущей или убывающей амплитудой. Поэтому была предложена модификация метода. Она также описана в главе~\ref{sec:modifications}.

В конце главы~\ref{sec:modifications} произведен подсчет и сравнение теоретической трудоемкости описанных методов. Было рассмотрено два случая: когда траекторная матрица близка к квадратной, и случай с вытянутой траекторной матрицей. В обоих случаях теоретическая трудоемкость последовательного метода из статьи~\cite{pcaL1book} оказывается меньше теоретической трудоемкости метода из статьи~\cite{Rodrigues}.
 
Глава~\ref{sec: experiments} содержит численные сравнения, в которых исследуется влияние выброса на результат восстановления сигнала. В данной главе описывается сравнение одного из вариантов метода L1-SSA с классическим методом L2-SSA, а также с методом из статьи~\cite{Chen} и с модификацией этого метода. Сравнение проводилось при отсутствии выделяющихся наблюдений, при 1\% выбросов в случайных точках ряда и при 5\% выбросов. 

%Сравнение проводилось на нескольких примерах. Один из них уже был представлен в работе~\cite{vkr}, но было добавлено большее количество выбросов. Было показано, что метод, использующий взвешенный метод наименьших квадратов оказывается наиболее устойчивым среди других методов для первого примера. Предложенная в пункте~\ref{sec: IRLS} модификация для второго ряда дает наименьшую ошибку в присутствии выделяющихся наблюдений.

%Была осуществлена попытка перенести вывод о преимуществе метода с регуляризацией на другие ряды. Поэтому в качестве второго примера был рассмотрен другой ряд, у которого быстро растет амплитуда, и разброс значений получается большим.

В заключении описаны основные результаты работы, подведены итоги. 

%Работа в текущем семестре заключалась в рассмотрении идеи с добавлением маленьких весов точкам, содержащим выбросы. Был рассмотрен метод, использующий взвешенный метод наименьших квадратов. Так как были обнаружены недостатки этого метода в случае рядов с растущей или убывающей амплитудой, то была разработана модификация с целью борьбы с этими недостатками. Проведено сравнение теоретических трудоемкостей рассматриваемых методов и их сравнение между собой. На трех примерах было проведено сравнение методов, а также проверена значимость сравнений.

Работа в текущем семестре заключалась в исследовании параметров метода из раздела~\ref{sec: IRLS}, использующего взвешенный метод наименьших квадратов. Была выведена формула для параметра $\alpha$, зависящая от вероятности $\gamma$. Благодаря этой формуле стала понятна интерпретация параметра, а также даны рекомендации по выбору $\alpha$. Вывод формулы представлен в разделе~\ref{alpha}. Изначально шум предполагался нормальный, с постоянной дисперсией. Было показано, что для нормального шума с меняющейся дисперсией формула для параметра $\alpha$ остается верной. 

В модификации метода предполагается замена параметра $\sigma$ на тренд из ряда из модулей остатков. Были рассмотрены несколько вариантов выделения тренда: скользящая медиана, локальная регрессия loess, а также взвешенная локальная регрессия lowess. Проведены сравнения на численном примере~\ref{ex4} с гетероскедастичным шумом. Также было проведено исследование числа итераций, требующегося для сходимости последовательного метода и взвешенного метода наименьших квадратов. Кроме того, на основе проведенных исследований можно высказать предположения, какой из рассмотренных методов рекомендуется использовать в том или ином случае.

\chapter{Стандартный метод SSA и его свойства}\label{sec:BasicSSA}
%\section{Постановка задачи}
%Пусть $x(t)$~--- функция, описывающая некоторый процесс во времени. Обозначим $t_i$~--- моменты времени через одинаковые интервалы, где $i=1,\ldots,N$, тогда $x_i=x(t_i)$ представляют собой временной ряд $(x_1, \ldots, x_{N})$. 
%
%Рассмотрим вещественнозначный временной ряд
%%\begin{equation*}
%$\tX{X}=(x_1, \ldots, x_{N})$,
%%\end{equation*}
%где $N>2$~--- длина ряда. Предположим, что ряд ненулевой, т.е. $\exists i: x_i\ne0$.
%
%Задача состоит в разложении временного ряда на интерпретируемые аддитивные составляющие: 
%\begin{equation*}
%\mathsf{X}=\tX{S}+\tX{R},
%\end{equation*}
%где
%$\tX{S}$~--- сигнал,
%$\tX{R}$~--- шум, например, некоторый стационарный процесс. В данной работе будем рассматривать белый гауссовский шум.
%
%Необходимо исследовать устойчивость метода SSA к выделяющимся наблюдениям. В данной работе будем моделировать выбросы прибавлением в отдельно взятой точке достаточно большого значения и исследовать устойчивость метода в зависимости от этого значения.

\section{Алгоритм метода SSA}\label{sec: SSA}
Кратко опишем базовый алгоритм метода «Гусеница»-SSA, следуя \cite{SSA}.
\subsection{Вложение}
На первом шаге алгоритма выбирается некоторое целое число $L$: $1<L<N$, называемое \emph{длиной окна}. Исходный временной ряд переводится в последовательность многомерных векторов длины $L$. В результате образуются $K=N-L+1$ векторов вложения 
\begin{equation*} 
X_i=(x_{i},\ldots,x_{i+L-1})^\mathrm{T}, 1\le i\le K.
\end{equation*} 

\emph{Траекторной матрицей} ряда $\tX{X}$ называется матрица \\

$\mathbf{X}=[X_1:\ldots:X_K]=\left( \begin{array} {ccccc}
x_1 & x_2 & x_3&\ldots &x_{K} \\
x_2 & x_3 & x_4&\ldots &x_{K+1} \\
x_3 & x_4 & x_5&\ldots &x_{K+2} \\
\vdots& \vdots& \vdots&\ddots &\vdots \\
x_{L}& x_{L+1} & x_{L+2}&\ldots &x_{N} \\
\end{array}\right)$.\\

Заметим, что построенная таким образом траекторная матрица $\mathbf{X}$ является \emph{ганкелевой}, т.е. элементы, находящиеся на диагоналях $i+j=\mathrm{const}$, равны между собой.
\subsection{Сингулярное разложение}
Пусть $\mathbf{S}=\mathbf{X}\mathbf{X}^\textrm{T}$, обозначим $\lambda_1\ge\lambda_2\ge\ldots\ge\lambda_d>0$~--- ненулевые собственные числа матрицы $\mathbf{S}$, $U_1,\ldots,U_d$ — ортонормированная система собственных векторов матрицы $\mathbf{S}$, соответствующих собственным числам. 
\emph{Сингулярным разложением} матрицы $\mathbf{X}$ называется разложение
\begin{equation*} 
\mathbf{X}=\mathbf{X}_1+\ldots+\mathbf{X}_d=\sum \limits_{i=1}^{d}{\sqrt{\lambda_i}U_iV_i^\mathrm{T}},
\end{equation*} 
где $\sqrt{\lambda_i}$~--- \emph{сингулярные числа}, 
$U_i$~--- \emph{левые сингулярные вектора}, 
$V_i=\frac{1}{\sqrt{\lambda_i}}X^\mathrm{T}U_i$~--- \emph{правые сингулярные вектора}.

Набор $(\sqrt{\lambda_i},U_i,V_i)$ назовем \emph{$i$-ой собственной тройкой} сингулярного разложения.
\subsection{Группировка}
Разделим множество индексов $\{1,\ldots,d\}$ на $m$ дизъюнктных подмножеств $I_1,\ldots,I_m$. Пусть $I=\{i_1,\ldots,i_p\}$. Тогда \emph{результирующая матрица} $\mathbf{X}_I$ имеет вид
\begin{equation*} 
\mathbf{X}_I=\mathbf{X}_{i_1}+\ldots+\mathbf{X}_{i_p}.
\end{equation*} 

Таким образом, получаем разложение матрицы $\mathbf{X}$ в сгруппированном виде 
\begin{equation*} 
\mathbf{X}=\mathbf{X}_{I_1}+\ldots+\mathbf{X}_{I_m}.
\end{equation*} 
\subsection{Диагональное усреднение}
На последнем шаге каждая матрица $\mathbf{X}_{I_i}$ переводится в новый ряд с помощью усреднения элементов матрицы вдоль антидиагоналей $i+j=k+1$. Применяя диагональное усреднение к результирующим матрицам, получаем ряды $\tilde{\tX{X}}^{(k)}=(\tilde{x}^k_1,\ldots,\tilde{x}^k_{N})$. 

В результате получаем разложение исходного ряда $(x_1,\ldots, x_{N})$ в сумму $m$ рядов:
\begin{equation*}
x_n=\sum_{k=1}^{m}{\tilde{x}_n^{(k)}}.
\end{equation*} 
\section{Разделимость}\label{sec: partibility}
Понятие разделимости подробно описано в \cite{SSA}. Однако условия разделимости являются слишком жесткими и редко выполнены в реальных задачах. Поэтому введем понятие приближенной разделимости.

Для ряда $\tX{X}=(x_1, \ldots, x_{N})$ положим $X_{i,j}=(x_i,\ldots,x_j),~ 1\le i \le j <N$. Пусть $\tX{X}^{(1)}_N=(x_1^{(1)}, \ldots, x_{N}^{(1)}),~\tX{X}^{(2)}_N=(x_1^{(2)}, \ldots, x_{N}^{(2)}) $. Пусть 
\begin{equation*}
\rho_{i,j}^{(M)} = \frac{(X_{i,i+M-1}^{(1)},X_{j,j+M-1}^{(2)})}{\|X_{i,i+M-1}^{(1)}\| ~\|X_{j,j+M-1}^{(2)}\|},~~ i,j \ge 1,~ M\le N-\max(i,j),
\end{equation*}
где $\|~.~\|$ --- евклидова норма, $(~.~,~.~)$ --- скалярное произведение векторов.
Если знаменатель равен нулю, то предполагаем, что $\rho_{i,j}^{(M)} = 0$.

Число $\rho_{i,j}^{(M)}$ равно косинусу угла между векторами $X_{i,i+M-1}^{(1)}$ и $X_{j,j+M-1}^{(2)}$.
\begin{def1}
Ряды $\tX{X}^{(1)}_N$ и $\tX{X}^{(2)}_N$ называются $\varepsilon$-разделимыми при длине окна $L$, если 
\begin{equation*}
\rho^{(L,K)} = \max (\max \limits_{1\le i,j\le K} |\rho_{i,j}^{(L)}|, \max  \limits_{1\le i,j\le L} |\rho_{i,j}^{(K)}|) \le \varepsilon,
\end{equation*}
$K=N-L+1$. Если число $\varepsilon$ мало, то ряды называются приближенно разделимыми.
\end{def1}
 Если $\rho^{(L,K)} = 0$, то это соответствует точной разделимости.
 
 Введем понятие асимптотической разделимости. Пусть $\tX{X}^{(1)}=(x^{(1)}_1,\ldots,x^{(1)}_n,\ldots)$, $\tX{X}^{(2)}=(x^{(2)}_1,\ldots,x^{(2)}_n,\ldots)$ и для любого $N>2$ ряды $\tX{X}^{(1)}_N$ и $\tX{X}^{(2)}_N$ состоят из первых $N$ членов рядов $\tX{X}^{(1)}$ и $\tX{X}^{(2)}$. Тогда если выбрать последовательность длин окон $1<L=L(N)<N$, получим последовательность $\rho_N=\rho^{(L(N),K(N))}$.
 \begin{def1}
 	Если $\rho^{(L(N),K(N))} \to 0$ при некоторой последовательности $L=L(N)$, $N\to~\infty$, то ряды $\tX{X}^{(1)}$ и $\tX{X}^{(2)}$ называются асимптотически $L(N)$-разделимыми. Если для любой последовательности $L(N)$: $L(N) \to \infty, K(N) \to \infty$ ряды $\tX{X}^{(1)}$ и $\tX{X}^{(2)}$ асимптотически $L(N)$-разделимы, то они называются асимптотически разделимыми.
 \end{def1}

Асимптотическая раделимость выполняется для более широкого класса рядов, чем точная разделимость. К примеру, $e^{\alpha n}$ и $\sin(2\pi\omega n)$,  где $\alpha\ne 0,~ \omega \in  (0,0.5]$, асимптотически разделимы.

Для достижения лучшей разделимости необходимо выбирать большую длину окна $(L \sim N/2)$. Большая длина окна позволяет выделить сигнал из зашумленного ряда, отделить тренд от периодических компонент. Не имеет смысла брать длину окна, большую чем половина длины ряда, а маленькая длина окна может привести к смешиванию компонент ряда. 
\section{Ранг ряда}\label{sec:rank}
Пусть 	$\tX{X}_N=\tX{X}_{N}^{(1)}+\tX{X}_{N}^{(2)}$ и ряды $\tX{X}_{N}^{(1)}$ и $\tX{X}_{N}^{(2)}$ разделимы. Тогда в сингулярном разложении ряда 	$\tX{X}_N$ часть слагаемых относится к сингулярному разложению ряда $\tX{X}_{N}^{(1)}$, а другая часть --- к сингулярному разложению ряда $\tX{X}_{N}^{(2)}$. Необходимо выяснить, сколько слагаемых относится к первому ряду и как их идентифицировать.

Рассмотрим ряд  $\tX{X}_N = (x_1, \ldots, x_{N})$, пусть $L$ -- длина окна. 

Обозначим $\mathcal{L}^{(L)}=\text{span}(X_1, \ldots, X_K)$ --- траекторное пространство ряда $\tX{X}_N$, где $X_i=(x_{i},\ldots,x_{i+L-1})^T$ --- векторы вложения, $1\le i\le K$.
\begin{def1}
	Пусть $0< d \le \min (L,K) $. Будем говорить, что ряд $\tX{X}_N$ имеет $L$-ранг $d$, если $\dim{\mathcal{L}^{(L)}} = d$.
\end{def1}

Например, в случае экспоненциального ряда $e^{\alpha n}$ для любых $N$ и $L$ ранг ряда равен 1, а ранг гармонического ряда $\sin(2\pi \omega n+\phi)$ равен 2 при $\omega<1/2$ и 1 при $\omega=1/2$, $\phi \in [0,2\pi)$.

	
\chapter{Модификации метода SSA c проекторами по некоторой норме}\label{sec:modifications}
\section{Схема методов}		
Пусть имеется временной ряд $\tX{X}=(x_1, \ldots, x_{N})$.

Выбирается длина окна $L$, и исходный временной ряд переводится в последовательность многомерных векторов длины $L$. В результате образуются $K=N-L+1$ векторов вложения 
\begin{equation*} 
X_i=(x_{i},\ldots,x_{i+L-1})^\mathrm{T}, 1\le i\le K.
\end{equation*} 

%\emph{Траекторной матрицей} ряда $F$ называется матрица \\
%
%$\mathbf{X}=[X_1:\ldots:X_K]=\left( \begin{array} {ccccc}
%x_1 & x_2 & x_3&\ldots &x_{K} \\
%x_2 & x_3 & x_4&\ldots &x_{K+1} \\
%x_3 & x_4 & x_5&\ldots &x_{K+2} \\
%\vdots& \vdots& \vdots&\ddots &\vdots \\
%x_{L}& x_{L+1} & x_{L+2}&\ldots &x_{N} \\
%\end{array}\right)$.\\
%
%Заметим, что построенная таким образом траекторная матрица $\mathbf{X}$ является \emph{ганкелевой}, т.е. элементы, находящиеся на диагоналях $i+j=\mathrm{const}$, равны между собой.

Обозначим  $\mathcal{M}$ --- пространство матриц $L\times K$,

$\mathcal{M}_{\mathcal{H}}$ --- пространство ганкелевых матриц $L\times K$,

$\mathcal{M}_{r}$ --- пространство матриц ранга, не превосходящего $r$.

Определим следующие операторы: 

\begin{itemize} 
	\item{ Оператор вложения $\mathcal{T}:\mathbb{R}^N \rightarrow \mathcal{M}_{\mathcal{H}}: \mathcal{T} (\tX{X}) = \mathbf{X} $.
	}
	\item{ $\Pi_{r}:\mathcal{M}\rightarrow \mathcal{M}_r$ --- проектор на множество матриц ранга, не превосходящего $r$, по некоторой норме в пространстве матриц.
	}
	\item{
		$\Pi_{\mathcal{H}}:\mathcal{M} \rightarrow \mathcal{M}_{\mathcal{H}}$ --- проектор на пространство ганкелевых матриц по некоторой норме в пространстве матриц.
		%по норме Фробениуса посредством усреднения элементов на диагоналях $i+j=\text{const}$.
	}
\end{itemize}

В результате применения данных операторов получаем оценку сигнала:
\begin{equation*}
\tilde{\tX{S}} = \mathcal{T}^{-1} \Pi_{\mathcal{H}} \Pi_{r} \mathcal{T} (\tX{X}).
\end{equation*}

Это соответствует алгоритму SSA, описанному в разделе~\ref{sec: SSA}, для случая, когда восстановление производится по одной группе, состоящей из первых $r$ компонент.

Проекторы $ \Pi_{r}$ и $\Pi_{\mathcal{H}} $ можно строить по различным нормам. С точки зрения вычислений, удобно выбирать $\mathbb{L}_2$-норму для построения проекторов на пространство ганкелевых матриц и матриц ранга, не превосходящего $r$, поскольку целевая функция является гладкой и выпуклой, и решить задачу минимизации довольно просто, можно даже говорить о задании решения в явной форме. Однако при наличии выбросов норма Фробениуса оказывается недостаточно устойчивой. Норма в пространстве $\mathbb{L}_1$ является более устойчивой к выделяющимся наблюдениям, однако сложность в ее использовании состоит в негладкой и невыпуклой строго целевой функции, поэтому возникает проблема в применении стандартных методов оптимизации. 

В работе будут рассмотрены проекторы по нормам в пространствах $\mathbb{L}_2$ и $\mathbb{L}_1$. В стандартном методе SSA оба проектора $ \Pi_{r}$ и $\Pi_{\mathcal{H}} $ строятся по норме в пространстве $\mathbb{L}_2$. Будем называть его L2-SSA. Метод с проекцией на множество ганкелевых матриц в $\mathbb{L}_1$ и проекцией на множество матриц ранга, не превосходящего $r$, в $\mathbb{L}_2$ назовем L2SVD-L1H-SSA. Метод с обеими проекциями в $\mathbb{L}_1$ будем называть L1-SSA. 


\section{Способы нахождения $\mathbb{L}_1$-проектора на множество матриц ранга, не превосходящего $r$}\label{methods}
В отличие от проектора на множество матриц ранга $r$ по норме Фробениуса, построение данного проектора в пространстве $\mathbb{L}_1$ является вычислительно сложной задачей. 

Рассмотрим несколько методов построения проектора на множество матриц ранга, не превосходящего $r$, по норме в пространстве $\mathbb{L}_1$.

\subsection{Метод, использующий взвешенную медиану}\label{sec: rodrig}
Рассмотрим метод вычисления проекции на множество матриц ранга, не превосходящего $r$, по норме в пространстве $\mathbb{L}_1$, описанный в статье~\cite{Rodrigues}.

Для начала введем обозначения. Пусть
\begin{equation*}
\Y = [Y_1:\ldots:Y_p] = \left( 
\begin{array} {ccc}
y_{11}&\ldots &y_{1p} \\
\vdots&\ddots &\vdots \\
y_{n1}&\ldots &y_{np} \\
\end{array}\right).
\end{equation*}

В наших обозначениях $n$ соответствует длине окна $L$, $p$ соответствует $K=N-L+1$, где $N$ --- длина ряда, $r$ --- ранг.

Опишем подробно каждый шаг алгоритма.

1. Инициализация. Пусть
\begin{equation*}
m =
\left( \begin{array} {c}
\med{(|y_{11}|,\ldots, |y_{1p}|)} \\
\vdots\\
\med{(|y_{n1}|,\ldots, |y_{np}|)}  \\
\end{array}\right).
\end{equation*}
Возьмем в качестве начального значения $U_1^{0} = m/ \lVert m \rVert_2$.
~~ \\

2. 
Находим проекцию в $\mathbb{L}_1$ каждого столбца матрицы $\Y$ на вектор $U_1^{(k)}$, то есть для каждого $j=1,\ldots, p$ решаем задачу минимизации
%\begin{equation*}
%\min _{c_j}  \norm{ \biggl\lVert \left( 
%\begin{array} {c}
%y_{1j} \\
%\vdots \\
%y_{nj} \\
%\end{array}\right) - c_j * \left( 
%\begin{array} {c}
%u_1 \\
%\vdots \\
%u_n \\
%\end{array}\right) }
%\rVert_1
%\end{equation*} 
\begin{equation*}
\lVert (y_{1j},\ldots, y_{nj})^{\mathrm{T}} - c_j * (u_1, \ldots, u_n)^{\mathrm{T}} \rVert_1 \to \min _{c_j} 
\end{equation*} 
методом взвешенной медианы (обозначим здесь $U_1^{(k)}=(u_1, \ldots, u_n)^{\mathrm{T}}$). 

Далее нормируем полученный вектор $C = (c_1,\ldots, c_p)$ и полагаем 
\begin{equation*}
V_1^{(k)} = C / \lVert C \rVert _2.
\end{equation*}

3. 
Находим проекцию в $\mathbb{L}_1$ каждой строки матрицы $\Y$ на вектор $V_1^{(k)}$, то есть для каждого $i=1,\ldots, n$ решаем задачу 
\begin{equation*}
\lVert (y_{i1},\ldots, y_{ip}) - d_i * (v_1, \ldots, v_p) \rVert_1 \to \min _{d_i} 
\end{equation*} 
методом взвешенной медианы.

Далее нормируем полученный вектор $D = (d_1,\ldots, d_n)^{\mathrm{T}}$ и полагаем
\begin{equation*}
U_1^{(k+1)} = D / \lVert D \rVert _2.
\end{equation*}

4. Критерий остановки: продолжаем выполнять шаги 2 и 3, пока изменение в $\mathbb{L}_2$ вектора $U_1$ превосходит $\varepsilon$, то есть
\begin{equation*}
\text{While~~} \sum_{i=1}^n (U_{1_i}^{(k+1)} - U_{1_i}^{(k)})^2 > \varepsilon.
\end{equation*} 
По умолчанию $\varepsilon = 10^{-10}$.
~~ \\

5. В результате имеем $U_1^*, V_1^*$. Далее находим $\lambda_1^*$, решая задачу
\begin{equation*}
\sum_{i=1}^n \sum_{j=1}^p |y_{ij} - \lambda_1 U_1^* V_1^{*^{\T}}| \to \min _{\lambda_1} .
\end{equation*}

6. Из матрицы $\Y$ вычитаем первую компоненту
\begin{equation*}
\Y = \Y - \lambda_1^* \U_1^* \V_1^{*^{\T}}
\end{equation*}
и продолжаем искать остальные собственные тройки.

В итоге получаем представление матрицы $\Y$ в виде $\U \mathbf{\Lambda} \V^{\T}$, где $\U$ составлена из $U_1,\ldots, U_p$, а $\V$ состоит из $V_1,\ldots, V_p$. Точнее, мы нашли решение задачи минимизации функции $\lVert \Y -\U \mathbf{\Lambda} \V^{\T}\rVert_1$.
~~\\

Важно отметить, что
\begin{itemize}
	\item собственные числа на выходе не отсортированы по убыванию,
	\item собственные вектора получаются не ортогональными, в отличие от L2-SVD.
\end{itemize}

В пакете pcaMethods \cite{pcaMethods} имеется метод robustSvd для вычисления проекции в  $\mathbb{L}_1$ на множество матриц ранга, не превосходящего $r$. Более подробно метод описан в статье~\cite{pcaMethods_article}.

\subsection{Последовательный метод} \label{sec: l1pca}
Стоит задача проектирования матрицы $\mathbf{X}$ на множество матриц ранга, не превосходящего $r$. Задачу оптимизации можно представить в виде
\begin{equation*}
\min_{\mathbf{V},\mathbf{U}} \|\mathbf{X}^{\textrm{T}}-\mathbf{V}\mathbf{U}^{\textrm{T}}\|_1=\sum_{i=1}^{L}\|X_i - \mathbf{V}U_i\|_1,
\end{equation*}
где $\mathbf{V}$ --- матрица $K \times r$, $\mathbf{U}$ --- матрица $L \times r$. Столбцы матрицы $\mathbf{V}$ определяют главные компоненты. Матрица $\mathbf{E}=\mathbf{U}$$\mathbf{V}^{\textrm{T}}$ --- проекция $\mathbf{X}$ на множество матриц ранга, не превосходящего $r$, которую необходимо найти.

В пакете pcaL1 \cite{pcaL1} имеется метод l1pca, позволяющий вычислить проекцию в $\mathbb{L}_1$ на множество матриц ранга, не превосходящего $r$. Подробнее метод описан в статье \cite{pcaL1book}.

Приведем алгоритм в обозначениях предыдущего пункта. Пусть $\Y \in \mathbb{R}^{n\times p}$.
Задача выглядит следующим образом: 
\begin{equation*} 
\min_{\U,\V} \lVert \Y - \U \V^\T \rVert. 
\end{equation*}
Предположим, что $p\le n$ и матрица $\Y$ полного ранга.

\begin{enumerate}
	\item Инициализация $\mathbf{U}(0)\in \mathbb{R}^{n\times p}$, нормировка столбцов  $\mathbf{U}(0)$,
	\item $t:=t+1$,
	\item  $\mathbf{V}(t) = \argminB\limits_{\mathbf{V}\in \mathbb{R}^{p\times p}} \|\mathbf{Y}-\mathbf{U}(t-1)\mathbf{V}^{\mathrm{T}}\|_1$,
	\item  $\mathbf{U}(t) = \argminB\limits_{\mathbf{U}\in \mathbb{R}^{n\times p}} \|\mathbf{Y}-\mathbf{U}\mathbf{V}^{\mathrm{T}}(t)\|_1$,
	\item Нормировка столбцов $\mathbf{U}(t)$,
	\item  if $\mathbf{U}(t) \ne \mathbf{U}(t-1)$ (по критерию остановки) then Go to Step 2 \\
	else $\mathbf{U}:=\mathbf{U}(t);~ \mathbf{V}:=\mathbf{V}(t)$.
	
	Критерий остановки: 
	\begin{equation*}
	\text{While~~} ((\max_{i,j} |u_{ij}^{(k)}-u_{ij}^{(k-1)}|>\varepsilon)\text{~и~}(\text{iter}\le \text{MaxIter})),
	\end{equation*} по умолчанию $\varepsilon = 10^{-4}, \text{MaxIter} = 10,$
	\item End.
\end{enumerate}



%\begin{algorithm}\label{alg1}
%	~~
%	\begin{enumerate}
%		\item Инициализация $\mathbf{V}(0)\in \mathbb{R}^{r\times n}$, нормировка столбцов  $\mathbf{V}(0)$,
%		\item $t:=t+1$,
%		\item  $\mathbf{U}(t) = \argminB\limits_{\mathbf{U}\in \mathbb{R}^{m\times r}} \|\mathbf{X}-\mathbf{U}\mathbf{V}^{\mathrm{T}}(t-1)\|_1$,
%		\item  $\mathbf{V}(t) = \argminB\limits_{\mathbf{V}\in \mathbb{R}^{r\times n}} \|\mathbf{X}-\mathbf{U}(t)\mathbf{V}^{\mathrm{T}}\|_1$,
%		\item Нормировка столбцов $\mathbf{V}(t)$,
%		\item  if $\mathbf{V}(t) \ne \mathbf{V}(t-1)$ then Go to Step 2 \\
%		else $\mathbf{V}:=\mathbf{V}(t);~ \mathbf{U}:=\mathbf{U}(t)$,
%		\item End.
%	\end{enumerate}
%\end{algorithm}

Решаем задачу, меняя на каждой итерации $\mathbf{U}$ и $\mathbf{V}$ и разбивая исходную задачу на линейные подзадачи. $\mathbf{V}(0)$ можно инициализировать с помощью сингулярного разложения траекторной матрицы $\mathbf{X}$ в пространстве $\mathbb{L}_2$.

\section {Сравнение методов построения $\mathbb{L}_1$-проектора}

В данном разделе сравним методы из разделов~\ref{sec: rodrig} и \ref{sec: l1pca} между собой. 

Метод из раздела~\ref{sec: rodrig} основан на том, чтобы в стандартном методе L2-SSA заменить сингулярное разложение на другое разложение для повышения устойчивости к выбросам. Далее берутся первые $r$ компонент данного разложения, группируются, и применяется диагональное усреднение.

В статье~\cite{Hawkins}, где вводится метод robustSvd, используемый вместо обычного сингулярного разложения в методе из раздела~\ref{sec: rodrig}, нет точной формулировки задачи, решаемой данным методом. Также стоит заметить, что при взятии первых $r$ компонент разложения, мы не получим проекцию на пространство матриц ранга, не превосходящего $r$.

Приведем другие важные отличия между методами.

Важно отметить, что в алгоритме из раздела~\ref{sec: rodrig} поиск решения ведется последовательно для каждой компоненты, то есть по очереди находится каждая компонента, она вычитается, и далее производится поиск остальных компонент. В методе из раздела~\ref{sec: l1pca} все собственные векторы ищутся параллельно в виде матрицы. 

Заметим, что в методе, использующем взвешенную медиану, собственные числа не отсортированы по убыванию. Это может привести к уменьшению точности. Например, если мы нашли неточно первые несколько компонент, вклад которых достаточно мал, а затем ищем компоненту с большим вкладом, то мы уже не так точно найдем эту компоненту.

Еще одно существенное отличие в методах --- это то, что задачи минимизации целевой функции решаются по-разному. В одном варианте используется метод взвешенной медианы, а задача в последовательном методе решается с помощью решения задач линейного программирования.

\section {Взвешенный метод наименьших квадратов}\label{sec: IRLS}
Пусть $\mathbf{Y} = [Y_1, \ldots, Y_p] \in \mathbb{R}^{n\times p}$. Пусть $y_i$ --- $i$-ая строка матрицы  $\mathbf{Y},~ i=1,\ldots,n$. Напомним, что $n$ --- это длина окна $L$, а $p$ соответствует $K=N-L+1$, где $N$ --- длина ряда, $r$ --- ранг.

Идея заключается в замене исходной задачи 
\begin{equation*}
\min_{\hat{\mathbf{Y}} \in \mathcal{M}_r} \|\mathbf{Y}-\hat{\mathbf{Y}}\|_F.
\end{equation*} 
на задачу

\begin{equation}\label{task1}
\min_{\hat{\mathbf{Y}} \in \mathcal{M}_r} \|\mathbf{Y}-\hat{\mathbf{Y}}\|_\rho = \min_{\hat{\mathbf{Y}} \in \mathcal{M}_r} \sum_{i=1}^n \sum_{j=1}^p \rho(\frac{y_{ij}-\hat{y}_{ij}}{\sigma}).
\end{equation} 

Опишем метод, представленный в статье~\cite{Chen}, использующий для решения задачи~\ref{task1} взвешенный метод наименьших квадратов с обновлением весов на каждой итерации. Веса должны зависеть от того, насколько большой остаток в точке. В качестве $\rho(x)$ возьмем функцию Тьюки, которая имеет вид
 
 \begin{equation}\label{Tukey}
 \rho (x) = 
 \begin{cases}
\frac{1}{6}\alpha^2\{1-(1-(\frac{|x|}{\alpha})^2)^3\}, &|x|\le\alpha\\
\frac{1}{6}\alpha^2, &|x|>\alpha
\end{cases},
 \end{equation} 
где $\alpha$ --- параметр. Причина выбора такой функции в качестве метрики заключается в том, что на краях она не так сильно возрастает, как квадратичная функция, а точнее, данная функция остается постоянной при $|x|>\alpha$, что приводит к более устойчивому к выбросам результату.

Представим матрицу $\hat{\mathbf{Y}}$ в виде $\hat{\mathbf{Y}}=\mathbf{U}\mathbf{V}^{\textrm{T}}$, где $\mathbf{U}\in \mathbb{R}^{n\times r}, \mathbf{V}\in \mathbb{R}^{p\times r} $. Задача нахождения проекции на множество матриц ранга, не превосходящего $r$, сводится к задаче 

\begin{equation*}
\min_{\mathbf{U},\mathbf{V}} \|\mathbf{Y}-\mathbf{U}\mathbf{V}^{\textrm{T}}\|_\rho.
\end{equation*} 



Метод имеет параметры $\alpha$ и $\sigma$. Их произведение $\alpha \sigma $ по сути является порогом для принятия решения о том, является ли наблюдение выбросом или нет. Опишем алгоритм. 

\begin{enumerate}
	\item Инициализация $\mathbf{U}\in \mathbb{R}^{n\times r}$ и $\mathbf{V}\in \mathbb{R}^{p\times r}$,
	\item Выбор параметра $\alpha$,
	\item  Вычисление матрицы остатков $\mathbf{R}=\{r_{ij}\}_{i,j=1}^{n,p} = \mathbf{Y}-\mathbf{U}\mathbf{V}^{\mathrm{T}}$,
	\item  Обновление параметра $\sigma$,
	\item Вычисление матрицы весов $\mathbf{W}$, используя функцию $w(x)=\frac{\partial \rho(x)}{\partial |x|} \frac{1}{|x|}$:
	 \begin{equation*}
	w(x) = 
	\begin{cases}
	(1-(\frac{|x|}{\alpha})^2)^2, &|x|\le\alpha\\
	0, &|x|>\alpha
	\end{cases}, ~~~~\text{где}~ x=\frac{1}{\sigma}r_{ij},
	\end{equation*} 
	то есть для каждого элемента матрицы $\frac{1}{\sigma}\mathbf{R}$ применяем функцию $w(x)$,
	\item Вычисление матрицы $\mathbf{U}$ с помощью решения задачи
	\begin{equation}\label{taskA}
	(y_i-\mathbf{V}u_i)^\mathrm{T} \mathbf{W}_i (y_i-\mathbf{V}u_i) \to \min_{u_i},~~ i=1,\ldots n,
	\end{equation} 
	где $\mathbf{W}_i=\diag(w_i)\in \mathbb{R}^{p\times p}$ --- диагональная матрица, составленная из $i$-ой строки матрицы $\mathbf{W}$.
	\item Вычисление матрицы $\mathbf{V}$ с помощью решения задачи
	\begin{equation}\label{taskB}
	(Y_j-\mathbf{U}v_j)^\mathrm{T} \mathbf{W}^j (Y_j-\mathbf{U}v^j) \to \min_{v_j},,~~ j=1,\ldots p,
	\end{equation} 
	где $\mathbf{W}^j=\diag(W_j)\in \mathbb{R}^{n\times n}$ --- диагональная матрица, составленная из $j$-го столбца матрицы $\mathbf{W}$.
	\item Если не выполнен критерий сходимости или максимальное число итераций $\text{MaxIterAM}$ не достигнуто, то повторяем шаги 6--7 (alternating minimization), 
	\item Если не выполнен критерий сходимости или максимальное число итераций $\text{MaxIterIRLS}$ не достигнуто, то повторяем шаги 2--8 (iteratively reweighed least-squares), 
%	\item  if $\norm{\mathbf{W}^{1/2}\odot(\mathbf{Y}-\mathbf{U}\mathbf{V}^{\mathrm{T})}}^2_F \le \varepsilon_1$ and $iter \le \text{maxiter}$ then Go to Step 6, \\
%	\item  if $\norm{\mathbf{W}^{1/2}\odot(\mathbf{Y}-\mathbf{U}\mathbf{V}^{\mathrm{T})}}^2_F \le \varepsilon_2$ and $ITER \le \text{MAXITER}$ then Go to Step 3, \\
	\item End.
\end{enumerate}

Задачи~\ref{taskA} и \ref{taskB} решаются с помощью QR-разложения матриц $\mathbf{V}^\mathrm{T}\mathbf{W}_i \mathbf{V}$ и  $\mathbf{U}^\mathrm{T}\mathbf{W}^j \mathbf{U}$ соответственно.

Авторы статьи~\cite{Chen}, ссылаясь на проведенные численные эксперименты, предлагают выбрать $\sigma = 1.4826 \med {|R-\med {|R|}|}$, где $R$ --- это вектор, составленный из всех элементов матрицы остатков $\mathbf{R} = \{r_{ij}\}_{i,j=1}^{n,p}$, то есть
\begin{equation*}
 R~=~(r_{11},\ldots,r_{1p}; r_{21},\ldots r_{2p};\ldots;r_{n1},\ldots,r_{np}).
 \end{equation*} 
 Параметр $\alpha$ предлагается взять равным $4.685$. Также говорится, что максимальное количество итераций $N_{\alpha}$ и $N_{IRLS}$, необходимых для сходимости, достаточно взять 5 и 10 для достижения приемлемой точности.

Критерий сходимости:
\begin{equation*}
\norm{\mathbf{W}^{1/2}\odot(\mathbf{Y}-\mathbf{U}\mathbf{V}^{\mathrm{T})}}^2_F \le \varepsilon.
\end{equation*} 

У данного метода присутствуют существенные недостатки. Описанный алгоритм из статьи не подходит, к примеру, для рядов с растущей амплитудой. По умолчанию остатки нормировались на $\sigma$, которая задавалась константой. Но в случае растущей амплитуды данная нормировка приводит к неправильной идентификации точек с выбросами. В таком случае в точках с большой амплитудой веса некорректно занижаются, и точки, не содержащие выбросов, могут получить вес, меньший чем у выбросов в начале ряда. Поэтому приходим к выводу, что нормирующий параметр необходимо задавать динамически. Будем рассматривать вариант с заменой числа $\sigma$ на ряд, равный тренду ряда из модуля остатков. 

Изменения $\sigma$ оказывается недостаточным для того, чтобы сделать метод подходящим для рядов с растущей амплитудой. В методе имеется параметр $\alpha$, который влияет на то, какие точки воспринимать как выбросы, а какие --- нет. В классическом методе из статьи этот параметр также задается константой. Но из-за роста разброса остатков в точках без выбросов к концу ряда, многие из этих точек также получают искусственно заниженные веса. Попробуем исправить этот недостаток. 

Пусть $R=(r_1,\ldots,r_q)^\mathrm{T}$ --- вектор остатков, где $q=mn$. 
Если остатки $r_i \sim N(0, \sigma^2)$, то $|r_i| \sim N_{H}(\sigma^2)$, где $N_{H}(\sigma^2)$ --- полунормальное распределение с параметром $\sigma^2$. Пусть $\mathbb{E}|r_i| = \mu$. Для полунормального распределения известны следующие формулы для среднего и дисперсии:
\begin{equation*}
\mathbb{E}|r_i|=\sigma \sqrt{\frac{2}{\pi}}, ~~~ \mathbb{D}|r_i|=\sigma^2 (1-\frac{2}{\pi}) = C\mu^2, ~~ \text{где}~ C=\text{const}.
\end{equation*}

В алгоритме при вычислении весов проводится сравнение $|\frac{R}{\sigma}|$ с константой $\alpha$.  Если модуль превосходит $\alpha$, то веса обнуляются. Чем ближе значение модуля к $\alpha$, тем ниже вес в данной точке. Необходимо как-то осмысленно выбирать данный параметр. Выбор параметра $\alpha$ и его вероятностная интерпретация представлены в разделе~\ref{alpha}.
%Но так как в конце ряда разброс остатков больше, чем в начале, то при задании $\alpha$ константой, многие точки получают маленькие веса. Поэтому необходимо либо задавать пороговое значение динамически, либо ввести дополнительное преобразование, чтобы отрегулировать разброс остатков в начале и конце ряда. Квадратичная связь дисперсии и среднего модуля остатков наталкивает на мысль брать логарифм модулей остатков перед тем, как проводить сравнение с $\alpha$. Однако очень маленькие значения остатков могут привести к большим отрицательным значениям логарифма, поэтому попробуем для начала извлекать корень из ряда модулей остатков. 

Таким образом, получаем модификацию алгоритма, подходящую для рядов с быстрорастущей (или убывающей) амплитудой. Данная модификация отличается от исходного алгоритма шагом 4. 

\begin{enumerate}
	\item [4.a]  Ганкелизация матрицы $\mathbf{R}$ и получение ряда длины $N$ из остатков: $\tX{R} = \mathcal{T}^{-1} \Pi_{\mathcal{H}} (\mathbf{R})$,
	\item [4.b]  Вычисление $\sigma= (\sigma_1,\ldots,\sigma_{N})$ как тренд из ряда $|\tX{R}|$,
	\item [4.c]  Вычисление ряда $|\sigma^{-1}\tX{R}|$ и получение матрицы $\mathbf{R}^{*} = \{r_{ij}^*\}_{i,j=1}^{n,p} = \mathcal{T} (|\sigma^{-1}\tX{R}|)$,
	\item [5.] Вычисление матрицы весов $\mathbf{W}$, используя функцию $w(x)=\frac{\partial \rho(x)}{\partial |x|} \frac{1}{|x|}$:
	\begin{equation*}
	w(x) = 
	\begin{cases}
	(1-(\frac{|x|}{\alpha})^2)^2, &|x|\le\alpha\\
	0, &|x|>\alpha
	\end{cases},
	\end{equation*} 
	где $x=r_{ij}^*$.
\end{enumerate}

Посмотрим на график весов, чтобы убедиться, что только точки, содержащие выбросы, получили маленькие веса. На рисунках~\ref{series_IRLS} и~\ref{weights_IRLS} изображен график ряда c 5\% выбросов и веса, получившиеся в результате применения модификации метода. На графике весов видно, что точки, в которых содержались выделяющиеся наблюдения, получили нулевые веса. В остальных точках веса колеблются от 0.8 до 1.

\begin{figure}[h]
	\begin{center}
		\begin{minipage}[h]{0.48\linewidth}
			\includegraphics[width=1\linewidth]{series_IRLS}
			\caption{График ряда с 5\% выбросов.} %% подпись к рисунку
			\label{series_IRLS} %% метка рисунка для ссылки на него
		\end{minipage}
		\hfill 
		\begin{minipage}[h]{0.48\linewidth}
			\includegraphics[width=1\linewidth]{weights_IRLS}
			\caption{Веса.}
			\label{weights_IRLS}
		\end{minipage}
	\end{center}
\end{figure}

%\subsection{Исследование ряда из модулей остатков}
%
%В исходном алгоритме при вычислении матрицы весов $\mathbf{W}$ функция $w(x)$ применяется к каждому элементу матрицы $\frac{1}{\sigma}\mathbf{R}$, которая представляет из себя нормированную на определенную константу матрицу остатков. В модификации метода мы заменили константу $\sigma$ на ряд $\sigma=(\sigma_1,\ldots,\sigma_N)$, равный тренду ряда из модулей остатков. 
%
%Далее необходимо проверить, является ли модель ряда из модулей нормированных остатков $|\sigma^{-1}R|$ аддитивной или мультипликативной.  Пусть у нас есть ряд
%\begin{equation*}
%x_i=s_i+s_i^\beta\varepsilon_i,
%\end{equation*}
%где $s_i$ --- сигнал, $\varepsilon_i$ --- шум. Необходимо оценить параметр $\beta$. Если $\beta=0$, то модель является аддитивной. Если $\beta \ne 0$ (например, $\beta=1$), то модель мультипликативная. Для этого воспользуемся тестом Парка, описанным в статье~\cite{Park}. За оценку $\beta$ мы можем взять оценку по методу наименьших квадратов линейной модели 
%\begin{equation*}
%\log(|\widehat{e}_i|) = \beta \log(|\widehat{s}_i|) + \delta_i,
%\end{equation*}
%где $\widehat{s}_i$ --- оценка сигнала, $\widehat{e}_i = x_i-\widehat{s}_i$ --- остатки.

\subsection{Выбор параметра $\alpha$}\label{alpha}

Имеем ряд $\tX{X}=\tX{S}+\tX{\mathcal{E}}$, то есть $x_i=s_i+\varepsilon_i$, где $\varepsilon_i \sim N(0,\sigma_{\epsilon}^2)$. Пусть мы знаем сигнал $\tX{S}$, тогда остатки $\tX{R}$ соответствуют ряду $\mathcal{E}$.

Попробуем описать параметр $\alpha$ на вероятностном языке. Этот параметр является неким порогом, обнуляющим веса элементов матрицы остатков $\mathbf{R}^{*} = \{r_{ij}^*\}_{i,j=1}^{n,p} = \mathcal{T} (|\sigma^{-1}\tX{R}|)$:
	 \begin{equation*}
w(r_{ij}^*) = 
\begin{cases}
(1-(\frac{|r_{ij}^*|}{\alpha})^2)^2, &|r_{ij}^*|\le\alpha\\
0, &|r_{ij}^*|>\alpha
\end{cases}.
\end{equation*} 

После ганкелизации матрицы $\mathbf{R}$, получаем ряд из остатков $\tX{R}=\{r_i\}_{i=1}^N$. В случае известного сигнала $\tX{R}=\{r_i\}_{i=1}^N=\{\varepsilon_i\}_{i=1}^N$.
 Напомним, что $\sigma =(\sigma_1,\ldots,\sigma_N)$ --- тренд из ряда $|\tX{R}|$. Обозначим $\eta=\frac{|\varepsilon|}{\sigma}$.

Задавать порог $\alpha$ напрямую довольно трудно, поэтому зададим вероятность $\gamma$:
\begin{equation*}
\mathrm{P}(\eta\in (0,\alpha))=\gamma.
\end{equation*}

Тогда все $\frac{|\varepsilon_i|}{\sigma_i}$, которые меньше $\alpha$, имеют ненулевой вес, а остальные соответствуют выбросам, и их веса обнуляются.
Распишем:
\begin{equation*}
\mathrm{P}(\eta\in (0,\alpha))=\mathrm{P}(0\le \frac{|\varepsilon|}{\sigma} \le \alpha) = \mathrm{P}(\frac{|\varepsilon|}{\sigma} \le \alpha) = \gamma.
\end{equation*}

Для того, чтобы получить выражение для $\alpha$, нам необходимо знать распределение $\frac{|\varepsilon|}{\sigma}$.

\begin{proposition}\label{lemma1}
Пусть $x_i=s_i+\varepsilon_i$,  $\varepsilon_i\sim N(0,\sigma_{\varepsilon}^2)$. Пусть известен сигнал $s_i$. Тогда $\eta = r^{*}=\frac{|\varepsilon|}{\sigma}$ имеет полунормальное распределение $N_h(\frac{\pi}{2}),$ среднее $\E r^* = 1,$ дисперсия $\D r^* = \frac{\pi}{2}-1$.
\end{proposition}
\begin{proof}
	Если $\varepsilon_i \sim N(0,\sigma_{\varepsilon}^2)$, то 
	\begin{equation*}
	\frac{\varepsilon}{\sigma} = \frac{\varepsilon}{\E|\varepsilon|} \sim N(0,\frac{\sigma^2_{\varepsilon}}{\sigma^2_{\varepsilon}\frac{2}{\pi}}) = N(0,\frac{\pi}{2}).
	\end{equation*} 
	Тогда $\frac{|\varepsilon|}{\sigma} \sim N_h(\frac{\pi}{2})$ по определению полунормального распределения.
	
	Посчитаем среднее и дисперсию $r^*$.
	\begin{equation*}
	r^*=\frac{|\varepsilon|}{\sigma}=\frac{|\varepsilon|}{\E|\varepsilon|} = \frac{|\varepsilon|}{\sigma_{\varepsilon}\sqrt{\frac{2}{\pi}}}.
	\end{equation*}
	Тогда
	\begin{equation*}
	\E{r^*} = \frac{\E|\varepsilon|}{\sigma_{\varepsilon}\sqrt{\frac{2}{\pi}}} = 1,
	\end{equation*}
	\begin{equation*}
	\D{r^*} = \frac{\D|\varepsilon|}{\sigma_{\varepsilon}^2 \frac{2}{\pi}} = \frac{\sigma_{\varepsilon}^2(1-\frac{2}{\pi})}{\sigma_{\varepsilon}^2 \frac{2}{\pi}} = \frac{\pi}{2}-1.
	\end{equation*}
\end{proof}	

Из утверждения~\ref{lemma1} следует, что уравнение $\mathrm{P}(\frac{|\varepsilon|}{\sigma} \le \alpha) = \gamma$ перепишется в виде $F_h(\alpha) = \gamma$, где $F_h$ --- функция распределения $N_h(\frac{\pi}{2})$.

Известно, что функция распределения полунормального распределения $N_h(\delta)$ имеет вид
\begin{equation*}
F_h(x;\delta) = \frac{2}{\sqrt{\pi}}\int\limits_{0}^{x/\sqrt{2}\delta} e^{-z^2} dz = \text{erf} (\frac{x}{\sqrt{2}\delta}),
\end{equation*}
где $\delta$ --- параметр полунормального распределения, $\text{erf}$ --- функция ошибок, которая имеется во многих пакетах в R. 

Тогда получаем уравнение
\begin{equation*}
\text{erf}(\frac{\alpha}{\sqrt{2}\frac{\pi}{2}}) = \gamma,
\end{equation*}
отсюда
\begin{equation}\label{alpha_gamma}
\alpha = \frac{\sqrt{2}\pi}{2} \text{erf}^{-1}(\gamma).
\end{equation}
К примеру, для $\gamma = 0.95$ получим $\alpha \approx 3.079$. Для $\gamma = 0.99$ получим $\alpha \approx 4.046$.

Пусть теперь шум $\xi_i$ гетероскедастичный, $\xi_i = t_i\varepsilon_i,$ где $\varepsilon_i \sim N(0,\sigma^2_{\varepsilon})$, то есть дисперсия шума $\xi_i$ уже не равна константе.
Покажем, что тогда снова $r^* \sim N_h(\frac{\pi}{2})$ и дисперсия $r^*$ постоянна, тогда все рассуждения остаются верными в предположениях нормальности шума.

\begin{proposition}
	Пусть шум гетероскедастичный, $x_i= s_i + t_i\varepsilon_i,$ где $\varepsilon_i\sim N(0,1)$. Пусть известен сигнал $s_i$, пусть $t_i>0$. Тогда $r^*=\frac{|r|}{\sigma}\sim N_h(\frac{\pi}{2})$, где $R=\{r_{i}\}_{i=1}^N$ --- остатки, $\sigma=(\sigma_1,\ldots,\sigma_N)$ --- тренд из $|R|$. В таком случае среднее $\E r^* = 1,$ дисперсия $\D r^* = \frac{\pi}{2}-1$.
\end{proposition}

\begin{proof}

Если сигнал $s_i$ отделим от шума, то остатки $R=t_i\varepsilon_i$.

Известно, что если $\varepsilon \sim N(0,\sigma_\varepsilon^2),$ то $\mathbb{E}|\varepsilon| = \sigma_\varepsilon \sqrt{\frac{2}{\pi}}$, $\mathbb{D}|\varepsilon| = \sigma_\varepsilon^2 (1-\frac{2}{\pi}).$
%$$\mathbb{E}|R| = Ae^{an} \mathbb{E} |\varepsilon| = Ae^{an}\sqrt{\frac{2}{\pi}}.$$
 Выпишем нормированные остатки: $\frac{r}{\sigma} = \frac{t_i\varepsilon}{t_i\mathbb{E}|\varepsilon|} \sim N(0,\frac{1}{\frac{2}{\pi}}) = N(0,\frac{\pi}{2}).$

Тогда получим следующее: $\frac{|r|}{\sigma} \sim  N_h(\frac{\pi}{2})$ по определению полунормального распределения.


$$|r^*|=\frac{|t_i\varepsilon|}{\sigma}=\frac{t_i|\varepsilon|}{t_i\mathbb{E}|\varepsilon|} = \frac{|\varepsilon|}{\sqrt{\frac{2}{\pi}}}.$$ Тогда (аналогично шуму с постоянной дисперсией) посчитаем среднее и дисперсию: $$\mathbb{E}{r^*} = \frac{\mathbb{E}|\varepsilon|}{\sqrt{\frac{2}{\pi}}} = 1,~~   \mathbb{D}{r^*} = \frac{\mathbb{D}|\varepsilon|}{\frac{2}{\pi}} = \frac{(1-\frac{2}{\pi})}{\frac{2}{\pi}} = \frac{\pi}{2}-1.$$
\end{proof}

\section{Оценка трудоемкости методов}

Вопросу трудоемкости L1-SSA уделялось большое внимание во многих работах, посвященных построению $\mathbb{L}_1$-проекции на множество матриц ранга, не превосходящего $r$. В статье~\cite{Optimal} приведен алгоритм, решающий точно эту задачу. Пусть имеется вещественная траекторная матрица размерности $n\times p$ ранга $r$. В случае $n>p$ метод имеет трудоемкость $O(2^p)$. Трудоемкость в случае $n<p$, представляющем больший интерес в случае временных рядов, составляет $O(p^r)$. В данной работе в разделе~\ref{methods} мы рассматривали методы, решающие эту задачу с меньшей точностью, но более эффективно.
 
Сравним теоретические трудоемкости описанных алгоритмов. 

\subsubsection{Метод, использующий взвешенную медиану}

Вычислим порядок операций, требующихся для построения проекции матрицы размерности $n\times p$ на множество матриц ранга, не превосходящего $r$, методом из раздела~\ref{sec: rodrig}. Трудоемкость нахождения медианы составляет $O(N)$, где $N$ --- объем выборки. То есть на внутренний цикл необходимо $np$ операций. Алгоритм находит все $p$ собственных троек. Таким образом, порядок операций составляет
\begin{equation}\label{complexity_robustSvd}
\mathrm{T}_{\mathrm{robustSvd}} = O(p^2nN_{iter}),
\end{equation}
где $N_{iter}$ --- число итераций, необходимых для сходимости. Число итераций не превышает максимального числа итераций, которое можно задать константным, не зависящим от $n$ и $p$.


\subsubsection{Последовательный метод}

Вычислим трудоемкость последовательного алгоритма из раздела~\ref{sec: l1pca}. Трудоемкость составляет $O((pP_1+P_2)N_{iter})$, где $P_1$ и $P_2$ --- трудоемкость решения задач линейного программирования, а $N_{iter}$ --- общее количество итераций для сходимости метода, которое также считаем не зависящим от $n, p, r$. Согласно~\cite{Chvatal}, сложность вычисления задачи линейного программирования с $v$ переменными и $c$ ограничениями составляет $O(c \log v)$. В статье~\cite{pcaL1book} вычислено количество переменных и ограничений в этих задачах, и получено, что  трудоемкость может быть оценена как 
\begin{equation}\label{complexity_pcaL1}
\mathrm{T}_{\mathrm{l1pca}} = O(np\log (2pn+nr)N_{iter}),
\end{equation}


\subsubsection{Взвешенный метод наименьших квадратов}

Теоретическая трудоемкость метода из раздела~\ref{sec: IRLS} составляет, согласно статье~\cite{Chen},
\begin{equation}\label{complexity_IRLS}
\mathrm{T}_{\mathrm{IRLS}} = O(npr^2N_\alpha N_{IRLS}),
\end{equation}
где $N_\alpha$ и $N_{IRLS}$ --- общее количество итераций для решения задач~(\ref{taskA}), (\ref{taskB}) и сходимости взвешенного метода наименьших квадратов с обновлением весов. Количество итераций мы брали постоянными и не зависящими от $n, p$ и $r$. При подсчете трудоемкости авторы статьи~\cite{Chen} используют книгу~\cite{Golub}, в которой приводятся эффективные алгоритмы QR-разложения матрицы.

\subsubsection{Сравнение трудоемкостей}

Сравним теоретические трудоемкости последовательного метода и взвешенного метода наименьших квадратов. Необходимо сравнить~(\ref{complexity_pcaL1}) и~(\ref{complexity_IRLS}). Рассмотрим 2 случая. Задача сводится к сравнению $\log(n(2p+r))$ и $r^2$.

Пусть $n$ фиксировано, маленькое, а $p \sim N$. Это соответствует случаю, когда длина окна маленькая, и траекторная матрица вытянута. В таком случае только при $N$ порядка $10^{r^2}$ и больше трудоемкость последовательного метода оказывается хуже. Но такая большая длина ряда маловероятна, поэтому можно сделать вывод, что в таком случае последовательный метод должен работать быстрее.

Пусть траекторная матрица ряда близка к квадратной, то есть $n \sim N/2, ~ p \sim N/2$. Тогда, если поводить сравнение, то получаем, что надо сравнить $\frac{N}{2}(N+r)$ и $10^{r^2}$. Вычислив корни квадратного уравнения, получаем, что при $N$ порядка, меньшего $\sqrt{10^{r^2}}$, трудоемкость последовательного метода оказывается меньше. 

Можно сделать вывод, что теоретическая трудоемкость последовательного метода оказывается меньше теоретической трудоемкости взвешенного метода наименьших квадратов.  

Сравним теоретические результаты со временем работы методов на конкретном примере. Рассмотрим ряд, являющийся суммой экспоненты и синуса с гауссовским шумом. Пусть длина ряда $N=240$. 

Для того, чтобы иметь возможность сравнить время работы методов, необходимо критерии остановки сделать такими, чтобы методы выдавали примерно одинаковые по точности результаты. Однако это оказывается нетривиальной задачей, поэтому поставим максимальное количество итераций для каждого метода такие, чтобы точность оказывалась приемлемой для каждого из методов. Сравним время работы, учитывая количество итераций.

В таблице~\ref{tab_time} приведено время работы четырех методов с  1\% выбросов для 10 реализаций ряда. Время работы метода, использующего взвешенную медиану, намного больше времени работы остальных двух методов. Методы l1pca и IRLS для данного ряда с маленьким рангом работают примерно одинаково по времени.

\begin{table}
	\caption{Время работы программы и число итераций для различных методов для $M=10$ реализаций ряда.}
	\label{tab_time}
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline
		 	& l1pca & robustSvd & IRLS  \\ 
		\hline
		time & 54 \text{sec} & 237 \text{sec} & 39 \text{sec} \\
		$N_{iter}$ & 10 & 10 & 5*10 \\
		\hline
	\end{tabular}
\end{table}





\chapter{Вычислительные эксперименты} \label{sec: experiments}

%Одним из основных результатов работы~\cite{vkr} было то, что метод с регуляризацией на рассмотренном примере оказался наиболее устойчивым к выделяющимся наблюдениям среди остальных методов L1-SSA. Выброс находился всего в одной фиксированной точке ряда (в начальной либо средней части). 

Сравним результаты работы описанных методов на нескольких примерах. Для начала возьмем ряд с экспоненциальным трендом и гауссовским шумом. Затем рассмотрим ряд с быстрорастущей амплитудой. Рассмотрим случаи с гетероскедастичным шумом и с шумом с постоянной дисперсией, проведем сравнения для таких рядов. 

%Попробуем перенести вывод о преимуществе метода с регуляризацией на другие ряды с большим количеством выделяющихся наблюдений.
%
%Также сравним метод из статьи~\cite{Rodrigues} с последовательным методом и методом с регуляризацией.
%Рассмотрим два примера.

\section{Пример 1}\label{ex1}

Для начала рассмотрим пример из работы~\cite{vkr}, но добавим большее количество выбросов (1\% и 5\%) в случайных точках ряда. Проверим, какой из приведенных алгоритмов окажется наиболее устойчивым.

Длина ряда $N=240$. Рассмотрим временной ряд 
\begin{equation*}
f_n= e^{n/N}+\sin{(2\pi n/120+\pi/6)}+\varepsilon_n, ~ \varepsilon_n \sim N(0,1).
\end{equation*}
На~рис.~\ref{series_plot} изображен график ряда при  $1\%$ выбросов с величиной выброса $5f_i$. В случайно выбранных точках ряда $f_i$ значение заменяется на $f_i+5f_i$.

			\begin{figure}[!h]
				\center{\includegraphics[width=0.55\linewidth]{ser1plot}}
				\caption{График ряда при $1\%$ выбросов с величиной выброса $5f_i$.}
				\label{series_plot}
			\end{figure}		

 Cравнение будет проводиться по величине среднеквадратичной ошибки, согласованной с $\mathbb{L}_2$, которая вычисляется по формуле
\begin{equation}\label{MSE}
\text{MSE} = \mathbb{E} \left(\frac{1}{N} \sum \limits_{i=1}^{N}(s_i - \hat s_i )^2 \right),
\end{equation}
где $S$ --- сигнал, $\hat{S}$ --- его оценка.
Будем вычислять
\begin{equation*}
\text{RMSE} = \sqrt{\textrm{MSE}},
\end{equation*}
а также будем сравнивать методы по величине ошибки, согласованной с $\mathbb{L}_1$, которая имеет вид
\begin{equation}\label{MAD}
\text{MAD} = \mathbb{E} \left(\frac{1}{N} \sum \limits_{i=1}^{N}|s_i - \hat s_i | \right).
\end{equation}

Возьмем количество реализаций ряда $M=10$. Будем находить оценки математических ожиданий~(\ref{MSE}) и~(\ref{MAD}), а далее из оценки MSE будем извлекать корень, получая RMSE.

%Для начала попробуем сравнить работу методов без шума и без выделяющихся наблюдений. В таком случае стандартный метод L2-SSA, метод с весами IRLS и последовательный метод l1pca выдают нулевую ошибку с точностью до погрешности вычислений. Оценка ошибки восстановления сигнала без шума и выбросов методом, использующим взвешенную медиану, не равна нулю (около 0.043). Это доказывает, что метод решает не ту задачу, которую нам необходимо решить. Поэтому этот метод рассматривать не будем.

В таблице~\ref{tab1_a} представлены результаты сравнения для четырех методов. Выброс добавлялся заменой значения $f_i$ на $f_i+5f_i$. 

Ранг ряда равен $3$. Во всех методах берется длина окна $L=120$, равная половине длины ряда, и восстановление сигнала ведется по $3$ компонентам.

Для сравнения метода, использующего взвешенный метод наименьших квадратов, и его модификации, выберем следующие параметры:
\begin{enumerate}
	\item \textbf{Инициализация.} Возьмем в качестве начальных значений для обоих методов $\mathbf{U}=\mathbf{U}_r \mathbf{\Lambda}_r^{1/2}$, $\mathbf{V}=\mathbf{V}_r$, где $\mathbf{U}_r=[U_1:\ldots:U_r]$, $\mathbf{V}_r=[V_1:\ldots:V_r]$, $\mathbf{\Lambda}_r=\diag (\lambda_1,\ldots,\lambda_r)$ --- первые $r$ компонент сингулярного разложения траекторной матрицы.
	\item \textbf{Параметр $\alpha$.} Авторы статьи~\cite{Chen}, содержащей описание исходного метода, предлагают выбрать $\alpha=4.685$. Исходя из полученной формулы~(\ref{alpha_gamma}), связывающей параметр $\alpha$ и вероятность $\gamma$, получим для $\gamma=0.99$ значение параметра $\alpha=4.046$. Его и будем брать. Однако стоит заметить, что такая вероятностная интерпретация параметра $\alpha$ верна только для модификации метода.
\end{enumerate}

 Первая строка таблиц соответствует стандартному методу SSA с большой длиной окна $(L=120)$. 
 %Вторая строка соответствует методу с регуляризацией (ALM), описанным в работе~\cite{vkr}.
  Вторая строка --- метод l1pca из пакета pcaL1~\cite{pcaL1} (соответствует последовательному методу из раздела~\ref{sec: l1pca}). Третья строка соответствует стандартному методу из раздела~\ref{sec: IRLS} с использованием взвешенного метода наименьших квадратов. Четвертая, пятая и шестая строки соответствуют модификации взвешенного метода наименьших квадратов с различными вариантами выделения тренда (локальная регрессия с параметром сглаживания 0.35, скользящая медиана с длиной окна 80 и взвешенная локальная регрессия). Была использована реализация lowess в R из статьи~\cite{lowess}. Параметр сглаживания выбран 0.35, остальные параметры оставлены по умолчанию: число итераций равно 3, параметр $\delta$, требующийся для ускорения вычисления, равен $0.01(\max \limits_{i} {f_i} - \min \limits_{i} {f_i})$. Из-за того, что дисперсия шума постоянная, можем предположить, что модификации IRLS с различными вариантами выделения тренда будут работать примерно одинаково. Поэтому подробнее об отличиях модификаций с разными вариантами выделения тренда пока что говорить не будем.
  
  Жирным шрифтом в каждом столбце выделено лучшее значение и значение, которое незначимо отличается от лучшего. Проверка значимости сравнений представлена  далее в таблицах~\ref{tab: pval1a} и~\ref{tab: pval1b}.

 
 При сравнении методов со стандартным использовался пакет Rssa \cite{Rssa}. 
 

\begin{table}
	\caption{Оценки RMSE и MAD для различных методов для $M=10$ реализаций ряда.}
	\label{tab1_a}
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline
		Method 	& 0\% & 1\% & 5\%  \\ 
		\hline
		\multicolumn{4}{|c|}{Оценки RMSE} \\
		\hline
		Basic SSA & \textbf{0.402} & 0.611 & 0.712  \\
		l1pca     & 0.477 & \textbf{0.426} & \textbf{0.459} \\
		IRLS  (orig.)   & 0.459 & 0.490 & \textbf{0.440}\\
		IRLS (loess) & 0.491` & 0.492 & 0.494\\
		IRLS (median) & 0.520 & 0.523 & 0.528\\
		IRLS (lowess) & 0.502 & 0.501 & 0.498\\
		\hline
		\multicolumn{4}{|c|}{Оценки MAD} \\
		\hline
		Basic SSA & \textbf{0.366} & 0.464 & 0.593 \\
		l1pca     & 0.391 & \textbf{0.393} & \textbf{0.386} \\
		IRLS  (orig.)   & 0.374 & 0.370 & \textbf{0.367}\\
        IRLS (loess) & 0.407 & 0.410 & 0.414\\
        IRLS (median) & 0.430 & 0.433 & 0.436\\
        IRLS (lowess) & 0.417 & 0.410 & 0.414\\
		\hline
	\end{tabular}
\end{table}



%\begin{table}
%	\caption{Оценки MAD для различных методов для $M=10$ реализаций ряда.}
%	\label{tab1_b}
%	\centering
%	\begin{tabular}{|c|c|c|c|}
%		\hline
%		Method 	& 0\% & 1\% & 5\%  \\ 
%		\hline
%		Basic SSA & \textbf{0.139} & 0.164 & 0.505 \\
%		l1pca     & 0.151 & 0.188 & 0.186 \\
%		IRLS      & \textbf{0.140} & \textbf{0.152} & \textbf{0.145} \\
%		IRLS (modif.) & 0.156 & \textbf{0.160} & \textbf{0.159} \\
%		\hline
%	\end{tabular}
%\end{table}

%Из таблиц можно сделать вывод, что метод с регуляризацией остается наиболее устойчивым и при увеличении количества выбросов. 

При отсутствии выбросов наиболее точным все так же остается классический метод SSA. В присутствии выделяющихся наблюдений наиболее устойчивым являются последовательный метод и взвешенный метод наименьших квадратов.

%Также заметим, что метод из статьи~\cite{Rodrigues} работает неплохо для этого ряда. 

%Итак, можно сделать вывод, что метод IRLS для рассмотренного ряда работает достаточно точно при отсутствии выбросов, а также является наиболее устойчивым к выделяющимся наблюдениям среди остальных рассмотренных методов. Модификация метода IRLS также дает небольшую ошибку при появлении выделяющихся наблюдений.

\subsubsection{Проверка значимости сравнения}

Опишем подробнее, как происходит сравнение метода, выдающего наименьшую ошибку, с остальными методами. Проверим значимость сравнения по критерию для зависимых выборок.
%Для каждой реализации ряда будем одновременно находить ошибку для всех методов и вычислять попарные разности между ошибками. Далее на основе этих данных для каждой пары попробуем выяснить, являются ли методы одинаково эффективными. 
Проверим гипотезу, что MSE для некоторых методов равны между собой.

$\mathrm{H}_0: \mathbb{E}(\xi_1-\xi_2)=0$.
Имеем две выборки $X=(x_1,\ldots,x_M)$ и $Y=(y_1,\ldots,y_M)$ объема $M$. Обозначим $\bar{X}$ и $\bar{Y}$ --- их выборочные средние, $s_x^2$ и $s_y^2$ --- выборочные дисперсии, $\hat\rho$ --- коэффициент корреляции. 
Статистика критерия 
\begin{equation*}
t = \frac{\sqrt{M}(\bar{X}-\bar{Y})}{\sqrt{s_x^2+s_y^2-2s_xs_y\hat\rho}}
\end{equation*}
имеет асимптотически нормальное распределение. Критерий является двухсторонним.

Проверим, является ли отличие между этими методами значимым. В таблице~\ref{tab: pval1a} приведены p-value для сравнения среднеквадратичных ошибок для стандартного SSA и остальных методов без выделяющихся наблюдений. Все сравнения оказываются значимыми при уровне значимости 0.05.
В таблице~\ref{tab: pval1b} приведены p-value для сравнения ошибок для метода взвешенных наименьших квадратов и остальных методов при 5\% выбросов.  При уровне значимости 0.05 при 5\% выбросов сравнение с l1pca оказывается незначимым.


\begin{table}
	\caption{P-value для сравнения различных методов с наилучшим без выбросов.}
	\label{tab: pval1a}
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|c|}
			\hline
			0\% 	& l1pca & IRLS (orig.) & IRLS (loess) & IRLS (median) & IRLS (lowess)  \\ 
			\hline
			Basic SSA   & 0.01  & 0.003 & 0.001 & 0.000009 & 0.0001\\
			\hline
		\end{tabular} \\
	\end{center}
\end{table}

\begin{table}
	\caption{P-value для сравнения различных методов с наилучшим в присутствии выбросов.}
	\label{tab: pval1b}
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|c|}
			\hline
			5\% & Basic SSA	& l1pca & IRLS (loess) & IRLS (median) & IRLS (lowess)  \\ 
			\hline
			IRLS (orig.) & 2.8e-5   & \textbf{0.51} &   0.03  &  0.01 & 0.02  \\
			\hline
		\end{tabular} \\
	\end{center}
\end{table}

%\begin{table}
%	\caption{P-value для сравнения RMSE методов l1pca и IRLS (modif.) в зависимости от количества выбросов}
%	\label{tab: difference1}
%	\begin{center}
%		\begin{tabular}{|c|c|c|c|}
%			\hline
%			& 0\% & 1\% & 5\%   \\ 
%			\hline
%			l1pca          & 0.194  & 0.226 & 0.236  \\
%			IRLS (modif.)  & 0.199 & 0.201 & 0.197  \\
%			\hline
%			p-value        & 0.539 & 0.078  & \textbf{0.003} \\
%			\hline
%		\end{tabular} \\
%	\end{center}
%\end{table}
%
%
%\begin{table}
%	\caption{P-value для сравнения RMSE методов IRLS и IRLS (modif.) в зависимости от количества выбросов}
%	\label{tab: difference4}
%	\begin{center}
%		\begin{tabular}{|c|c|c|c|}
%			\hline
%			& 0\% & 1\% & 5\%   \\ 
%			\hline
%			IRLS           & 0.178 & 0.190 & 0.186 \\
%			IRLS (modif.)  & 0.199 & 0.201 & 0.197 \\
%			\hline
%			p-value        & 0.096 & 0.157 & 0.254 \\
%			\hline
%		\end{tabular} \\
%	\end{center}
%\end{table}

%Таким образом, увеличение числа выбросов не влияет на устойчивость метода с регуляризацией. Но важно убедиться, что этот метод будет работать хорошо не только на одном примере. Далее попробуем заменить ряд на другой, с большим разбросом значений, и исследуем поведение методов на таком примере.

\section{Пример 2}\label{ex4}
Попробуем рассмотреть не похожий на предыдущий пример ряд, у которого будет достаточно большой разброс значений, и исследуем устойчивость методов.

Длина ряда $N=240$. Рассмотрим ряд с гетероскедастичным шумом
\begin{equation*}
f_n=e^{4n/N} \sin(2\pi n/30) + Ae^{4n/N}\varepsilon_n, ~ \varepsilon_n \sim N(0,1).
\end{equation*}

График ряда представлен на рисунке~\ref{series_plot4}. Ранг ряда равен 2.

\begin{figure}[!h]
	\center{\includegraphics[width=0.5\linewidth]{ser4plot}}
	\caption{График ряда при $1\%$ выбросов с величиной выброса $5f_i$.}
	\label{series_plot4}
\end{figure}

В модификации метода IRLS будем выделять тренд следующими способами: скользящей медианой с длиной окна 80, локальной регрессией loess с параметром сглаживания 0.35 и взвешенной локальной регрессией lowess. Была использована реализация lowess в R, описанная в статье~\cite{lowess}. Параметр сглаживания 0.35, число итераций, как и в первом примере, равно 3, параметр $\delta$ равен $0.01(\max \limits_{i} {f_i} - \min \limits_{i} {f_i})$.  Также для сравнения вычислим реальный тренд из ряда из модулей остатков:

$$\mathbb{E}|R| = Ae^{4n/N} \mathbb{E} |\varepsilon| = Ae^{4n/N}\sqrt{\frac{2}{\pi}}.$$

На рисунке~\ref{trends} представлено выделение тренда из модуля остатков различными способами. Можно сделать следующие выводы. Прежде всего, локальная регрессия сильнее реагирует на выбросы, тренд чуть завышен. Скользящая медиана плохо работает на конце ряда, даже если брать длину окна в скользящей медиане большой. Взвешенная локальная регрессия хорошо справляется с выбросами и показывает результат, близкий к реальному тренду. 

\begin{figure}[!h]
	\center{\includegraphics[width=0.7\linewidth]{trends}}
	\caption{Выделение тренда из ряда $|R|$ несколькими способами.}
	\label{trends}
\end{figure}

Если сравнить точки, получившие нулевые веса различными методами, можно сделать следующие выводы: наибольшие выбросы все методы идентифицировали одинаково хорошо, скользящая медиана обнуляет наибольшее количество точек, нулевые веса получили также точки, не являющиеся выбросами.
Loess и lowess присваивают нулевые веса одинаковым точки и не обнуляют ничего лишнего. Есть предположение, что с использованием этих двух методов ошибка восстановления сигнала будет наименьшая. Отличие в этих методах состоит в том, что lowess чуть сильнее занижает веса в точках, не являющихся выбросами, чем loess. Возможно, из-за этого loess окажется лучше.

Результаты сравнения методов представлены в таблице~\ref{tab4_a}. Без выбросов модификация метода IRLS с выделением тренда с помощью локальной регрессии дает такую же маленькую ошибку, как и стандартный SSA. Наименьшая ошибка в присутствии выбросов получилась при использовании модификации IRLS со знанием реального тренда. На практике тренд из модуля остатков нам неизвестен, однако можно сделать вывод, что в данном случае качество выделения тренда важно. Также небольшую ошибку показала модификация IRLS с выделением тренда с помощью локальной регрессии. Метод l1pca при 5\% выбросов работает так же хорошо. 

\begin{table}
	\caption{Оценки RMSE для различных методов для $M=10$ реализаций ряда.}
	\label{tab4_a}
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline
		Method 	& 0\% & 1\% & 5\%  \\ 
		\hline
		\multicolumn{4}{|c|}{Оценки RMSE} \\
		\hline
		Basic SSA & $\mathbf{1.72}$  & 3.24  & 4.85 \\
		l1pca & \textbf{1.80} & \textbf{2.02}  & $\mathbf{1.93}$ \\
		IRLS (orig.) & 2.63 & 2.67 & 2.70 \\
		IRLS (loess) & $\mathbf{1.78}$ & \textbf{2.16} & $\mathbf{1.87}$ \\
		IRLS (median) & 2.24 & 2.19 & 2.41 \\
		IRLS (lowess) & \textbf{2.11} & \textbf{2.15} & 2.03 \\
		\hdashline
		IRLS (real trend) & \textbf{1.80} & \textbf{2.08} & $\mathbf{1.86}$ \\
		\hline
		\multicolumn{4}{|c|}{Оценки MAD} \\
		\hline
		Basic SSA & $\mathbf{0.95}$ & 1.37 & 2.54 \\
		l1pca & \textbf{0.89}  & \textbf{0.99} & $\mathbf{0.98}$ \\
		IRLS (orig.) & 1.24 & 1.28 & 1.30\\
		IRLS (loess) & $\mathbf{0.93}$ & \textbf{1.08} & $\mathbf{0.96}$ \\
		IRLS (median) & 1.09  & 1.10 & 1.17 \\
		IRLS (lowess) & \textbf{1.03} & \textbf{1.09} & 1.00 \\
		\hdashline
		IRLS (real trend) & $\mathbf{0.92}$ & \textbf{1.06} & $\mathbf{0.97}$ \\
		\hline
	\end{tabular}
\end{table}

%Для того, чтобы остатки имели вид $R = Ae^{4n/N}\varepsilon_i$, надо, чтобы сигнал хорошо отделялся от шума. Можно провести следующий эксперимент: возьмем в модификации IRLS на первой итерации за оценку сигнала настоящий сигнал $e^{4n/N} \sin(2\pi n/30)$ (его траекторную матрицу). 

%\subsubsection{Проверка значимости сравнения}

В таблицах~\ref{tab: pval4a} и~\ref{tab: pval4b} представлены p-value для проверки значимости сравнения наилучших методов с остальными при 0\% и 5\% выбросов.

\begin{table}
	\caption{P-value для сравнения различных методов с наилучшим без выбросов.}
	\label{tab: pval4a}
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|c|}
			\hline
			0\% 	& l1pca & IRLS (orig.) & IRLS (loess) & IRLS (median) & IRLS (lowess)  \\ 
			\hline
			Basic SSA   & \textbf{0.67}  & 0.03 & \textbf{0.69}  & 0.04 & \textbf{0.06}\\
			\hline
		\end{tabular} \\
	\end{center}
\end{table}

\begin{table}
	\caption{P-value для сравнения различных методов с наилучшим в присутствии выбросов.}
	\label{tab: pval4b}
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|c|}
			\hline
			5\% & Basic SSA	& l1pca & IRLS (orig.) & IRLS (median) & IRLS (lowess)  \\ 
			\hline
			IRLS (loess)    & 0.02 &   \textbf{0.68}  & 0.04 & 0.04 & \textbf{0.13}  \\
			\hline
		\end{tabular} \\
	\end{center}
\end{table}

\section{Пример 3}


Возьмем похожий ряд, но с шумом, имеющим постоянную дисперсию. Рассмотрим пример, предложенный в статье~\cite{Rodrigues}, и проведем для этого примера вычислительный эксперимент.

Пусть длина ряда $N=240$. Рассмотрим временной ряд 
\begin{equation*}
f_n= ne^{4n/N}\sin{(2\pi n/30)}+\varepsilon_n, ~ \varepsilon_n \sim N(0,1).
\end{equation*}

Ранг ряда равен 4. У такого ряда разброс собственных значений очень велик. Это может приводить к тому, что некоторые компоненты сигнала могут смешиваться с шумом. Однако шум рассматриваемого размера не портит отделимость сигнала от шума. Выбросы будут находиться в случайно выбранных точках ряда. Сравнение будем проводить при 1\% и 5\% выбросов, а также без выделяющихся наблюдений. В случайно выбранных точках $f_i$ значение будет заменяться на $f_i+1.5f_i$.

На~рис.~\ref{series_plot2} изображен график ряда при  $1\%$ выбросов с величиной выброса $1.5f_i$.

			\begin{figure}[!h]
				\center{\includegraphics[width=0.5\linewidth]{ser2plot_2}}
				\caption{График ряда при $1\%$ выбросов с величиной выброса $1.5f_i$.}
				\label{series_plot2}
			\end{figure}		

%Будем рассматривать большую длину окна $L=240$, это будет соответствовать выделению сигнала.


%Метод с регуляризацией ALM работает хуже на таком ряде, погрешность в случае отсутствия шума и выделяющихся наблюдений равна приблизительно 0.000003. Несмотря на то, что это значение мало, оно не так близко к нулю, как ошибки для методов L2-SSA и l1pca. При добавлении выбросов ошибка метода с регуляризацией начнет расти и метод перестанет быть наилучшим. 

%Возможные причины того, что на рассмотренных рядах лучшими оказываются разные методы:
%\begin{itemize}
%	\item увеличение длины ряда во втором примере, 
%	\item наличие у второго ряда больших по модулю значений, 
%	\item быстрый рост амплитуды второго ряда, 
%	\item наличие возрастающего тренда у первого ряда. 
%\end{itemize}
%
%Рассмотрим каждую причину отдельно.

% тем, что амплитуда второго ряда достаточно сильно растет, разброс значений оказывается слишком большим. Также такой эффект может быть связан с большой длиной второго ряда. Еще одно отличие между представленными рядами --- это наличие тренда у первого ряда. Рассмотрим каждую причину отдельно.

%Попробуем уменьшить длину ряда и проверить качество восстановление сигнала методом с регуляризацией.
%Рассмотрим похожий ряд, но с меньшей длиной.

%Пусть длина ряда $N=500$. Зададим ряд следующим образом.  Пусть
%\begin{equation*}
%f_n= 2ne^{4n/N}\sin{(2\pi n/60)}+\varepsilon_n, ~ \varepsilon_n \sim N(0,1).
%\end{equation*}

%На таком ряде проверим, будет ли метод с регуляризацией неустойчивым из-за сильно растущей амплитуды ряда. 

Результаты сравнения методов при различном проценте выделяющихся наблюдений представлены в таблице~\ref{tab2_a}.

%Из таблиц видно, что метод с регуляризацией оказывается уже не наилучшим методом.
 %В данном случае метод l1pca восстанавливает сигнал более устойчиво. %Следовательно, при переходе от ряда, на котором метод с регуляризацией наилучший, к ряду с большим разбросом значений, увеличение длины ряда не играет большой роли. 

%В методе с регуляризацией присутствует нормировка траекторной матрицы перед началом выполнения алгоритма (алгоритм подробно описан в работе~\cite{vkr}). Соответственно, проблемы в том, что в ряде присутствуют большие по модулю значения, нет. Поэтому большей проблемой оказывается быстрорастущая амплитуда ряда. 

%Видим, что метод robustSvd работает на данном ряде плохо как при отсутствии выделяющихся наблюдений, так и с выбросами. Намного лучше работает последовательный вариант L1-SSA. Однако при большом количестве выбросов он тоже дает ошибку.

Метод IRLS при такой быстрорастущей амплитуде ряда работает плохо, однако его модификация с выделением тренда с помощью взвешенной локальной регрессии достаточно хорошо справляется с восстановлением сигнала как при отсутствии выбросов, так и при большом их количестве. Последовательный метод на таком примере работает хуже. 

Проверка значимости сравнения наилучшего метода с остальными представлена в таблицах~\ref{tab: pval2a} (без выбросов) и~\ref{tab: pval2b} (с 5\% выбросов).

Без выделяющихся наблюдений все методы, кроме последовательного, незначимо отличаются от взвешенного метода наименьших квадратов. При 5\% выбросов все сравнения оказываются значимыми при уровне значимости 0.05.

\begin{table}
	\caption{Оценки RMSE и MAD для различных методов для $M=10$ реализаций ряда.}
	\label{tab2_a}
	\centering
	\begin{tabular}{|c|c|c|c|}
		\hline
		Method 	& 0\% & 1\% & 5\%  \\ 
		\hline
		\multicolumn{4}{|c|}{Оценки RMSE} \\
		\hline
		Basic SSA & \textbf{0.203} & 215.01 & 476.52  \\
		l1pca     & 0.228 & 10.246  & 21.270 \\
		IRLS (orig.)    & \textbf{0.196} & 220.40 & 398.2\\ 
		IRLS (loess) & \textbf{0.198}  & 15.254 & 54.212\\
		IRLS (median) & \textbf{0.213}  & 30.21 &  112.6\\
		IRLS (lowess) & \textbf{0.211}  & \textbf{0.217} &  \textbf{0.202}\\
		%IRLS (real trend) & 0.240  & \textbf{0.257} &  \textbf{0.262}\\
		\hline
		\multicolumn{4}{|c|}{Оценки MAD} \\
		\hline
		Basic SSA & \textbf{0.157} & 82.9 & 168.2 \\
		l1pca & 0.171 & 2.180 & 3.145 \\
		IRLS (orig.) & \textbf{0.151} & 27.311 & 38.541 \\ 
		IRLS (loess) & \textbf{0.154} & 2.189 & 5.180 \\
		IRLS (median) & \textbf{0.158} & 14.182 & 30.121 \\
		IRLS (lowess) & \textbf{0.155} & \textbf{0.179} & \textbf{0.157} \\
		%IRLS (real trend) & 0.180 & \textbf{0.189} & \textbf{0.180} \\
		\hline
	\end{tabular}
\end{table}

%Посмотрим на осмысленность восстановления последовательным методом и методом взвешенных наименьших квадратов. На рисунке~\ref{recplot} изображены исходный сигнал и восстановление методом l1pca и IRLS. Видно, что восстановление и тем, и другим методами довольно осмысленно и практически не отличается друг от друга (два графика практически полностью наложились друг на друга). Выбросы обрезались хорошо и восстановленный сигнал совпал с исходным.

%	\begin{figure}[!h]
%	\center{\includegraphics[width=0.5\linewidth]{recplot}}
%	\caption{График восстановленного ряда двумя методами при $5\%$ выбросов с величиной выброса $1.5y_i$.}
%	\label{recplot}
%\end{figure}	

%Для того, чтобы проверить, что восстановление сигнала описанными методами осмысленно, попробуем рассмотреть сам ряд, включая выбросы, как оценку сигнала и посчитать среднее отклонение от истинного сигнала. Ошибка в таком случае должна получиться больше, чем оценки ошибок методов. Результаты представлены в таблице~\ref{tab2_c}. Действительно, оценки ошибок восстановления сигнала рассматриваемыми методами оказались меньше, чем ошибка, если в качестве оценки сигнала рассматривать исходный ряд.
%
%\begin{table}
%	\caption{RMSE и MAD для исходного ряда в качестве оценки сигнала.}
%	\label{tab2_c}
%	\centering
%	\begin{tabular}{|c|c|c|c|}
%		\hline
%		 	& 0\% & 1\% & 5\%  \\ 
%		\hline
%		RMSE & 0.97 & 542.9 & 953.8 \\
%		MAD & 0.77 & 39.1  & 114.7 \\
%		\hline
%	\end{tabular}
%\end{table}


%\subsubsection{Проверка значимости сравнения}

%В таблице~\ref{tab: difference2} приведены p-value для сравнения среднеквадратичных ошибок для последовательного метода l1pca и модифицированного метода с весами IRLS (modif.). При уровне значимости 0.05 сравнения оказываются незначимыми.

%\begin{table}
%	\caption{P-value для сравнения MSE методов l1pca и IRLS (modif.) в зависимости от количества выбросов}
%	\label{tab: difference2}
%	\begin{center}
%		\begin{tabular}{|c|c|c|c|}
%			\hline
%			& 0\% & 1\% & 5\%   \\ 
%			\hline
%			l1pca & 0.234 & 0.246  & 0.270    \\
%			IRLS (modif.)  & 0.240  & 0.257 & 0.262   \\
%			\hline
%			p-value	& 0.340  & 0.343 & 0.371  \\
%			\hline
%		\end{tabular} \\
%	\end{center}
%\end{table}

\begin{table}
	\caption{P-value для сравнения различных методов с наилучшим без выбросов.}
	\label{tab: pval2a}
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|c|}
			\hline
			0\% & Basic SSA	& l1pca & IRLS (loess) & IRLS (median) & IRLS (lowess) \\ 
			\hline
			IRLS (orig.) & \textbf{0.57} & 0.004 &  \textbf{0.82} & \textbf{0.13} & \textbf{0.49} \\
			\hline
		\end{tabular} \\
	\end{center}
\end{table}

\begin{table}
	\caption{P-value для сравнения различных методов с наилучшим в присутствии выбросов.}
	\label{tab: pval2b}
	\begin{center}
		\begin{tabular}{|c|c|c|c|c|c|}
			\hline
			5\% & Basic SSA	& l1pca & IRLS (orig.) & IRLS (loess) & IRLS (median) \\
			\hline
			IRLS (lowess) & 0.037 & 0.33 & 0.14 & 0.27 & 0.24\\
			\hline
		\end{tabular} \\
	\end{center}
\end{table}

%Проверим зависимость устойчивости метода с регуляризацией от параметров метода. Метод имеет 3 параметра. Наиболее важным из них является коэффициент регуляризации $\lambda$. Остальные параметры технические: начальное значение параметра $\mu$ в функции Лагранжа и коэффициент увеличения параметра $\mu$ на каждой итерации (шаг). 
%
%Увеличение максимального числа итераций во внутреннем цикле и уменьшение $\varepsilon$ в критерии сходимости значимых результатов не дают. 
%
%Попробуем менять коэффициент регуляризации и найдем оценки ошибок при отсутствии шума и выбросов. В идеальном случае должны получиться ошибки, близкие к нулю. В таблице~\ref{tab_lambda} находятся оценки RMSE для ряда без шума и выбросов при различных $\lambda$. 
%
%
%\begin{table}
%	\caption{Зависимость оценки RMSE для метода с регуляризацией от величины $\lambda$.}
%	\label{tab_lambda}
%	\centering
%	\begin{tabular}{|c|c|c|c|}
%		\hline
%		 	& $\lambda = 1\E{-}03$ & $\lambda = 1\E{-}05$ & $\lambda = 1\E{-}09$  \\ 
%		\hline
%		ALM & $2.7\E{-}06$ & $2.4\E{-}05$ & $3.9\E{-}11$  \\
%		\hline
%	\end{tabular}
%\end{table}
%
%Таким образом, на данном примере при уменьшении коэффициента регуляризации метод оказывается точнее.
%

% Попробуем уменьшить рост амплитуды ряда и исследовать устойчивость методов. Возьмем ряд с меньшей скоростью роста амплитуды и посмотрим на ошибки (таблицы~\ref{tab3_a} и \ref{tab3_b}).
% 
% \begin{table}
% 	\caption{Оценки RMSE для различных методов для $M=10$ реализаций ряда.}
% 	\label{tab3_a}
% 	\centering
% 	\begin{tabular}{|c|c|c|c|}
% 		\hline
% 		Method 	& 0\% & 1\% & 5\%  \\ 
% 		\hline
% 		Basic SSA & 0.201 & 2.376 & 10.76  \\
% 		%	ALM & 0.201 &  0.219 & 0.282  \\
% 		l1pca & 0.260 & 0.256 & 0.300 \\
% 		robustSvd & 1.45 & 1.59 & 3.368 \\ 
% 		IRLS & 0.206 & 0.218 & 0.237 \\ 
% 		IRLS (modif.) &  &  & \\
% 		\hline
% 	\end{tabular}
% \end{table}
% 
% \begin{table}
% 	\caption{Оценки MAD для различных методов для $M=10$ реализаций ряда.}
% 	\label{tab3_b}
% 	\centering
% 	\begin{tabular}{|c|c|c|c|}
% 		\hline
% 		Method 	& 0\% & 1\% & 5\%  \\ 
% 		\hline
% 		Basic SSA & 0.151 & 1.52 & 6.557 \\
% 		%ALM & 0.134 &  0.143 & 0.158  \\
% 		l1pca & 0.201 & 0.196 & 0.231 \\
% 		robustSvd & 0.879 & 0.94 & 1.96 \\ 
% 		IRLS & 0.154 & 0.169 & 0.178 \\ 
% 		IRLS (modif.) &  &  & \\
% 		\hline
% 	\end{tabular}
% \end{table}
%
%%\begin{equation*}
%%f_n= 0.5ne^{4n/N}\sin{(2\pi n/120)}+\varepsilon_n, ~ \varepsilon_n \sim N(0,1).
%%\end{equation*}
%
%%Для сокращения времени вычислений уберем шум и будем брать выброс в фиксированной точке ряда. Параметр $\lambda$ возьмем такой же, как был изначально ($1\E{-}03$), чтобы можно было понять, влияет ли скорость роста амплитуды на работу методов. 
%%
%%Для начала вычислим ошибку без выброса. Для метода с регуляризацией она равна $1.7\E{-}07$. Для метода robustSvd --- $1.6\E{-}04$. Остальные методы, как уже говорилось ранее, дают приблизительно нулевую ошибку.
%%
%%В случае выброса в фиксированной точке $f_{390}$ (взята ближе к концу ряда, чтобы значение выброса не было слишком маленьким) метод с регуляризацией и последовательный метод оказываются наиболее устойчивыми и дают ошибки RMSE $1.80\E{-}06$ и $1.56\E{-}07$ соответственно. 
%
%При уменьшении скорости роста амплитуды ряда, метод IRLS восстанавливает сигнал наиболее устойчиво.

%Одной из возможных причин того, что на разных рядах лучшими оказываются разные методы, является наличие возрастающего тренда у первого ряда. Первый ряд растет, а у второго ряда изменяется только амплитуда. Попробуем постепенно уменьшать рост первого ряда и исследуем устойчивость методов. 

%При уменьшении тренда ошибка RMSE метода l1pca начинает постепенно уменьшаться. На исходном ряде из первого примера без шума и с выделяющимся наблюдением в фиксированной точке (ближе к концу ряда) оценка RMSE равна $3\E{-}08$. Оценка RMSE для метода с регуляризацией в таком случае была равна $1.5\E{-}11$. При постепенном уменьшении тренда метод l1pca начинает работать лучше, и уже на ряде 
%\begin{equation*}
%f_n= e^{n/(4N)}+\sin{(2\pi n/120+\pi/6)}
%\end{equation*}
%ошибки метода l1pca и метода с регуляризацией равны $9.8\E{-}14$ и $1.1\E{-}11$ соответственно.
%Поэтому при переходе от первого примера ко второму основным отличием, влияющим на ошибки методов, является то, что у второго ряда меняется только амплитуда, а возрастающий тренд отсутствует. В таком случае ошибка последовательного метода оказывается меньше, чем его ошибка на первом примере с трендом. Поэтому на первом примере лучшим оказывается метод с регуляризацией, но затем при уменьшении тренда ошибки последовательного метода убывают, и на втором примере последовательный метод оказывается лучше.

%\section{Пример 3}
%
%Основное отличие первого и второго примера состояло в том, что у второго ряда был сильный рост амплитуды. Однако у первого ряда присутствовал растущий экспоненциальный тренд, что могло также повлиять на работу методов. Попробуем рассмотреть простой стационарный ряд, у которого не растет амплитуда, и сравним методы на таком примере. Длина ряда по-прежнему $N=240$. Рассмотрим ряд 
%\begin{equation*}
%f_n=\sin{(2\pi n/120+\pi/6)}+\varepsilon_n, ~ \varepsilon_n \sim N(0,1).
%\end{equation*}
%
%График ряда при  $1\%$ выбросов с величиной выброса $5y_i$ изображен на~рис.~\ref{series_plot3}.
%
%\begin{figure}[!h]
%	\center{\includegraphics[width=0.5\linewidth]{ser3plot}}
%	\caption{График ряда при $1\%$ выбросов с величиной выброса $5y_i$.}
%	\label{series_plot3}
%\end{figure}
%
%Результаты сравнения представлены в таблице~\ref{tab3_a}. Метод IRLS и его модификация оказываются для этого ряда устойчивее, чем последовательный метод. 
% 
%\begin{table}
%	\caption{Оценки RMSE и MAD для различных методов для $M=10$ реализаций ряда.}
%	\label{tab3_a}
%	\centering
%	\begin{tabular}{|c|c|c|c|}
%		\hline
%		Method 	& 0\% & 1\% & 5\%  \\ 
%		\hline
%		\multicolumn{4}{|c|}{Оценки RMSE} \\
%		\hline
%		Basic SSA & \textbf{0.153} & \textbf{0.161} & 0.254 \\
%		l1pca & 0.165 & 0.187  & 0.207 \\
%		IRLS & \textbf{0.150} & \textbf{0.163} & \textbf{0.183}\\
%		IRLS (modif.) & 0.191 & \textbf{0.167} & \textbf{0.171}\\
%		\hline
%		\multicolumn{4}{|c|}{Оценки MAD} \\
%		\hline
%		Basic SSA & \textbf{0.123} & \textbf{0.122} & 0.192 \\
%		l1pca & 0.134 & 0.147 & 0.154 \\
%		IRLS & \textbf{0.123} & \textbf{0.122} & \textbf{0.139} \\
%		IRLS (modif.) & 0.130 & 0.150 & \textbf{0.137}\\
%		\hline
%	\end{tabular}
%\end{table}
%
%
%%\begin{table}
%%	\caption{Оценки MAD для различных методов для $M=10$ реализаций ряда.}
%%	\label{tab3_b}
%%	\centering
%%	\begin{tabular}{|c|c|c|c|}
%%		\hline
%%		Method 	& 0\% & 1\% & 5\%  \\ 
%%		\hline
%%		Basic SSA & \textbf{0.123} & 0.122 & 0.192 \\
%%		l1pca & 0.134 & 0.147 & 0.154 \\
%%		IRLS & \textbf{0.123} & 0.122 & \textbf{0.139} \\
%%		IRLS (modif.) & 0.130 & 0.150 & \textbf{0.137}\\
%%		\hline
%%	\end{tabular}
%%\end{table}
%
%%\subsubsection{Проверка значимости сравнения}
%%
%%В таблице~\ref{tab: difference3} приведены p-value для проверки значимости отличия последовательного метода и метода с весами. Как и в примере из пункта~\ref{ex1}, при отсутствии выбросов при уровне значимости 0.05 отличие не значимо, а с выбросами --- значимо.
%%
%%\begin{table}
%%	\caption{P-value для сравнения MSE методов l1pca и IRLS в зависимости от количества выбросов}
%%	\label{tab: difference3}
%%	\begin{center}
%%		\begin{tabular}{|c|c|c|c|}
%%			\hline
%%			& 0\% & 1\% & 5\%  \\ 
%%			\hline
%%			l1pca & 0.194 & 0.231  & 0.230   \\
%%			IRLS & 0.164 & 0.165 & 0.195    \\
%%			\hline
%%			p-value	& 0.105 & 0.003 & 0.002   \\
%%			\hline
%%		\end{tabular} \\
%%	\end{center}
%%\end{table}
%
%\begin{table}
%	\caption{P-value для сравнения различных методов с наилучшим без выбросов.}
%	\label{tab: pval3a}
%	\begin{center}
%		\begin{tabular}{|c|c|c|c|}
%			\hline
%			0\% 	& l1pca & IRLS & IRLS (modif.)   \\ 
%			\hline
%			Basic SSA         &  &  &   \\
%			\hline
%		\end{tabular} \\
%	\end{center}
%\end{table}
%
%\begin{table}
%	\caption{P-value для сравнения различных методов с наилучшим в присутствии выбросов.}
%	\label{tab: pval3b}
%	\begin{center}
%		\begin{tabular}{|c|c|c|c|}
%			\hline
%			5\% & Basic SSA	& l1pca & IRLS (modif.)   \\ 
%			\hline
%			IRLS         &  &  &   \\
%			\hline
%		\end{tabular} \\
%	\end{center}
%\end{table}



%\section{Выводы} \label{sec: results}
%
%Исходя из полученных результатов сравнений методов, можем сделать следующие выводы.
%
%%Увеличение числа выбросов в примере из работы~\cite{vkr} не влияет на устойчивость метода с регуляризацией.
%
%%Преимущество метода с регуляризацией исчезает при замене ряда на ряд с большим разбросом значений и быстрорастущей амплитудой. Наилучшим в таком случае оказывается последовательный вариант L1-SSA. 
%
%%При отсутствии выбросов и шума нулевую ошибку с точностью до погрешности вычислений дают только стандартный метод L2-SSA, последовательный метод l1pca и IRLS. Ошибка метода robustSvd достаточно большая, то есть метод не решает необходимую нам задачу.
%
%Хорошими оказались два метода: последовательный метод и взвешенный метод наименьших квадратов, включая его модификацию. 
%
%Для ряда, амплитуда которого остается постоянной, наилучшим методом при отсутствии выбросов остается стандартный L2-SSA, однако значение ошибки метода IRLS при отсутствии выбросов оказывается ненамного больше. Метод IRLS является для такого примера наиболее устойчивым.
%
%Для ряда с растущей амплитудой наиболее устойчивыми методами оказываются l1pca и модификация метода IRLS. 
%
%Теоретическая трудоемкость последовательного метода $O(np\log(2pn+nr)N_{iter})$ оказывается меньше теоретической трудоемкости метода с весами, которая составляет $ O(npr^2N_\alpha N_{iter}) $. Однако на практике время работы этих двух методов небольшое и не сильно отличающееся друг от друга: 54 и 39 секунд.


%Уменьшение параметра $\lambda$ в методе с регуляризацией улучшает точность работы метода в случае ряда с большой амплитудой. Присутствие больших по модулю значений в ряде проблемы не составляет, потому что в методе с регуляризацией присутствует нормировка траекторной матрицы. 

%Уменьшение скорости роста амплитуды ряда приводит к тому, что при отсутствии выбросов и шума метод с регуляризацией и метод из статьи~\cite{Rodrigues} работают более точно, чем работали на примере с большой скоростью роста амплитуды. В случае выбросов последовательный метод работает немного лучше, чем метод с регуляризацией. 

%Наличие возрастающего тренда влияет на точность восстановления сигнала последовательным методом. При уменьшении тренда ошибки последовательного метода уменьшаются, и на втором примере наилучшим оказывается последовательный метод.

%Если сравнивать методы по величине оценки RMSE, то можно сделать вывод, что при отсутствии выбросов наиболее точным методом является стандартный L2-SSA с большой длиной окна. Наиболее устойчивым к выбросам оказывается последовательный метод l1pca. При наличии выделяющихся наблюдений ошибка восстановления сигнала методом robustSvd, использующим взвешенную медиану, оказалась меньше, чем ошибка восстановления стандартным методом L2-SSA. Однако последовательный метод все же остается наиболее устойчивым по сравнению с остальными методами. 
%
%Аналогичные выводы можно сделать и при сравнении методов по величине оценки MAD. Наиболее точным методом оказывается L2-SSA с большой длиной окна, а наиболее устойчивым является последовательный метод из раздела~\ref{sec: l1pca}.

%В таблице~\ref{tab3} приведено время работы программы для данных методов для $M=10$ реализаций ряда. 


%Стандартный алгоритм L2-SSA работает во много раз быстрее, чем варианты метода L1-SSA. Сравним теперь два варианта метода L1-SSA между собой. Можно заметить, что время, затраченное на выполнение алгоритма, использующего взвешенную медиану, немного меньше, чем время работы последовательного метода. Однако, как уже говорилось ранее, последовательный метод является намного более устойчивым.
%



%\begin{table}
%	\caption{Время работы программы для различных методов для $M=10$ реализаций ряда.}
%	\label{tab3}
%	\centering
%	\begin{tabular}{|c|c|c|c|}
%		\hline
%		Method 	& 0\% & 1\% & 5\%  \\ 
%		\hline
%		Basic SSA & 0.5227 \text{sec} & 1.3491 \text{sec} & 1.3913 \text{sec}\\
%		l1pca & 264.3195 \text{min} & 635.9288 \text{min} & 586.0057 \text{min} \\
%		robustSvd & 149.6039 \text{min} & 390.4035 \text{min} & 383.8407 \text{min} \\ 
%		\hline
%	\end{tabular}
%\end{table}

\section{Выводы}

Результаты исследования для трех рассмотренных примеров представлены в таблице~\ref{tab_all}. На основе проведенного исследования можно сделать следующие выводы. Для первого примера без растущей амплитуды ряда в случае гауссовского шума наиболее точным методом без выбросов оказывается стандартный SSA, однако самыми устойчивыми к выбросам являются последовательный метод и взвешенный метод наименьших квадратов. 

В случае быстрорастущей амплитуды ряда преимущество взвешенного метода наименьших квадратов пропадает, однако модификация с использованием взвешенной локальной регрессии при выделении тренда из остатков оказывается устойчивой к выбросам. 

Если рассмотреть ряд с гетероскедастичным шумом, то наиболее устойчивыми методами оказываются последовательный и метод IRLS с использованием локальной регрессии.

Исходя из проведенного исследования можно сказать, что если у ряда нет растущей амплитуды и разброс значений небольшой, то следует исследовать разделимость. Если сигнал хорошо отделяется от шума, то в присутствии выбросов рекомендуется использовать взвешенный метод наименьших квадратов. Если разделимости нет, то можно использовать последовательный вариант L1-SSA. В случае появления у ряда растущей амплитуды, последовательный метод начинает работать хуже. В таком случае следует использовать модификацию взвешенного метода наименьших квадратов. Можно также выдвинуть гипотезу о том, что в случае гетероскедастичного шума следует использовать модификацию IRLS с выделением тренда с помощью локальной регрессии, поскольку она тогда точнее выделяет тренд из остатков, а в случае шума с постоянной дисперсией --- модификацию с выделением тренда с помощью взвешенной локальной регрессии, которая хорошо справляется с выбросами.  

\begin{table}
	\caption{Оценки RMSE и MAD для трех рассмотренных примеров для $M=10$ реализаций ряда.}
	\label{tab_all}
	\centering
	\begin{tabular}{|c||c|c||c|c||c|c|}
		\hline
		\multicolumn{7}{|c|}{Оценки RMSE} \\
		\hline
		Method 	& 0\% & 5\% & 0\% & 5\% & 0\% & 5\% \\ 
		\hline
		Basic SSA & \textbf{0.402} & 0. 712  & $\mathbf{1.72}$  & 4.85 & \textbf{0.203} & 476.52 \\
		l1pca & 0.477 & \textbf{0.459}  & \textbf{1.80} & $\mathbf{1.93}$ & 0.228 & 21.270\\
		IRLS (orig.) & 0.459 & \textbf{0.440}  & 2.63  & 2.70 & \textbf{0.196} & 398.2 \\
		IRLS (loess) & 0.491 & 0.494  & $\mathbf{1.78}$ &  $\mathbf{1.87}$ &\textbf{0.198} & 54.212 \\
		IRLS (median) & 0.520 & 0.528 & 2.24  & 2.41 & \textbf{0.213} & 112.6 \\
		IRLS (lowess) & 0.502 & 0.498  &\textbf{2.11} &  2.03& \textbf{0.211} & \textbf{0.202} \\
		\hline
		\multicolumn{7}{|c|}{Оценки MAD} \\
		\hline
		Method 	& 0\% & 5\% & 0\% & 5\% & 0\% & 5\% \\ 
		\hline
		Basic SSA & \textbf{0.366} & 0.593  & $\mathbf{0.95}$  & 2.54 & \textbf{0.157} & 168.2\\
		l1pca & 0.391 & \textbf{0.386} & \textbf{0.89}  & $\mathbf{0.98}$ & 0.171 & 3.145 \\
		IRLS (orig.) & 0.374 & \textbf{0.367}  & 1.24  & 1.30 & \textbf{0.151} & 38.541\\
		IRLS (loess) & 0.407 & 0.414 & $\mathbf{0.93}$ &  $\mathbf{0.96}$ & \textbf{0.154} & 5.180 \\
		IRLS (median) & 0.430 & 0.436  & 1.09  &  1.17 & \textbf{0.158} & 30.121\\
		IRLS (lowess) & 0.417 & 0.414  &\textbf{1.03}  & 1.00 &\textbf{0.155} & \textbf{0.157} \\
		\hline
	\end{tabular}
\end{table}


\section{Исследование числа итераций}

В последовательном методе l1pca и методе IRLS есть дополнительный параметр: максимальное число итераций в цикле. Во взвешенном методе наименьших квадратов задается максимальное число итераций для внешнего цикла и для внутреннего. Для того, чтобы иметь возможность сравнить трудоемкости методов, необходимо исследовать, какое количество итераций требуется для сходимости этих методов. На рисунках~\ref{pcal1_1} и~\ref{pcal1_2} показана зависимость ошибки RMSE от числа итераций для двух примеров (первый пример с экспоненциальным трендом, второй пример с быстрорастущей амплитудой ряда). На рисунках~\ref{irls_1} и~\ref{irls_2} представлена зависимость RMSE от числа итераций во внешнем цикле для метода IRLS. Сравнения для различного числа итераций проводились на одинаковых реализациях ряда.

 Из графиков видно, что отличия в ошибках при увеличении итераций совсем незначительные, и в последовательном методе можно было бы ограничиться и 5 итерациями.
 
 Для метода IRLS достаточно брать порядка 7 итераций внешнего цикла для второго примера. Для первого примера ошибка практически перестает убывать при 20 итерациях. Число итераций во внутреннем цикле можно оставить по умолчанию равным 5, так как отличия в таком случае совсем незначительные.
 
 Таким образом, если сравнивать трудоемкости последовательного метода (\ref{complexity_pcaL1}) и метода IRLS (\ref{complexity_IRLS}), то оказывается, что трудоемкость последовательного метода все же оказывается меньше трудоемкости взвешенного метода наименьших квадратов. 
 
\begin{figure}[h]
	\begin{center}
		\begin{minipage}[h]{0.45\linewidth}
			\includegraphics[width=1\linewidth]{l1pca_1_}
			\caption{Зависимость RMSE от числа итераций для последовательного метода l1pca, пример 1.} %% подпись к рисунку
			\label{pcal1_1} %% метка рисунка для ссылки на него
		\end{minipage}
		\hfill 
		\begin{minipage}[h]{0.48\linewidth}
			\includegraphics[width=1\linewidth]{l1pca_2_}
			\caption{Зависимость RMSE от числа итераций для последовательного метода l1pca, пример 2.}
			\label{pcal1_2}
		\end{minipage}
	\end{center}
\end{figure}


\begin{figure}[h]
	\begin{center}
		\begin{minipage}[h]{0.46\linewidth}
			\includegraphics[width=1\linewidth]{IRLS_1_}
			\caption{Зависимость RMSE от числа итераций для метода IRLS, пример 1.} %% подпись к рисунку
			\label{irls_1} %% метка рисунка для ссылки на него
		\end{minipage}
		\hfill 
		\begin{minipage}[h]{0.47\linewidth}
			\includegraphics[width=1\linewidth]{IRLS_2_}
			\caption{Зависимость RMSE от числа итераций для метода IRLS, пример 2.}
			\label{irls_2}
		\end{minipage}
	\end{center}
\end{figure}

\conclusion
В работе были приведены и исследованы некоторые варианты модификации метода SSA с целью повышения устойчивости к выбросам.

 В главе~\ref{sec:BasicSSA} были введены основные понятия и обозначения, описан алгоритм базового метода.
 
 В главе~\ref{sec:modifications} была введена общая схема метода с проекторами на пространство ганкелевых матриц и множество матриц ранга, не превосходящего $r$, без указания конкретной нормы. Для построения $\mathbb{L}_1$-проектора на множество матриц ранга, не превосходящего $r$, было предложено два способа. Первый из них был взят из статьи~\cite{Rodrigues} и использует взвешенную медиану. Второй метод последовательный, который уже реализован в R-пакете~\cite{pcaL1}. Главное отличие этих методов состоит в том, что первый метод ищет собственные тройки по очереди, но не в порядке убывания собственных чисел. Второй метод ищет все компоненты одновременно в виде матрицы.
 
  Также была рассмотрена другая идея построения устойчивого метода --- присвоение точкам, содержащим выбросы, меньший вес. Этой идее соответствует метод из статьи~\cite{Chen}. Но так как было обнаружено, что этот метод не подходит для рядов с растущей или убывающей амплитудой, была разработана модификация данного метода.
 
 Бало проведено сравнение теоретических трудоемкостей методов. Рассмотрено два случая: случай с квадратной и вытянутой траекторной матрицей. В обоих случаях теоретическая трудоемкость последовательного метода оказалась наименьшей.
 
 Глава~\ref{sec: experiments} была посвящена численным экспериментам. Классический метод L2-SSA сравнивался с двумя вариантами L1-SSA (l1pca и IRLS). В случайных точках ряда $f_i$ добавлялись выделяющиеся наблюдения, равные  $5f_i$ для первого и третьего примеров и $1.5f_i$ для второго. Для начала проводилось сравнение методов без выделяющихся наблюдений, а затем при 1\% и 5\% выбросов. Сравнение проводилось по величине оценок ошибок RMSE и MAD. 

%Оказалось, что два метода работают достаточно хорошо --- это последовательный метод l1pca и метод с весами IRLS, а также его модификация. 
 
 Сначала рассматривался пример из работы~\cite{vkr}, но с добавлением большего количества выбросов. Было показано, что в таком случае метод IRLS оказался довольно точным и устойчивым.
 
 Далее ряд был заменен на ряд с большим разбросом значений. Был рассмотрен случай с гетероскедастичным шумом. На таком примере преимущество метода IRLS пропадает. Однако его модификация с выделением тренда с помощью локальной регрессии  оказывается устойчивой к выбросам и точной при отсутствии выбросов. В случае шума с постоянной дисперсией лучшим методом оказалась модификация IRLS с использованием взвешенной локальной регрессии.
 

 
 %Далее были проведены сравнения на измененных рядах с целью определения причины того, что для рассмотренных рядов наилучшими оказались разные методы. В результате выяснилось, что при уменьшении тренда метод l1pca восстанавливает сигнал точнее. Поэтому на первом примере ошибки последовательного примера были чуть больше, а в случае второго ряда этот метод оказался наилучшим.
 
% При росте числа выделяющихся наблюдений наиболее устойчивым оказался последовательный метод l1pca, но время его работы значительно превышает время работы стандартного метода. Метод robustSvd работает немного быстрее, чем l1pca, но ошибка восстановления сигнала этим методом оказалась значительно больше. 
 
% Таким образом, в работе были предложены два варианта метода L1-SSA, которые являются более устойчивыми к выделяющимся наблюдениям, чем стандартный метод, но они оказываются гораздо более трудоемкими, чем L2-SSA. Один из методов выделяет сигнал достаточно точно и является устойчивым, но время работы алгоритма слишком большое. Другой метод работает быстрее, но восстанавливает сигнал неточно. Одним из решений данной проблемы может являться модификация метода robustSvd. Если изменить метод таким образом, чтобы компоненты находились в порядке убывания собственных значений, то метод должен работать точнее. Для этого можно прогонять метод один раз, сортировать полученные компоненты по убыванию собственных чисел, а затем запускать алгоритм еще раз, но за начальные приближения брать уже полученные компоненты. Тогда время работы метода увеличится совсем ненамного, а точность возрастет.
 
% Для начала было проведено сравнение нескольких вариантов оценки сигнала стандартным методом L2-SSA с различными параметрами: SSA с большой длиной окна с восстановлением по трем компонентам, SSA с небольшой длиной окна (с восстановлением по одной компоненте и по трем компонентам), SSA с маленькой длиной окна с восстановлением по одной компоненте, SSA с проекцией. Эти методы сравнивались с результатом фильтрации с помощью треугольного фильтра. Скользящая медиана также была рассмотрена, и было показано, что данный фильтр является устойчивым при большом размере выбросов, но при отсутствии выделяющихся наблюдений ошибка оказывается достаточно большой, поэтому подробно в работе рассматривались только линейные фильтры. 
% 
% В результате проведенных исследований среди вариантов метода L2-SSA можно сделать вывод, что метод SSA с большой длиной окна при отсутствии выделяющихся наблюдений является наиболее точным до величины выброса, в 7 раз большей дисперсии шума, но, при б\'ольших значениях выброса лучшим оказывается фильтр Бартлетта.  При восстановлении методом L2-SSA с небольшой длиной окна при таком размере выброса ошибка растет медленнее, хотя при отсутствии выброса ошибка восстановления данным методом была больше, чем ошибка метода L2-SSA с большой длиной окна.
%
%Затем была произведена замена проектора на ганкелевы матрицы на проектор по норме в $\mathbb{L}_1$, и проведено сравнение метода L2-SSA с L2SVD-L1H-SSA. Было показано, что при замене на проектор по норме в $\mathbb{L}_1$ оценка MSE для некоторых методов уменьшается, а для некоторых увеличивается. Однако оценка MAD в присутствии выделяющихся наблюдений становится меньше для всех методов. Был сделан вывод, что использование проектора по норме в $\mathbb{L}_1$ оказывает незначительное влияние на повышение устойчивости метода.
%
%Сравнение трех вариантов реализации метода L1-SSA между собой и с методом L2-SSA с большой длиной окна показало, что при отсутствии выбросов ошибка MSE оказывается наименьшей для стандартного метода L2-SSA, но наиболее устойчивым методом является L1-SSA с регуляризацией. Однако если сравнивать ошибки MAD, то наилучшим методом как при отсутствии выбросов, так и с выделяющимися наблюдениями, является метод с регуляризацией. Важно отметить, что последовательный метод и метод с регуляризацией оказались точнее и устойчивее метода, предложенного в статье~\cite{Hassani}. 
%
%Таким образом, в работе были предложены два варианта метода L1-SSA, которые являются устойчивыми к выделяющимся наблюдениям, но они оказываются гораздо более трудоемкими, чем методы L2-SSA и L2SVD-L1H-SSA.


%\\
\nocite{*}\bibliography{biblio_report}
\bibliographystyle{gost2008}
%\addcontentsline{toc}{section}{Список литературы}



\end{document}

